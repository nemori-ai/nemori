# Nemoriï¼šè‡ªç„¶å¯å‘çš„æƒ…æ™¯è®°å¿†ç³»ç»Ÿ  
Nemori: Nature-Inspired Episodic Memory System

## é¡¹ç›®æ¦‚è¿°  [![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/nemori-ai/nemori)
## Project Overview

Nemori-AI æ—¨åœ¨è®©å¤§è¯­è¨€æ¨¡å‹å…·å¤‡ç±»äººçš„æƒ…æ™¯è®°å¿†èƒ½åŠ›ã€‚  
Nemori-AI empowers large language models with human-like episodic memory.

Nemori é€šè¿‡è‡ªç„¶ã€äº‹ä»¶åŒ–çš„ç´¢å¼•æ–¹å¼ï¼Œå¸®åŠ©ç³»ç»Ÿåœ¨å…³é”®æ—¶åˆ»ç²¾å‡†å›æº¯åŸå§‹ç»å†ã€‚  
Nemori stores experiences as natural, event-centric traces, enabling precise recall when it matters.

æ„¿æ™¯ï¼šè®©æ¯ä¸€æ¬¡æ•°æ®äº¤äº’ï¼Œéƒ½èƒ½åƒäººç±»è®°å¿†ä¸€æ ·è¢«ç†è§£ã€è¢«å›å¿†ã€è¢«å»¶ç»­ã€‚  
**Vision:** Every piece of data remembered and retrieved as intuitively as human recollection.

Nemori æºè‡ªæˆ‘ä»¬å›¢é˜Ÿ Tanka.ai é¡¹ç›®ä¸­è®°å¿†ç³»ç»Ÿçš„æƒ…æ™¯è®°å¿†ç´¢å¼•æ¨¡å—â€”â€”è¿™æ˜¯ä¸€ä¸ªæˆ‘ä»¬è®¡åˆ’å¼€æºçš„ MVP å®ç°ã€‚å…¶æ ¸å¿ƒç›®æ ‡æ˜¯åˆ†äº«æˆ‘ä»¬é€šè¿‡è‡ªç„¶å¯å‘çš„æƒ…æ™¯è®°å¿†æ¥æ„å»ºè®°å¿†ç´¢å¼•ï¼ˆNemori: Nature-Inspired Episodic Memoryï¼‰çš„æ–¹æ³•ã€‚  
Nemori is derived from our team's episodic memory indexing module within the memory system of our Tanka.ai projectâ€”an MVP implementation that we plan to open-source. Its core purpose is to share our approach to building memory indexing through Nature-Inspired Episodic Memory.

è™½ç„¶ Mem0ã€Lettaã€Supermemoryã€ZEPã€MemOS ç­‰å…ˆå‰ç³»ç»Ÿåœ¨ AI è®°å¿†æ–¹é¢åšå‡ºäº†å“è¶Šå°è¯•ï¼Œåœ¨ LoCoMo åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†å…ˆè¿›çš„æ€§èƒ½ï¼Œä½† Nemori å¼•å…¥äº†ä¸€ç§åˆ›æ–°ä¸”ç®€çº¦çš„æ–¹æ³•ï¼Œä¸“æ³¨äºä¸äººç±»æƒ…æ™¯è®°å¿†æ¨¡å¼ä¿æŒä¸€è‡´ã€‚é‰´äºæœ€è¿‘ä¼˜ç§€çš„å¼€æºé¡¹ç›®å’Œè®°å¿†ç³»ç»Ÿç ”ç©¶çš„æ¶Œç°ï¼Œæˆ‘ä»¬éƒ½æ±‡èšåœ¨ä½¿ç”¨ LoCoMo æ•°æ®é›†ä½œä¸ºåŸºå‡†ï¼Œå› æ­¤æˆ‘ä»¬å†³å®šç”¨å±•ç¤ºæˆ‘ä»¬æƒ…æ™¯è®°å¿†ç´¢å¼•æ–¹æ³•çš„ MVP å®ç°å‚ä¸è¿™ä¸€åŸºå‡†æµ‹è¯•ã€‚  
While previous systems like Mem0, Letta, Supermemory, ZEP, and MemOS have made remarkable attempts at AI memory, achieving advanced performance on the LoCoMo benchmark, Nemori introduces an innovative and minimalist approach centered on aligning with human episodic memory patterns. Given the recent surge of excellent open-source projects and research in memory systems, we've all converged on using the LoCoMo dataset as a benchmark. Consequently, we decided to participate in this benchmark with our MVP implementation that demonstrates our episodic memory indexing approach.

## å®éªŒç»“æœ  
## Experimental Results

ä¸ºäº†çªå‡º Nemori è¿™ä¸ªç®€æ´æ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼Œæˆ‘ä»¬åœ¨ LoCoMo åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œä¸å…ˆå‰çš„æœ€å…ˆè¿›æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒï¼š  
To highlight the superiority of Nemori's concise approach, we conducted evaluations on the LoCoMo benchmark, comparing against previous state-of-the-art approaches:

### LoCoMo åŸºå‡†æµ‹è¯•ç»“æœ  
### LoCoMo Benchmark Results

åœ¨ LoCoMoï¼ˆé•¿ä¸Šä¸‹æ–‡å¯¹è¯å»ºæ¨¡ï¼‰æ•°æ®é›†ä¸Šï¼ŒNemori å±•ç°äº†å“è¶Šçš„æ€§èƒ½ï¼š  
On the LoCoMo (Long-Context Conversation Modeling) dataset, Nemori demonstrates exceptional performance:

![LoCoMo Benchmark Results](figures/locomo-scores.png)

## å¤ç°æ–¹æ³•ï¼ˆReproduction Guideï¼‰  
## Reproduction Guide

è¦å¤ç° Nemori åœ¨ LoCoMo åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœï¼Œè¯·å‚è€ƒ [evaluation/README.md](evaluation/README.md) è·å–è¯¦ç»†çš„è¯„æµ‹ç¯å¢ƒæ­å»ºä¸è¿è¡Œæ­¥éª¤ã€‚  
To reproduce Nemori's experimental results on the LoCoMo benchmark, please refer to [evaluation/README.md](evaluation/README.md) for detailed evaluation environment setup and execution steps.

## è®¾è®¡ç†å¿µ  
## Design Philosophy

å½“æˆ‘ä»¬äººç±»å›å¿†è¿‡å»çš„äº‹ä»¶æ—¶ï¼Œæˆ‘ä»¬çš„è„‘æµ·ä¸­ç»å¸¸é—ªç°ç›¸å…³çš„å›¾åƒã€åŠ¨ä½œæˆ–å£°éŸ³ã€‚æˆ‘ä»¬çš„å¤§è„‘é€šè¿‡è®©æˆ‘ä»¬é‡æ–°ä½“éªŒå½“æ—¶å‘ç”Ÿçš„äº‹æƒ…æ¥å¸®åŠ©æˆ‘ä»¬è®°å¿†â€”â€”è¿™ç§è®°å¿†æœºåˆ¶è¢«ç§°ä¸ºæƒ…æ™¯è®°å¿†ã€‚  
When we humans recall past events, our minds often flash with related images, actions, or sounds. Our brains help us remember by essentially making us re-experience what happened at that time - this memory mechanism is called episodic memory.

Nemori çš„è®¾è®¡çµæ„Ÿæ¥è‡ªäººç±»çš„æƒ…æ™¯è®°å¿†ã€‚Nemori å¯ä»¥è‡ªä¸»åœ°å°†äººä¸äººã€äººä¸ AI æ™ºèƒ½ä½“æˆ– AI æ™ºèƒ½ä½“ä¹‹é—´çš„å¯¹è¯é‡å¡‘ä¸ºæƒ…æ™¯ç‰‡æ®µã€‚ä¸åŸå§‹å¯¹è¯ç›¸æ¯”ï¼Œæƒ…æ™¯ç‰‡æ®µå…·æœ‰æ›´è¿è´¯çš„å› æœå…³ç³»å’Œæ—¶é—´è¡¨è¾¾èƒ½åŠ›ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæƒ…æ™¯ç‰‡æ®µçš„è¡¨è¾¾åœ¨æŸç§ç¨‹åº¦ä¸Šä¸æˆ‘ä»¬äººç±»è®°å¿†å›å¿†çš„ç²’åº¦ç›¸ä¸€è‡´ï¼Œè¿™æ„å‘³ç€ä½œä¸ºäººç±»ï¼Œæˆ‘ä»¬æ›´å¯èƒ½æå‡ºå…³äºæƒ…æ™¯ç‰‡æ®µçš„é—®é¢˜ï¼Œè¿™äº›é—®é¢˜åœ¨è¯­ä¹‰ä¸Šæ›´æ¥è¿‘æƒ…æ™¯ç‰‡æ®µæœ¬èº«ï¼Œè€Œä¸æ˜¯åŸå§‹æ¶ˆæ¯ã€‚  
Nemori's design inspiration comes from human episodic memory. Nemori can autonomously reshape conversations between humans, between humans and AI agents, or between AI agents into episodes. Compared to raw conversations, episodes have more coherent causal relationships and temporal expression capabilities. More importantly, the expression of episodes aligns to some extent with the granularity of our human memory recall, meaning that as humans, we are likely to ask questions about episodes that are semantically closer to the episodes themselves rather than the original messages.

### é¢—ç²’åº¦ä¸å¤§æ¨¡å‹è®­ç»ƒåˆ†å¸ƒçš„å¯¹é½  
### Granularity Alignment with LLM Training Distribution

æˆ‘ä»¬è®¾è®¡ä¸­çš„ä¸€ä¸ªå…³é”®æ´å¯Ÿæ˜¯ï¼Œæƒ…æ™¯è®°å¿†é¢—ç²’åº¦å¯¹é½ä¸ºå¤§è¯­è¨€æ¨¡å‹æä¾›äº†æ½œåœ¨çš„ä¼˜åŒ–æ”¶ç›Šã€‚ç”±äºå¤§æ¨¡å‹çš„è®­ç»ƒæ•°æ®é›†ä¼šå¯¹é½äººç±»ä¸–ç•Œçš„æ–‡æœ¬åˆ†å¸ƒï¼Œæ‰€ä»¥å¯¹é½å›å¿†é¢—ç²’åº¦çš„åŒæ—¶ï¼Œä¹Ÿåœ¨å¯¹é½ã€Œè‡ªç„¶ä¸–ç•Œä¸­æœ€å¤§æ¦‚ç‡çš„äº‹ä»¶è¡¨è¿°é¢—ç²’åº¦ã€ã€‚  
A key insight in our design is that episodic memory granularity alignment offers potential optimization benefits for large language models. Since LLM training datasets align with the textual distribution of the human world, aligning recall granularity simultaneously aligns with the "most probable event description granularity in the natural world."

è¿™ç§å¯¹é½æä¾›äº†å‡ ä¸ªä¼˜åŠ¿ï¼š  
This alignment provides several advantages:
- **å‡å°‘åˆ†å¸ƒåç§»**ï¼šå½“å­˜å‚¨çš„æƒ…æ™¯ç‰‡æ®µä¸è®­ç»ƒè¯­æ–™ä¸­çš„å…¸å‹äº‹ä»¶è·¨åº¦åŒ¹é…æ—¶ï¼Œå›å¿†æç¤ºæ›´æ¥è¿‘é¢„è®­ç»ƒåˆ†å¸ƒï¼Œæé«˜äº† token é¢„æµ‹æ¦‚ç‡  
  **Reduced Distributional Shift**: When stored episodes match typical event spans found in training corpora, recall prompts resemble the pre-training distribution, improving token prediction probabilities
- **å¢å¼ºæ£€ç´¢ç²¾åº¦**ï¼šå­˜å‚¨ã€Œäººç±»å°ºåº¦ã€äº‹ä»¶çš„è®°å¿†ç´¢å¼•æ“ä½œçš„æ˜¯è¯­ä¹‰çº ç¼ è¾ƒå°‘çš„å•å…ƒï¼Œæé«˜äº†æ£€ç´¢ä¸­çš„ä¿¡å™ªæ¯”  
  **Enhanced Retrieval Precision**: Memory indices storing "human-scale" events operate on semantically less entangled units, increasing signal-to-noise ratio in retrieval

## æŠ€æœ¯å®ç°æ–¹æ³•  
## Technical Implementation

### æ•°æ®é¢„å¤„ç†  
### Data Preprocessing

ç”±äºæˆ‘ä»¬çš„ç”Ÿäº§ç³»ç»Ÿå¢é‡å¤„ç†åŸå§‹æƒ…æ™¯æ•°æ®ï¼Œæˆ‘ä»¬é‡ç”¨äº†ä¸»é¢˜åˆ†å‰²ç­–ç•¥ã€‚è¿™ä½“ç°äº†æƒ…æ™¯è®°å¿†åˆ›å»ºçš„æ ¸å¿ƒå“²å­¦ï¼š"ä¸äººç±»è®°å¿†äº‹ä»¶æƒ…æ™¯çš„ç²’åº¦ä¿æŒä¸€è‡´"ã€‚è™½ç„¶æˆ‘ä»¬çš„æ–¹æ³•å¯èƒ½çœ‹èµ·æ¥ä½æ•ˆä¸”ç®€å•ï¼Œä½†è¿™åæ˜ äº†æˆ‘ä»¬ MVP ä¸ºäº†æ¸…æ™°å±•ç¤ºæ–¹æ³•è€Œåšå‡ºçš„ç®€åŒ–ã€‚åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨æ›´å…·æˆæœ¬æ•ˆç›Šå’Œé«˜æ•ˆçš„æ–¹æ³•ã€‚  
Since our production system processes raw episodic data incrementally, we reused our topic segmentation strategy. This embodies the core philosophy of episodic memory creation: "aligning with the granularity of human memory event episodes." While our approach may appear inefficient and simplistic, this reflects the simplifications made for our MVP. In production, we employ more cost-effective and efficient methods.

å¯¹äºæƒ…æ™¯ç”Ÿæˆï¼Œæˆ‘ä»¬é€‰æ‹©äº†æœ€ç›´æ¥çš„ç‰ˆæœ¬æ¥æœ€å¥½åœ°è¯´æ˜æˆ‘ä»¬çš„æ–¹æ³•ï¼Œä»…ä½¿ç”¨ gpt-4o-mini/gpt-4.1-mini è¿›è¡Œæƒ…æ™¯è®°å¿†æå–ã€‚  
For episode generation, we chose the most straightforward version that best illustrates our approach, using only gpt-4o-mini/gpt-4.1-mini for episodic memory extraction.

### æ£€ç´¢ç­–ç•¥  
### Retrieval Strategy

æˆ‘ä»¬ä¸ºæ¯ä¸ªç”¨æˆ·çš„æƒ…æ™¯è®°å¿†å»ºç«‹äº†æœ€å°çš„ BM25 ç´¢å¼•ã€‚è¿™å¯èƒ½ä¼šå¼•èµ·ç–‘é—®ï¼Œä½†è¿™åŒæ ·æ˜¯ä¸€ç§ç®€åŒ–ã€‚æˆ‘ä»¬çš„ç”Ÿäº§ç³»ç»Ÿé‡‡ç”¨ç»“åˆç¨€ç–ï¼ˆBM25ï¼‰å’Œå¯†é›†ï¼ˆå‘é‡æ£€ç´¢ï¼‰æ–¹æ³•çš„æ··åˆæ£€ç´¢ç­–ç•¥ï¼Œä»¥å¹³è¡¡å¬å›å’Œè¯­ä¹‰åŒ¹é…èƒ½åŠ›ï¼Œå¹¶é’ˆå¯¹ç‰¹å®šä¸šåŠ¡éœ€æ±‚å®šåˆ¶ä¸åŒçš„é‡æ’ç­–ç•¥ã€‚  
We established a minimal BM25 index for each user's episodic memories. This might raise questions, but again, it's a simplification. Our production system employs a hybrid retrieval strategy combining sparse (BM25) and dense (vector retrieval) methods to balance recall and semantic matching capabilities, with different reranking strategies tailored to specific business needs.

åœ¨é¢„å¤„ç†å®Œæˆåï¼Œåç»­è¿‡ç¨‹ç›¸å¯¹ç®€å•ã€‚æˆ‘ä»¬æ£€ç´¢å‰ 20 ä¸ªç»“æœï¼Œè®© gpt-4o-mini/gpt-4.1-mini ç”Ÿæˆå“åº”ï¼Œå¹¶éµå¾ªä¸å…¶ä»–é¡¹ç›®å‡ ä¹ç›¸åŒçš„è¯„ä¼°æ–¹æ³•ã€‚  
With the preprocessing complete, the subsequent process is relatively straightforward. We retrieve the top 20 results, have gpt-4o-mini/gpt-4.1-mini generate responses, and follow an evaluation approach nearly identical to other projects.

## æœªæ¥è·¯çº¿å›¾  
## Future Roadmap

1. [è®¡åˆ’å¼€æº] æ·»åŠ ã€Œè¯­ä¹‰è®°å¿†ã€èƒ½åŠ›ï¼Œç”¨äºæ”¹å–„æƒ…æ™¯è®°å¿†ä¸¢å¤±åŸæ–‡ä¸­åç§°ã€åœ°ç‚¹ç­‰ä¿¡æ¯çš„é—®é¢˜ã€‚  
   [Planned open source] Add "semantic memory" capability to address the issue of episodic memory losing information such as names and locations from the original text.

2. ä»…ä»…æ‹¥æœ‰ç‰¹å®šäº‹ä»¶çš„æƒ…æ™¯è®°å¿†æ˜¯ä¸å¤Ÿçš„ã€‚æˆ‘ä»¬å¸Œæœ›é€šè¿‡ç›¸ä¼¼æ€§åº¦é‡ç­‰æ–¹æ³•è‡ªåŠ¨èšåˆæƒ…æ™¯ç‰‡æ®µï¼Œå½¢æˆæ›´é•¿æœŸå’Œé€šç”¨çš„é«˜çº§æƒ…æ™¯ç‰‡æ®µã€‚  
   Having episodic memory of specific events alone is insufficient. We hope to aggregate episodes through similarity measures and other methods to form longer-term and more general high-level episodes.

## FAQ

### 1. Nemori åœ¨ LoCoMo æ•°æ®é›†çš„åˆ†æ•°çœ‹èµ·æ¥è·Ÿ MemOS æå‡å¹¶ä¸å¤§ï¼Œä½ ä»¬çš„è®°å¿†æ–¹æ¡ˆæœ‰ä»€ä¹ˆæ˜¾è‘—çš„ä¼˜åŠ¿å—ï¼Ÿ  
**Q: Nemori's score on the LoCoMo dataset doesn't seem much higher than MemOS. What are the significant advantages of your memory approach?**

MemOS æ˜¯ä¸€ä¸ªéå¸¸ä¼˜ç§€çš„æ–¹æ¡ˆï¼Œå¹¶ä¸”åœ¨æˆ‘ä»¬çœŸå®çš„ç³»ç»Ÿä¸­ï¼Œæœ‰éå¸¸å¤šç±»ä¼¼æ¨¡å—è®¾è®¡ï¼Œåœ¨å®é™…çš„è½åœ°åœºæ™¯ä¸­ï¼Œè®°å¿†è¿˜æ˜¯ä¸€ä¸ªä¸ä¸šåŠ¡å¼ºç›¸å…³çš„é—®é¢˜ï¼Œå› æ­¤æˆ‘ä»¬å¹¶æ— æ„å¯¹æ¯”æ¡†æ¶æœ¬èº«çš„ä¼˜åŠ£ã€‚æˆ‘ä»¬æƒ³è¡¨è¾¾çš„æ˜¯ï¼Œæ²¿ç€ã€Œä¸äººç±»è®°å¿†äº‹ä»¶çš„é¢—ç²’åº¦å¯¹é½ã€è¿™ä¸€ä¸ªç®€å•è€Œæ·±åˆ»çš„æ´å¯Ÿä½œä¸ºèµ·ç‚¹ï¼Œ**åœ¨ç‰¹å®šç±»å‹çš„ä»»åŠ¡ä¸­**åªéœ€è¦å¾ˆç®€å•çš„æ–¹æ³•å°±èƒ½åª²ç¾å¤æ‚çš„è®°å¿†æ¡†æ¶ã€‚åœ¨è¿™ä¸ªç‰ˆæœ¬ä¸­ï¼Œç”šè‡³éƒ½æ²¡æœ‰ä½¿ç”¨ Embedding æ¥å¢å¼ºè¯­æ„ç›¸å…³æ€§çš„å¬å›æ•ˆæœã€‚å› æ­¤ï¼Œå¦‚æœåªæ˜¯æƒ³æå‡è¿™ä¸ªæ•°æ®é›†çš„åˆ†æ•°ï¼Œè¿˜æœ‰éå¸¸å¤§çš„æå‡ç©ºé—´ã€‚å¤§å®¶æ„Ÿå…´è¶£å¯ä»¥è‡ªè¡Œå°è¯•å¼•å…¥ä¸€äº›å¸¸è§„çš„ä¼˜åŒ–æ–¹æ¡ˆï¼Œæ¯”å¦‚æƒ…æ™¯åˆ‡å‰²åçš„è¾¹ç•Œä¸Šä¸‹æ–‡ä¿®å¤ï¼Œç­–ç•¥æ€§çš„å¸¦å…¥éƒ¨åˆ†åŸæ–‡ï¼Œå¼•å…¥æ··åˆå¬å›+Rerankï¼Œæˆ–è€…è¿›è¡Œè®°å¿†å…³è”èåˆç­‰ï¼Œè¿™äº›éƒ½æ˜¯æˆ‘ä»¬åœ¨çœŸå®ç³»ç»Ÿä¸­ä½¿ç”¨å¹¶ä¸”è¢«è¯æ˜æœ‰æ•ˆçš„æ–¹æ³•ã€‚  
MemOS is an excellent solution, and in our real-world systems, we have many similar module designs. In practical scenarios, memory is still a business-specific problem, so we do not intend to compare the merits of the frameworks themselves. What we want to express is that starting from the simple yet profound insight of "aligning with the granularity of human memory events," **for certain types of tasks**, even simple methods can rival complex memory frameworks. In this version, we didn't even use embeddings to enhance semantic recall. Therefore, if you just want to improve the score on this dataset, there is still a lot of room for improvement. You are welcome to try common optimization strategies, such as boundary context repair after episode segmentation, strategically including parts of the original text, introducing hybrid retrieval + rerank, or memory association fusion. These are all methods we use and have proven effective in real systems.

### 2. ä¸ºä»€ä¹ˆæ£€ç´¢ä¹‹ååªç”¨æƒ…æ™¯ï¼Œä¸è€ƒè™‘å¸¦ä¸ŠåŸæ–‡ï¼Ÿ  
**Q: Why do you only use episodes after retrieval, without including the original text?**

å› ä¸ºæˆ‘ä»¬çœŸå®çš„ç³»ç»Ÿä¸­ï¼Œæœ‰å¦ä¸€å¥—æ–¹æ¡ˆæ¥è§£å†³ã€ŒæŸäº›ä¿¡æ¯ä»…èƒ½åœ¨åŸæ–‡è·å–ã€çš„é—®é¢˜ï¼ˆåç»­è®¡åˆ’å¼€æºï¼‰ï¼Œå¤§æ¦‚çš„æ–¹å¼å°±æ˜¯ä¼šæœ‰é€‰æ‹©æ€§çš„ fusion å…³é”®æ€§è¯­ä¹‰è®°å¿†å›æƒ…æ™¯è®°å¿†ï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬ç®€åŒ–æ‰äº†ã€‚åŒæ—¶æˆ‘ä»¬ä¸è§‰å¾—ç›´æ¥å¸¦å…¨éƒ¨åŸæ–‡æˆ–è€…å¸¦ top-x ä¸ªåŸæ–‡ï¼Œä¼šæœ‰ç‰¹åˆ«æ˜æ˜¾çš„æå‡ã€‚ç›´è§‰ä¸Šä¸ªåˆ« case å¯èƒ½å¯ä»¥ç­”å¯¹ï¼Œä½†æ˜¯æ•´ä½“ä¸Šå˜åŒ–ä¸ä¼šå¤ªå¤§ï¼ˆè¿™é‡Œä¸»è¦å—é™äº gpt-4o-miniï¼‰ï¼Œæ„Ÿå…´è¶£çš„å¯ä»¥è¯•ä¸€ä¸‹ã€‚  
In our real system, we have another solution (planned to be open-sourced) to address the issue that "some information can only be obtained from the original text." The general approach is to selectively fuse key semantic memories back into episodic memory, which we have omitted here for simplicity. We also don't think that directly including all or the top-x original texts would bring significant improvement. Intuitively, it may help in some cases, but overall, the change won't be substantial (mainly limited by gpt-4o-mini). Feel free to try it out if you're interested.

### 3. Nemori çš„å®éªŒæ•°æ®ä¸­ï¼Œæ¯ä¸ªé—®é¢˜æ¶ˆè€—çš„ token æ˜æ˜¾è¶…è¿‡äº†å…¶ä»–æ–¹æ³•ï¼Œæ‰€ä»¥è¿™ä¸ªæ•ˆæœæ˜¯é å¤§é‡çš„ä¸Šä¸‹æ–‡æ¢æ¥çš„å—ï¼Ÿ  
**Q: In Nemori's experimental data, each question consumes significantly more tokens than other methods. Is this effect achieved by using a large amount of context?**

æƒ…æ™¯è®°å¿†çš„æ„å»ºç­–ç•¥ï¼Œä¼šè®©æƒ…æ™¯è¡¨è¿°æ¯”ä¸€èˆ¬çš„æ€»ç»“æ€§è¯­å¥é•¿ï¼Œåœ¨åŒæ ·å– topk = 20 çš„æ¡ä»¶ä¸‹ï¼Œæ€» token æ•°è¦æ¯”å…¶ä»–æ–¹æ³•é«˜ä¸å°‘ï¼Œä½†æ ¹æ®æˆ‘ä»¬çš„ç»éªŒï¼Œå³ä½¿ topk = 10ï¼ˆå³ token â‰ˆ å‡åŠï¼‰ï¼Œå¯èƒ½è¡¨ç°å·®åˆ«ä¸ä¼šå¤ªå¤§ã€‚åè¿‡æ¥ï¼Œå¦‚æœåœ¨ä¸€ä¸ªåˆç†èŒƒå›´å†…æå‡çª—å£ä¸Šä¸‹æ–‡çš„ä½¿ç”¨å¯¹æ€§èƒ½æœ‰æå‡ï¼Œä¸ºä»€ä¹ˆä¸å‘¢ï¼Ÿ  
The construction strategy of episodic memory makes episode descriptions longer than typical summary statements. With the same topk = 20, the total token count is much higher than other methods, but in our experience, even with topk = 10 (i.e., about half the tokens), the performance difference may not be significant. Conversely, if increasing the context window within a reasonable range improves performance, why not do it?

### 4. è¯¥æ–¹æ³•ä¸­å…³äºæƒ…æ™¯çš„è¡¨è¿°ï¼Œæœ‰å¤§é‡çš„ç±»ä¼¼ã€Œthe previous Friday (June 23, 2023)ã€è¿™æ ·çš„è¡¨è¿°ï¼Œæ˜¯é¢å‘è¯„ä¼°é›†çš„å®šå‘ä¼˜åŒ–å—ï¼Ÿ  
**Q: In your method, there are many expressions like "the previous Friday (June 23, 2023)" in the episodes. Is this a targeted optimization for the evaluation set?**

åœ¨æˆ‘ä»¬å›¢é˜ŸçœŸå®çš„ Agent ç³»ç»Ÿä¸­ï¼Œæœ‰ä¸“é—¨çš„æ—¶é—´å¢å¼ºå¤„ç†è¿‡ç¨‹ï¼Œå¹¶å°†ç»“æœæŒ‚è½½åœ¨æƒ…æ™¯çš„å…ƒæ•°æ®ä¸­ï¼Œå¸®åŠ©åœ¨ä¸åŒåœºæ™¯çš„ä¸šåŠ¡ä¸­å‡†ç¡®æ‰¾åˆ°ç›¸å¯¹/ç»å¯¹æ—¶é—´ã€‚æˆ‘ä»¬åœ¨ MVP å®ç°åšäº†ç®€åŒ–ï¼Œç›´æ¥ä½¿ç”¨è¿™æ ·çš„å½¢å¼æ‹¼æ¥åœ¨æƒ…æ™¯æ­£æ–‡ä¸­ã€‚è€Œå¦‚æœè¿™ä¸€ä¸ªæ“ä½œå¦‚æœå¯¹ LoCoMo æ•°æ®é›†æœ‰å¸®åŠ©ï¼ˆå…¶å®æˆ‘å¹¶ä¸å¤ªç¡®å®šï¼Œå› ä¸ºç¬¬ä¸€ç‰ˆå°±æ˜¯è¿™ä¹ˆåšçš„ï¼‰ï¼Œé‚£ä¹ˆè¯´æ˜è¿™ä¸ªæ•°æ®é›†çš„æ„é€ æ–¹å¼ï¼Œä¸æˆ‘ä»¬çœŸå®çš„ä¸šåŠ¡åœºæ™¯æ¯”è¾ƒæ¥è¿‘ï¼Œä¹Ÿæ˜¯éå¸¸ç¬¦åˆé¢„æœŸçš„ã€‚  
In our team's real Agent system, there is a dedicated time enhancement process, and the results are attached to the episode metadata to help accurately locate relative/absolute time in different business scenarios. In the MVP, we simplified this by directly concatenating such expressions in the episode text. If this operation helps with the LoCoMo dataset (I'm not entirely sure, as this is how we did it in the first version), it means the dataset's construction is quite close to our real business scenarios, which is very much in line with expectations.

### 5. åœ¨ä»£ç ä¸­ï¼Œæœ‰å¾ˆå¤šçœ‹èµ·æ¥ä¸çŸ¥é“ç”¨åœ¨å“ªé‡Œçš„å¤„ç†ï¼Œæ¯”å¦‚ EpisodeLevelã€Time Gap çš„è®¡ç®—ç­‰ï¼Œä¸ºä»€ä¹ˆåœ¨ä¸€ä¸ª MVP é¡¹ç›®é‡Œé¢ä¼šè®¾è®¡è¿™äº›å…ƒç´ ï¼Ÿ  
**Q: There are many elements in the code, such as EpisodeLevel and Time Gap calculation, that don't seem to be used. Why design these in an MVP project?**

50% æ˜¯ç”Ÿäº§é¡¹ç›®è¿ç§»æ—¶çš„ä¸€äº›åŠŸèƒ½æ²¡è¢«ä»”ç»†å‰¥ç¦»ï¼Œ50% æ˜¯ Claude Code è‡ªç”±å‘æŒ¥ã€‚  
50% are features not carefully stripped out during production project migration, and 50% are Claude Code's creative freedom.

### 6. æƒ…æ™¯è®°å¿†è¿™ä¸ªæ–¹æ³•æœ‰åœ¨å…¶ä»–åœºæ™¯è¯æ˜æœ‰æ•ˆæ€§å—ï¼Ÿ  
**Q: Has the episodic memory method proven effective in other scenarios?**

æˆ‘ä»¬çš„æœ¬æ„ï¼Œå°±æ˜¯åœ¨ã€ŒAI Agent ä½œä¸ºç”¨æˆ·ç¤¾äº¤/åŠå…¬çš„åŠ©ç†ã€å’Œã€Œé€šç”¨ ChatBotã€ä¸¤ä¸ªåœºæ™¯ä¸‹ï¼Œè®¾è®¡ä¸€ç§æ›´é«˜æ•ˆçš„ã€Œè®°å¿†ç´¢å¼•æ–¹å¼ã€ã€‚æ‰€ä»¥è™½ç„¶æˆ‘ä»¬æ²¡åšè¿‡å¤ªå¤šå®éªŒï¼Œä½†æ˜¯åŸºæœ¬å¯ä»¥æ¨æ–­å‡ºæ¥ï¼Œè¿™ä¸ªæ–¹æ³•æ‹¿å»åšæ–‡æ¡£åœºæ™¯ã€çŸ¥è¯†åº“åœºæ™¯çš„è®°å¿†ï¼Œåº”è¯¥æ˜¯ä¸ä¼šæœ‰ä»€ä¹ˆç›´æ¥æå‡çš„ã€‚  
Our intention is to design a more efficient "memory indexing method" for two scenarios: "AI Agent as a user's social/office assistant" and "general ChatBot." So although we haven't done many experiments, it can be basically inferred that using this method for document or knowledge base memory scenarios is unlikely to bring direct improvements.

## ç‰¹åˆ«æ„Ÿè°¢  
## Special Thanks

MemOS å›¢é˜Ÿâ€”â€”æˆ‘ä»¬ä»ä»–ä»¬çš„é¡¹ç›®ä¸­åˆ†å‰å¹¶æ‰©å±•äº†è¯„ä¼°æ¡†æ¶ä»¥æ”¯æŒ Nemori åŸºå‡†æµ‹è¯•ã€‚  
MemOS teamâ€”we forked their project and extended the evaluation framework to support Nemori benchmarking.

**Nemori** - èµ‹äºˆ AI æ™ºèƒ½ä½“é•¿æœŸè®°å¿†ä»¥é©±åŠ¨å…¶è‡ªæˆ‘è¿›åŒ– ğŸš€  
**Nemori** - Endowing AI agents with human-like episodic memory to drive their evolution ğŸš€
