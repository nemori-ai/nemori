# Nemori è¯­ä¹‰è®°å¿†è®¾è®¡æ–‡æ¡£ | Semantic Memory Design Document

## é¡¹ç›®æ¦‚è¿° | Project Overview

### English Overview

This document outlines the design and implementation strategy for adding **Semantic Memory** capabilities to the Nemori episodic memory system. Semantic memory complements episodic memory by capturing and maintaining evolving private domain knowledge that would otherwise be lost during episodic compression.

**Core Innovation**: Using episodic memory as a "knowledge mask" to automatically discover semantic information that Large Language Models (LLMs) don't possess, creating an iteratively updatable private knowledge base with relationship mapping.

### ä¸­æ–‡æ¦‚è¿°

æœ¬æ–‡æ¡£æ¦‚è¿°äº†ä¸º Nemori æƒ…æ™¯è®°å¿†ç³»ç»Ÿå¢åŠ **è¯­ä¹‰è®°å¿†**èƒ½åŠ›çš„è®¾è®¡ä¸å®ç°ç­–ç•¥ã€‚è¯­ä¹‰è®°å¿†é€šè¿‡æ•è·å’Œç»´æŠ¤åœ¨æƒ…æ™¯å‹ç¼©è¿‡ç¨‹ä¸­å¯èƒ½ä¸¢å¤±çš„æ¼”å˜ä¸­çš„ç§åŸŸçŸ¥è¯†ï¼Œæ¥è¡¥å……æƒ…æ™¯è®°å¿†çš„ä¸è¶³ã€‚

**æ ¸å¿ƒåˆ›æ–°**ï¼šä½¿ç”¨æƒ…æ™¯è®°å¿†ä½œä¸º"çŸ¥è¯†æ©ç "ï¼Œè‡ªåŠ¨å‘ç°å¤§è¯­è¨€æ¨¡å‹ä¸å…·å¤‡çš„è¯­ä¹‰ä¿¡æ¯ï¼Œåˆ›å»ºå¯è¿­ä»£æ›´æ–°çš„å¸¦å…³ç³»æ˜ å°„çš„ç§åŸŸçŸ¥è¯†åº“ã€‚

---

## è®¾è®¡ç†å¿µ | Design Philosophy

### The Knowledge Loss Problem | çŸ¥è¯†ä¸¢å¤±é—®é¢˜

**English**: Episodic memory, by design, compresses raw conversations into narrative summaries. This compression inevitably leads to the loss of detailed semantic information. The lost information falls into two categories:

1. **Common Knowledge**: Information already known to the LLM (e.g., "Python vs JavaScript advantages")
2. **Private Domain Knowledge**: User-specific information unknown to the LLM (e.g., "Tanka project details", "John's research focus")

**ä¸­æ–‡**: æƒ…æ™¯è®°å¿†è®¾è®¡ä¸Šä¼šå°†åŸå§‹å¯¹è¯å‹ç¼©ä¸ºå™è¿°æ€§æ‘˜è¦ã€‚è¿™ç§å‹ç¼©ä¸å¯é¿å…åœ°å¯¼è‡´è¯¦ç»†è¯­ä¹‰ä¿¡æ¯çš„ä¸¢å¤±ã€‚ä¸¢å¤±çš„ä¿¡æ¯åˆ†ä¸ºä¸¤ç±»ï¼š

1. **é€šç”¨çŸ¥è¯†**ï¼šå¤§è¯­è¨€æ¨¡å‹å·²çŸ¥çš„ä¿¡æ¯ï¼ˆå¦‚"Python vs JavaScript çš„ä¼˜åŠ¿"ï¼‰
2. **ç§åŸŸçŸ¥è¯†**ï¼šå¤§è¯­è¨€æ¨¡å‹æœªçŸ¥çš„ç”¨æˆ·ç‰¹å®šä¿¡æ¯ï¼ˆå¦‚"Tanka é¡¹ç›®è¯¦æƒ…"ã€"John çš„ç ”ç©¶é‡ç‚¹"ï¼‰

### The Semantic Discovery Mechanism | è¯­ä¹‰å‘ç°æœºåˆ¶

**English**: Our approach uses episodic memory as a "mask" to identify private domain knowledge through a differential reconstruction process:

1. **Masking Phase**: Episodic memory naturally masks semantic details through compression
2. **Reconstruction Phase**: LLM attempts to reconstruct original content using its world knowledge
3. **Differential Analysis**: Compare reconstructed vs. original content to identify knowledge gaps
4. **Semantic Extraction**: Knowledge gaps represent valuable private domain information

**ä¸­æ–‡**: æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨æƒ…æ™¯è®°å¿†ä½œä¸º"æ©ç "ï¼Œé€šè¿‡å·®åˆ†é‡å»ºè¿‡ç¨‹è¯†åˆ«ç§åŸŸçŸ¥è¯†ï¼š

1. **æ©ç é˜¶æ®µ**ï¼šæƒ…æ™¯è®°å¿†é€šè¿‡å‹ç¼©è‡ªç„¶åœ°æ©ç›–è¯­ä¹‰ç»†èŠ‚
2. **é‡å»ºé˜¶æ®µ**ï¼šå¤§è¯­è¨€æ¨¡å‹å°è¯•ä½¿ç”¨å…¶ä¸–ç•ŒçŸ¥è¯†é‡å»ºåŸå§‹å†…å®¹
3. **å·®åˆ†åˆ†æ**ï¼šæ¯”è¾ƒé‡å»ºå†…å®¹ä¸åŸå§‹å†…å®¹ï¼Œè¯†åˆ«çŸ¥è¯†å·®è·
4. **è¯­ä¹‰æå–**ï¼šçŸ¥è¯†å·®è·ä»£è¡¨æœ‰ä»·å€¼çš„ç§åŸŸä¿¡æ¯

---

## æ ¸å¿ƒæ¶æ„è®¾è®¡ | Core Architecture Design

### è®¾è®¡åŸåˆ™ | Design Principles

**English**: Based on the core requirements, the semantic memory system follows these principles:

1. **Gap-Driven Discovery**: All semantic memory originates from episodic memory analysis
2. **Dual Retrieval Capability**: Independent similarity-based search for both semantic and episodic memories
3. **Bidirectional ID-based Association**: Precise linking between episodes and semantic nodes, including evolved versions
4. **Context-Aware Generation**: Utilize related semantic memories and historical episodes during generation

**ä¸­æ–‡**: åŸºäºæ ¸å¿ƒéœ€æ±‚ï¼Œè¯­ä¹‰è®°å¿†ç³»ç»Ÿéµå¾ªä»¥ä¸‹åŸåˆ™ï¼š

1. **é—´éš™é©±åŠ¨å‘ç°**ï¼šæ‰€æœ‰è¯­ä¹‰è®°å¿†éƒ½æºäºæƒ…æ™¯è®°å¿†åˆ†æ
2. **åŒé‡æ£€ç´¢èƒ½åŠ›**ï¼šè¯­ä¹‰è®°å¿†å’Œæƒ…æ™¯è®°å¿†çš„ç‹¬ç«‹ç›¸ä¼¼åº¦æœç´¢
3. **åŒå‘IDå…³è”**ï¼šæƒ…æ™¯ä¸è¯­ä¹‰èŠ‚ç‚¹çš„ç²¾ç¡®é“¾æ¥ï¼ŒåŒ…æ‹¬æ¼”å˜ç‰ˆæœ¬
4. **ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç”Ÿæˆ**ï¼šç”Ÿæˆæ—¶åˆ©ç”¨ç›¸å…³è¯­ä¹‰è®°å¿†å’Œå†å²æƒ…æ™¯

### 1. è¯­ä¹‰è®°å¿†æ•°æ®ç»“æ„ | Semantic Memory Data Structures

#### SemanticNode | è¯­ä¹‰èŠ‚ç‚¹

```python
@dataclass
class SemanticNode:
    """
    Represents a single piece of semantic knowledge.
    è¡¨ç¤ºå•ä¸ªè¯­ä¹‰çŸ¥è¯†ç‰‡æ®µã€‚
    """
    # Core identification | æ ¸å¿ƒæ ‡è¯†
    node_id: str = field(default_factory=lambda: str(uuid4()))
    owner_id: str = ""
    
    # Knowledge content | çŸ¥è¯†å†…å®¹
    key: str = ""          # Knowledge key/identifier | çŸ¥è¯†é”®/æ ‡è¯†ç¬¦
    value: str = ""        # Knowledge content | çŸ¥è¯†å†…å®¹
    context: str = ""      # Original context where discovered | å‘ç°æ—¶çš„åŸå§‹ä¸Šä¸‹æ–‡
    
    # Confidence and evolution | ç½®ä¿¡åº¦ä¸æ¼”å˜
    confidence: float = 1.0           # Confidence in this knowledge | å¯¹è¯¥çŸ¥è¯†çš„ç½®ä¿¡åº¦
    version: int = 1                  # Version number for evolution | æ¼”å˜çš„ç‰ˆæœ¬å·
    evolution_history: list[str] = field(default_factory=list)  # Previous values | å†å²å€¼
    
    # Temporal information | æ—¶é—´ä¿¡æ¯
    created_at: datetime = field(default_factory=datetime.now)
    last_updated: datetime = field(default_factory=datetime.now)
    last_accessed: datetime | None = None
    
    # Discovery metadata | å‘ç°å…ƒæ•°æ®
    discovery_episode_id: str | None = None  # Episode that led to discovery | å¯¼è‡´å‘ç°çš„æƒ…æ™¯
    discovery_method: str = "diff_analysis"   # How this was discovered | å‘ç°æ–¹æ³•
    
    # Bidirectional associations | åŒå‘å…³è”
    linked_episode_ids: list[str] = field(default_factory=list)  # Episodes that reference this knowledge | å¼•ç”¨æ­¤çŸ¥è¯†çš„æƒ…æ™¯
    evolution_episode_ids: list[str] = field(default_factory=list)  # Episodes that caused evolution | å¯¼è‡´æ¼”å˜çš„æƒ…æ™¯
    
    # Search optimization | æœç´¢ä¼˜åŒ–
    search_keywords: list[str] = field(default_factory=list)  # Keywords for similarity search | ç›¸ä¼¼åº¦æœç´¢å…³é”®è¯
    embedding_vector: list[float] | None = None  # Optional vector for semantic search | å¯é€‰çš„è¯­ä¹‰æœç´¢å‘é‡
    
    # Usage statistics | ä½¿ç”¨ç»Ÿè®¡
    access_count: int = 0
    relevance_score: float = 0.0
    importance_score: float = 0.0
```

#### SemanticRelationship | è¯­ä¹‰å…³ç³»

```python
@dataclass 
class SemanticRelationship:
    """
    Represents relationships between semantic nodes.
    è¡¨ç¤ºè¯­ä¹‰èŠ‚ç‚¹ä¹‹é—´çš„å…³ç³»ã€‚
    """
    # Core identification | æ ¸å¿ƒæ ‡è¯†
    relationship_id: str = field(default_factory=lambda: str(uuid4()))
    
    # Relationship definition | å…³ç³»å®šä¹‰
    source_node_id: str = ""
    target_node_id: str = ""
    relationship_type: RelationshipType = RelationshipType.RELATED
    
    # Relationship properties | å…³ç³»å±æ€§
    strength: float = 0.5      # Relationship strength [0-1] | å…³ç³»å¼ºåº¦ [0-1]
    description: str = ""      # Optional description | å¯é€‰æè¿°
    
    # Temporal information | æ—¶é—´ä¿¡æ¯
    created_at: datetime = field(default_factory=datetime.now)
    last_reinforced: datetime = field(default_factory=datetime.now)
    
    # Discovery context | å‘ç°ä¸Šä¸‹æ–‡
    discovery_episode_id: str | None = None
    
class RelationshipType(Enum):
    """Types of semantic relationships | è¯­ä¹‰å…³ç³»ç±»å‹"""
    RELATED = "related"           # General relationship | ä¸€èˆ¬å…³ç³»
    EVOLVED_FROM = "evolved_from" # One concept evolved from another | ä¸€ä¸ªæ¦‚å¿µä»å¦ä¸€ä¸ªæ¼”å˜è€Œæ¥
    PART_OF = "part_of"          # Part-whole relationship | éƒ¨åˆ†-æ•´ä½“å…³ç³»
    SIMILAR_TO = "similar_to"    # Similarity relationship | ç›¸ä¼¼å…³ç³»
    OPPOSITE_TO = "opposite_to"  # Opposition relationship | å¯¹ç«‹å…³ç³»
    TEMPORAL = "temporal"        # Time-based relationship | åŸºäºæ—¶é—´çš„å…³ç³»
```

### 2. åŒé‡æ£€ç´¢ç³»ç»Ÿ | Dual Retrieval System

#### UnifiedRetrievalService | ç»Ÿä¸€æ£€ç´¢æœåŠ¡

```python
class UnifiedRetrievalService:
    """
    Unified service providing similarity-based retrieval for both episodic and semantic memories.
    ä¸ºæƒ…æ™¯è®°å¿†å’Œè¯­ä¹‰è®°å¿†æä¾›ç›¸ä¼¼åº¦æ£€ç´¢çš„ç»Ÿä¸€æœåŠ¡ã€‚
    """
    
    def __init__(
        self, 
        episodic_storage: EpisodeStorage,
        semantic_storage: SemanticStorage
    ):
        self.episodic_storage = episodic_storage
        self.semantic_storage = semantic_storage
    
    async def search_episodic_memories(
        self, 
        owner_id: str, 
        query: str, 
        limit: int = 10
    ) -> list[Episode]:
        """
        Independent similarity search for episodic memories.
        æƒ…æ™¯è®°å¿†çš„ç‹¬ç«‹ç›¸ä¼¼åº¦æœç´¢ã€‚
        """
        return await self.episodic_storage.similarity_search(owner_id, query, limit)
    
    async def search_semantic_memories(
        self, 
        owner_id: str, 
        query: str, 
        limit: int = 10
    ) -> list[SemanticNode]:
        """
        Independent similarity search for semantic memories.
        è¯­ä¹‰è®°å¿†çš„ç‹¬ç«‹ç›¸ä¼¼åº¦æœç´¢ã€‚
        """
        return await self.semantic_storage.similarity_search(owner_id, query, limit)
    
    async def get_episode_semantics(self, episode_id: str) -> list[SemanticNode]:
        """
        Get all semantic nodes discovered from a specific episode.
        è·å–ä»ç‰¹å®šæƒ…æ™¯å‘ç°çš„æ‰€æœ‰è¯­ä¹‰èŠ‚ç‚¹ã€‚
        """
        return await self.semantic_storage.find_by_discovery_episode(episode_id)
    
    async def get_semantic_episodes(self, semantic_node_id: str) -> dict[str, list[Episode]]:
        """
        Get all episodes associated with a semantic node, including evolution history.
        è·å–ä¸è¯­ä¹‰èŠ‚ç‚¹å…³è”çš„æ‰€æœ‰æƒ…æ™¯ï¼ŒåŒ…æ‹¬æ¼”å˜å†å²ã€‚
        
        Returns:
            {
                "linked_episodes": [episodes that reference this knowledge],
                "evolution_episodes": [episodes that caused knowledge evolution]
            }
        """
        semantic_node = await self.semantic_storage.get_by_id(semantic_node_id)
        if not semantic_node:
            return {"linked_episodes": [], "evolution_episodes": []}
        
        linked_episodes = await self.episodic_storage.get_by_ids(semantic_node.linked_episode_ids)
        evolution_episodes = await self.episodic_storage.get_by_ids(semantic_node.evolution_episode_ids)
        
        return {
            "linked_episodes": linked_episodes,
            "evolution_episodes": evolution_episodes
        }
```

### 3. è¯­ä¹‰å‘ç°å¼•æ“ | Semantic Discovery Engine

#### ContextAwareSemanticDiscoveryEngine | ä¸Šä¸‹æ–‡æ„ŸçŸ¥è¯­ä¹‰å‘ç°å¼•æ“

```python
class ContextAwareSemanticDiscoveryEngine:
    """
    Context-aware engine for discovering semantic knowledge through differential analysis.
    é€šè¿‡å·®åˆ†åˆ†æå‘ç°è¯­ä¹‰çŸ¥è¯†çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥å¼•æ“ã€‚
    """
    
    def __init__(
        self, 
        llm_provider: LLMProvider,
        retrieval_service: UnifiedRetrievalService
    ):
        self.llm_provider = llm_provider
        self.retrieval_service = retrieval_service
        
    async def discover_semantic_knowledge(
        self, 
        episode: Episode, 
        original_content: str
    ) -> list[SemanticNode]:
        """
        Discover semantic knowledge with context from related memories.
        åˆ©ç”¨ç›¸å…³è®°å¿†çš„ä¸Šä¸‹æ–‡å‘ç°è¯­ä¹‰çŸ¥è¯†ã€‚
        
        Process | æµç¨‹:
        1. Gather related semantic memories and historical episodes | æ”¶é›†ç›¸å…³è¯­ä¹‰è®°å¿†å’Œå†å²æƒ…æ™¯
        2. Use episode as mask to reconstruct original | ä½¿ç”¨æƒ…æ™¯ä½œä¸ºæ©ç é‡å»ºåŸå§‹å†…å®¹
        3. Compare reconstructed vs original with context | ç»“åˆä¸Šä¸‹æ–‡æ¯”è¾ƒé‡å»ºä¸åŸå§‹å†…å®¹
        4. Extract knowledge gaps as semantic nodes | æå–çŸ¥è¯†å·®è·ä½œä¸ºè¯­ä¹‰èŠ‚ç‚¹
        """
        # Step 1: Gather context from related memories
        # æ­¥éª¤1ï¼šä»ç›¸å…³è®°å¿†ä¸­æ”¶é›†ä¸Šä¸‹æ–‡
        context = await self._gather_discovery_context(episode)
        
        # Step 2: Context-aware reconstruction
        # æ­¥éª¤2ï¼šä¸Šä¸‹æ–‡æ„ŸçŸ¥é‡å»º
        reconstructed_content = await self._reconstruct_with_context(episode, context)
        
        # Step 3: Perform differential analysis
        # æ­¥éª¤3ï¼šæ‰§è¡Œå·®åˆ†åˆ†æ
        knowledge_gaps = await self._analyze_knowledge_gaps(
            original=original_content,
            reconstructed=reconstructed_content,
            episode=episode,
            context=context
        )
        
        # Step 4: Create semantic nodes with bidirectional links
        # æ­¥éª¤4ï¼šåˆ›å»ºå¸¦åŒå‘é“¾æ¥çš„è¯­ä¹‰èŠ‚ç‚¹
        semantic_nodes = []
        for gap in knowledge_gaps:
            node = SemanticNode(
                owner_id=episode.owner_id,
                key=gap.get("key", ""),
                value=gap.get("value", ""),
                context=gap.get("context", ""),
                discovery_episode_id=episode.episode_id,
                linked_episode_ids=[episode.episode_id],  # Initial linking
                confidence=gap.get("confidence", 1.0),
                search_keywords=self._extract_keywords(gap)
            )
            semantic_nodes.append(node)
            
        return semantic_nodes
    
    async def _gather_discovery_context(self, episode: Episode) -> dict[str, Any]:
        """
        Gather related semantic memories and historical episodes for context.
        æ”¶é›†ç›¸å…³è¯­ä¹‰è®°å¿†å’Œå†å²æƒ…æ™¯ä½œä¸ºä¸Šä¸‹æ–‡ã€‚
        """
        # Search for related semantic memories
        # æœç´¢ç›¸å…³è¯­ä¹‰è®°å¿†
        related_semantics = await self.retrieval_service.search_semantic_memories(
            owner_id=episode.owner_id,
            query=f"{episode.title} {episode.summary}",
            limit=5
        )
        
        # Search for related historical episodes
        # æœç´¢ç›¸å…³å†å²æƒ…æ™¯
        related_episodes = await self.retrieval_service.search_episodic_memories(
            owner_id=episode.owner_id,
            query=episode.content,
            limit=3
        )
        
        return {
            "related_semantic_memories": related_semantics,
            "related_historical_episodes": related_episodes,
            "current_episode": episode
        }
```

#### Differential Analysis Prompts | å·®åˆ†åˆ†ææç¤ºè¯

```python
RECONSTRUCTION_PROMPT = """
You are an expert at reconstructing original conversations from episodic summaries.
æ‚¨æ˜¯ä»æƒ…æ™¯æ‘˜è¦é‡å»ºåŸå§‹å¯¹è¯çš„ä¸“å®¶ã€‚

Given this episodic memory:
ç»™å®šä»¥ä¸‹æƒ…æ™¯è®°å¿†ï¼š
{episode_content}

Please reconstruct what the original conversation might have looked like, using your general world knowledge.
è¯·ä½¿ç”¨æ‚¨çš„é€šç”¨ä¸–ç•ŒçŸ¥è¯†é‡å»ºåŸå§‹å¯¹è¯å¯èƒ½çš„æ ·å­ã€‚

Important guidelines | é‡è¦å‡†åˆ™:
1. Use only common knowledge that a typical LLM would know | åªä½¿ç”¨å…¸å‹å¤§è¯­è¨€æ¨¡å‹ä¼šçŸ¥é“çš„å¸¸è¯†
2. Make reasonable assumptions for missing details | å¯¹ç¼ºå¤±ç»†èŠ‚åšåˆç†å‡è®¾
3. Focus on factual reconstruction, not creative interpretation | ä¸“æ³¨äºäº‹å®é‡å»ºï¼Œè€Œéåˆ›æ„è§£é‡Š
4. Maintain the same conversation structure and flow | ä¿æŒç›¸åŒçš„å¯¹è¯ç»“æ„å’Œæµç¨‹

Return the reconstructed conversation:
è¿”å›é‡å»ºçš„å¯¹è¯ï¼š
"""

KNOWLEDGE_GAP_ANALYSIS_PROMPT = """
You are an expert at identifying private domain knowledge gaps.
æ‚¨æ˜¯è¯†åˆ«ç§åŸŸçŸ¥è¯†å·®è·çš„ä¸“å®¶ã€‚

Original content | åŸå§‹å†…å®¹:
{original_content}

Reconstructed content (using general LLM knowledge) | é‡å»ºå†…å®¹ï¼ˆä½¿ç”¨é€šç”¨å¤§è¯­è¨€æ¨¡å‹çŸ¥è¯†ï¼‰:
{reconstructed_content}

Please identify specific pieces of information that exist in the original but are missing or incorrectly assumed in the reconstruction. These represent private domain knowledge.
è¯·è¯†åˆ«åŸå§‹å†…å®¹ä¸­å­˜åœ¨ä½†åœ¨é‡å»ºä¸­ç¼ºå¤±æˆ–é”™è¯¯å‡è®¾çš„å…·ä½“ä¿¡æ¯ã€‚è¿™äº›ä»£è¡¨ç§åŸŸçŸ¥è¯†ã€‚

Focus on | å…³æ³¨:
1. Proper names, project names, specific terminology | ä¸“æœ‰åè¯ã€é¡¹ç›®åç§°ã€ç‰¹å®šæœ¯è¯­
2. Personal preferences, habits, and characteristics | ä¸ªäººåå¥½ã€ä¹ æƒ¯å’Œç‰¹å¾
3. Specific facts, dates, numbers that differ | å…·ä½“çš„äº‹å®ã€æ—¥æœŸã€æ•°å­—å·®å¼‚
4. Context-specific meanings and interpretations | ä¸Šä¸‹æ–‡ç‰¹å®šçš„å«ä¹‰å’Œè§£é‡Š

Return your analysis in JSON format:
ä»¥ JSON æ ¼å¼è¿”å›åˆ†æï¼š
{
    "knowledge_gaps": [
        {
            "key": "specific identifier or topic",
            "value": "the correct private knowledge",
            "context": "surrounding context from original",
            "gap_type": "proper_noun|personal_fact|specific_detail|contextual_meaning",
            "confidence": 0.0-1.0
        }
    ]
}
"""
```

### 4. è¯­ä¹‰è®°å¿†æ¼”å˜ç®¡ç†å™¨ | Semantic Memory Evolution Manager

#### SemanticEvolutionManager | è¯­ä¹‰æ¼”å˜ç®¡ç†å™¨

```python
class SemanticEvolutionManager:
    """
    Manages semantic memory evolution with bidirectional episode associations.
    ç®¡ç†è¯­ä¹‰è®°å¿†æ¼”å˜åŠåŒå‘æƒ…æ™¯å…³è”ã€‚
    """
    
    def __init__(
        self, 
        storage: SemanticStorage, 
        discovery_engine: ContextAwareSemanticDiscoveryEngine,
        retrieval_service: UnifiedRetrievalService
    ):
        self.storage = storage
        self.discovery_engine = discovery_engine
        self.retrieval_service = retrieval_service
        
    async def process_episode_for_semantics(
        self, 
        episode: Episode, 
        original_content: str
    ) -> list[SemanticNode]:
        """
        Process an episode to discover and update semantic knowledge with full context.
        å¤„ç†æƒ…æ™¯ä»¥å‘ç°å’Œæ›´æ–°è¯­ä¹‰çŸ¥è¯†ï¼ŒåŒ…å«å®Œæ•´ä¸Šä¸‹æ–‡ã€‚
        """
        # Discover new semantic knowledge with context
        # åˆ©ç”¨ä¸Šä¸‹æ–‡å‘ç°æ–°çš„è¯­ä¹‰çŸ¥è¯†
        discovered_nodes = await self.discovery_engine.discover_semantic_knowledge(
            episode, original_content
        )
        
        # Process each discovered node for evolution and linking
        # å¤„ç†æ¯ä¸ªå‘ç°çš„èŠ‚ç‚¹ä»¥è¿›è¡Œæ¼”å˜å’Œå…³è”
        processed_nodes = []
        for new_node in discovered_nodes:
            processed_node = await self._process_semantic_node(new_node, episode)
            processed_nodes.append(processed_node)
            
        return processed_nodes
        
    async def _process_semantic_node(
        self, 
        new_node: SemanticNode, 
        episode: Episode
    ) -> SemanticNode:
        """
        Process a semantic node for evolution, linking, and storage.
        å¤„ç†è¯­ä¹‰èŠ‚ç‚¹ä»¥è¿›è¡Œæ¼”å˜ã€å…³è”å’Œå­˜å‚¨ã€‚
        """
        # Check if knowledge already exists
        # æ£€æŸ¥çŸ¥è¯†æ˜¯å¦å·²å­˜åœ¨
        existing_node = await self.storage.find_semantic_node_by_key(
            owner_id=new_node.owner_id, 
            key=new_node.key
        )
        
        if existing_node:
            # Handle evolution of existing knowledge
            # å¤„ç†ç°æœ‰çŸ¥è¯†çš„æ¼”å˜
            return await self._evolve_semantic_knowledge(existing_node, new_node, episode)
        else:
            # Store new knowledge with bidirectional linking
            # å­˜å‚¨æ–°çŸ¥è¯†å¹¶å»ºç«‹åŒå‘é“¾æ¥
            await self.storage.store_semantic_node(new_node)
            await self._establish_bidirectional_links(new_node, episode)
            return new_node
    
    async def _evolve_semantic_knowledge(
        self, 
        existing: SemanticNode, 
        new: SemanticNode, 
        episode: Episode
    ) -> SemanticNode:
        """
        Evolve existing semantic knowledge with comprehensive evolution tracking.
        ç”¨ç»¼åˆæ¼”å˜è·Ÿè¸ªæ¼”å˜ç°æœ‰è¯­ä¹‰çŸ¥è¯†ã€‚
        """
        if existing.value != new.value:
            # Knowledge evolution detected
            # æ£€æµ‹åˆ°çŸ¥è¯†æ¼”å˜
            evolved_node = replace(existing,
                value=new.value,
                context=new.context,  # Update context
                version=existing.version + 1,
                evolution_history=existing.evolution_history + [existing.value],
                evolution_episode_ids=existing.evolution_episode_ids + [episode.episode_id],
                last_updated=datetime.now(),
                confidence=(existing.confidence + new.confidence) / 2
            )
            
            # Update storage
            # æ›´æ–°å­˜å‚¨
            await self.storage.update_semantic_node(evolved_node)
            
            # Link this episode as an evolution trigger
            # å°†æ­¤æƒ…æ™¯é“¾æ¥ä¸ºæ¼”å˜è§¦å‘å™¨
            await self._establish_bidirectional_links(evolved_node, episode)
            
            return evolved_node
        else:
            # Reinforce existing knowledge without evolution
            # å¼ºåŒ–ç°æœ‰çŸ¥è¯†ä½†ä¸æ¼”å˜
            reinforced_node = replace(existing,
                linked_episode_ids=list(set(existing.linked_episode_ids + [episode.episode_id])),
                confidence=min(1.0, existing.confidence + 0.1),
                last_accessed=datetime.now(),
                access_count=existing.access_count + 1
            )
            
            await self.storage.update_semantic_node(reinforced_node)
            await self._establish_bidirectional_links(reinforced_node, episode)
            
            return reinforced_node
    
    async def _establish_bidirectional_links(
        self, 
        semantic_node: SemanticNode, 
        episode: Episode
    ) -> None:
        """
        Establish bidirectional links between semantic node and episode.
        åœ¨è¯­ä¹‰èŠ‚ç‚¹å’Œæƒ…æ™¯ä¹‹é—´å»ºç«‹åŒå‘é“¾æ¥ã€‚
        """
        # Update episode to reference this semantic node
        # æ›´æ–°æƒ…æ™¯ä»¥å¼•ç”¨æ­¤è¯­ä¹‰èŠ‚ç‚¹
        if "semantic_node_ids" not in episode.metadata.custom_fields:
            episode.metadata.custom_fields["semantic_node_ids"] = []
        
        if semantic_node.node_id not in episode.metadata.custom_fields["semantic_node_ids"]:
            episode.metadata.custom_fields["semantic_node_ids"].append(semantic_node.node_id)
            
        # Note: Episode storage update would happen in the calling context
        # æ³¨æ„ï¼šæƒ…æ™¯å­˜å‚¨æ›´æ–°ä¼šåœ¨è°ƒç”¨ä¸Šä¸‹æ–‡ä¸­å‘ç”Ÿ
    
    async def get_semantic_evolution_history(
        self, 
        semantic_node_id: str
    ) -> dict[str, Any]:
        """
        Get comprehensive evolution history including all related episodes.
        è·å–åŒ…æ‹¬æ‰€æœ‰ç›¸å…³æƒ…æ™¯çš„ç»¼åˆæ¼”å˜å†å²ã€‚
        
        Returns:
            {
                "node": current_semantic_node,
                "evolution_timeline": [
                    {
                        "version": 1,
                        "value": "historical_value",
                        "episode": episode_that_caused_change,
                        "timestamp": change_timestamp
                    }
                ],
                "linked_episodes": [episodes that reference this knowledge],
                "evolution_episodes": [episodes that caused evolution]
            }
        """
        semantic_node = await self.storage.get_by_id(semantic_node_id)
        if not semantic_node:
            return {}
        
        # Build evolution timeline
        # æ„å»ºæ¼”å˜æ—¶é—´çº¿
        evolution_timeline = []
        
        # Add historical versions
        # æ·»åŠ å†å²ç‰ˆæœ¬
        for i, historical_value in enumerate(semantic_node.evolution_history):
            episode_id = semantic_node.evolution_episode_ids[i] if i < len(semantic_node.evolution_episode_ids) else None
            episode = None
            if episode_id:
                episodes = await self.retrieval_service.episodic_storage.get_by_ids([episode_id])
                episode = episodes[0] if episodes else None
            
            evolution_timeline.append({
                "version": i + 1,
                "value": historical_value,
                "episode": episode,
                "timestamp": episode.temporal_info.timestamp if episode else semantic_node.created_at
            })
        
        # Add current version
        # æ·»åŠ å½“å‰ç‰ˆæœ¬
        evolution_timeline.append({
            "version": semantic_node.version,
            "value": semantic_node.value,
            "episode": None,  # Current version
            "timestamp": semantic_node.last_updated
        })
        
        # Get associated episodes
        # è·å–å…³è”æƒ…æ™¯
        associated_episodes = await self.retrieval_service.get_semantic_episodes(semantic_node_id)
        
        return {
            "node": semantic_node,
            "evolution_timeline": evolution_timeline,
            "linked_episodes": associated_episodes["linked_episodes"],
            "evolution_episodes": associated_episodes["evolution_episodes"]
        }
```

### 4. å…³ç³»å‘ç°ä¸ç®¡ç† | Relationship Discovery and Management

#### RelationshipDiscoveryEngine | å…³ç³»å‘ç°å¼•æ“

```python
class RelationshipDiscoveryEngine:
    """
    Discovers and manages relationships between semantic nodes.
    å‘ç°å’Œç®¡ç†è¯­ä¹‰èŠ‚ç‚¹ä¹‹é—´çš„å…³ç³»ã€‚
    """
    
    async def discover_relationships(
        self, 
        nodes: list[SemanticNode], 
        context_episode: Episode
    ) -> list[SemanticRelationship]:
        """
        Discover relationships between semantic nodes based on context.
        åŸºäºä¸Šä¸‹æ–‡å‘ç°è¯­ä¹‰èŠ‚ç‚¹ä¹‹é—´çš„å…³ç³»ã€‚
        """
        relationships = []
        
        # Simple bidirectional association discovery
        # ç®€å•çš„åŒå‘å…³è”å‘ç°
        for i, node_a in enumerate(nodes):
            for node_b in nodes[i+1:]:
                relationship_strength = await self._calculate_relationship_strength(
                    node_a, node_b, context_episode
                )
                
                if relationship_strength > 0.3:  # Threshold for meaningful relationship
                    relationship = SemanticRelationship(
                        source_node_id=node_a.node_id,
                        target_node_id=node_b.node_id,
                        relationship_type=RelationshipType.RELATED,
                        strength=relationship_strength,
                        discovery_episode_id=context_episode.episode_id,
                        description=f"Co-discovered in episode: {context_episode.title}"
                    )
                    relationships.append(relationship)
                    
        return relationships
        
    async def _calculate_relationship_strength(
        self, 
        node_a: SemanticNode, 
        node_b: SemanticNode, 
        context: Episode
    ) -> float:
        """
        Calculate relationship strength between two semantic nodes.
        è®¡ç®—ä¸¤ä¸ªè¯­ä¹‰èŠ‚ç‚¹ä¹‹é—´çš„å…³ç³»å¼ºåº¦ã€‚
        
        Simple implementation - can be enhanced with NLP similarity analysis.
        ç®€å•å®ç° - å¯é€šè¿‡ NLP ç›¸ä¼¼æ€§åˆ†æå¢å¼ºã€‚
        """
        # Temporal proximity
        # æ—¶é—´é‚»è¿‘æ€§
        time_factor = 1.0 if node_a.created_at == node_b.created_at else 0.7
        
        # Context similarity (same episode)
        # ä¸Šä¸‹æ–‡ç›¸ä¼¼æ€§ï¼ˆç›¸åŒæƒ…æ™¯ï¼‰
        context_factor = 1.0 if node_a.discovery_episode_id == node_b.discovery_episode_id else 0.5
        
        # Textual similarity (basic implementation)
        # æ–‡æœ¬ç›¸ä¼¼æ€§ï¼ˆåŸºæœ¬å®ç°ï¼‰
        text_similarity = self._calculate_text_similarity(node_a.context, node_b.context)
        
        return (time_factor * 0.3 + context_factor * 0.4 + text_similarity * 0.3)
```

---

## å­˜å‚¨å±‚è®¾è®¡ | Storage Layer Design

### SemanticStorage Interface | è¯­ä¹‰å­˜å‚¨æ¥å£

```python
class SemanticStorage(ABC):
    """Abstract interface for semantic memory storage."""
    
    @abstractmethod
    async def store_semantic_node(self, node: SemanticNode) -> None:
        """Store a semantic node."""
        pass
        
    @abstractmethod
    async def find_semantic_node_by_key(self, owner_id: str, key: str) -> SemanticNode | None:
        """Find semantic node by key."""
        pass
        
    @abstractmethod
    async def store_relationship(self, relationship: SemanticRelationship) -> None:
        """Store a semantic relationship."""
        pass
        
    @abstractmethod
    async def find_related_nodes(self, node_id: str) -> list[tuple[SemanticNode, SemanticRelationship]]:
        """Find nodes related to given node."""
        pass
        
    @abstractmethod
    async def query_semantic_knowledge(self, owner_id: str, query: str) -> list[SemanticNode]:
        """Query semantic knowledge with text search."""
        pass
```

### Database Schema | æ•°æ®åº“æ¨¡å¼

#### PostgreSQL Implementation | PostgreSQL å®ç°

```sql
-- Semantic nodes table | è¯­ä¹‰èŠ‚ç‚¹è¡¨
CREATE TABLE semantic_nodes (
    node_id VARCHAR(36) PRIMARY KEY,
    owner_id VARCHAR(255) NOT NULL,
    key VARCHAR(500) NOT NULL,
    value TEXT NOT NULL,
    context TEXT,
    confidence FLOAT DEFAULT 1.0,
    version INTEGER DEFAULT 1,
    evolution_history JSONB DEFAULT '[]',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_accessed TIMESTAMP,
    discovery_episode_id VARCHAR(36),
    discovery_method VARCHAR(50) DEFAULT 'diff_analysis',
    access_count INTEGER DEFAULT 0,
    relevance_score FLOAT DEFAULT 0.0,
    importance_score FLOAT DEFAULT 0.0,
    
    UNIQUE(owner_id, key)
);

-- Semantic relationships table | è¯­ä¹‰å…³ç³»è¡¨
CREATE TABLE semantic_relationships (
    relationship_id VARCHAR(36) PRIMARY KEY,
    source_node_id VARCHAR(36) NOT NULL,
    target_node_id VARCHAR(36) NOT NULL,
    relationship_type VARCHAR(50) NOT NULL,
    strength FLOAT DEFAULT 0.5,
    description TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_reinforced TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    discovery_episode_id VARCHAR(36),
    
    FOREIGN KEY (source_node_id) REFERENCES semantic_nodes(node_id) ON DELETE CASCADE,
    FOREIGN KEY (target_node_id) REFERENCES semantic_nodes(node_id) ON DELETE CASCADE,
    UNIQUE(source_node_id, target_node_id, relationship_type)
);

-- Indexes for performance | æ€§èƒ½ç´¢å¼•
CREATE INDEX idx_semantic_nodes_owner_key ON semantic_nodes(owner_id, key);
CREATE INDEX idx_semantic_nodes_owner_updated ON semantic_nodes(owner_id, last_updated);
CREATE INDEX idx_semantic_relationships_source ON semantic_relationships(source_node_id);
CREATE INDEX idx_semantic_relationships_target ON semantic_relationships(target_node_id);

-- Full-text search | å…¨æ–‡æœç´¢
CREATE INDEX idx_semantic_nodes_search ON semantic_nodes USING gin(to_tsvector('english', key || ' ' || value || ' ' || context));
```

---

## æ£€ç´¢å±‚è®¾è®¡ | Retrieval Layer Design

### SemanticRetrievalService | è¯­ä¹‰æ£€ç´¢æœåŠ¡

```python
class SemanticRetrievalService:
    """
    Service for retrieving semantic knowledge.
    è¯­ä¹‰çŸ¥è¯†æ£€ç´¢æœåŠ¡ã€‚
    """
    
    def __init__(self, storage: SemanticStorage):
        self.storage = storage
        
    async def query_semantic_knowledge(
        self, 
        owner_id: str, 
        query: str, 
        limit: int = 10
    ) -> list[SemanticNode]:
        """
        Query semantic knowledge with text search.
        ä½¿ç”¨æ–‡æœ¬æœç´¢æŸ¥è¯¢è¯­ä¹‰çŸ¥è¯†ã€‚
        """
        # Basic text search implementation
        # åŸºæœ¬æ–‡æœ¬æœç´¢å®ç°
        results = await self.storage.query_semantic_knowledge(owner_id, query, limit)
        
        # Sort by relevance and recency
        # æŒ‰ç›¸å…³æ€§å’Œæ—¶æ•ˆæ€§æ’åº
        sorted_results = sorted(results, 
            key=lambda x: (x.relevance_score, x.last_updated), 
            reverse=True
        )
        
        return sorted_results[:limit]
        
    async def get_related_knowledge(
        self, 
        node_id: str, 
        max_depth: int = 2
    ) -> dict[str, list[SemanticNode]]:
        """
        Get related knowledge nodes with specified depth.
        è·å–æŒ‡å®šæ·±åº¦çš„ç›¸å…³çŸ¥è¯†èŠ‚ç‚¹ã€‚
        """
        visited = set()
        result = {"direct": [], "indirect": []}
        
        # Get direct relationships
        # è·å–ç›´æ¥å…³ç³»
        direct_related = await self.storage.find_related_nodes(node_id)
        result["direct"] = [node for node, _ in direct_related]
        visited.add(node_id)
        
        # Get indirect relationships if max_depth > 1
        # å¦‚æœ max_depth > 1 åˆ™è·å–é—´æ¥å…³ç³»
        if max_depth > 1:
            for node, _ in direct_related:
                if node.node_id not in visited:
                    visited.add(node.node_id)
                    indirect_related = await self.storage.find_related_nodes(node.node_id)
                    for indirect_node, _ in indirect_related:
                        if indirect_node.node_id not in visited:
                            result["indirect"].append(indirect_node)
                            
        return result
        
    async def get_knowledge_evolution(
        self, 
        owner_id: str, 
        key: str
    ) -> list[dict[str, Any]]:
        """
        Get evolution history of a specific knowledge key.
        è·å–ç‰¹å®šçŸ¥è¯†é”®çš„æ¼”å˜å†å²ã€‚
        """
        node = await self.storage.find_semantic_node_by_key(owner_id, key)
        if not node:
            return []
            
        evolution = []
        
        # Add historical versions
        # æ·»åŠ å†å²ç‰ˆæœ¬
        for i, historical_value in enumerate(node.evolution_history):
            evolution.append({
                "version": i + 1,
                "value": historical_value,
                "timestamp": node.created_at,  # Approximate - could be enhanced
                "confidence": 1.0  # Historical confidence unknown
            })
            
        # Add current version
        # æ·»åŠ å½“å‰ç‰ˆæœ¬
        evolution.append({
            "version": node.version,
            "value": node.value,
            "timestamp": node.last_updated,
            "confidence": node.confidence
        })
        
        return evolution
```

---

## é›†æˆç­–ç•¥ | Integration Strategy

### Integration with Existing Nemori Architecture | ä¸ç°æœ‰ Nemori æ¶æ„çš„é›†æˆ

#### 1. Episode Builder Enhancement | æƒ…æ™¯æ„å»ºå™¨å¢å¼º

```python
class EnhancedConversationEpisodeBuilder(ConversationEpisodeBuilder):
    """
    Enhanced conversation builder with semantic memory integration.
    é›†æˆè¯­ä¹‰è®°å¿†çš„å¢å¼ºå¯¹è¯æ„å»ºå™¨ã€‚
    """
    
    def __init__(
        self, 
        llm_provider: LLMProvider | None = None,
        semantic_manager: SemanticMemoryManager | None = None,
        **kwargs
    ):
        super().__init__(llm_provider, **kwargs)
        self.semantic_manager = semantic_manager
        
    async def build_episode(self, data: RawEventData, for_owner: str) -> Episode:
        """
        Build episode and process semantic knowledge.
        æ„å»ºæƒ…æ™¯å¹¶å¤„ç†è¯­ä¹‰çŸ¥è¯†ã€‚
        """
        # Build episode using parent method
        # ä½¿ç”¨çˆ¶æ–¹æ³•æ„å»ºæƒ…æ™¯
        episode = await super().build_episode(data, for_owner)
        
        # Process semantic knowledge if manager available
        # å¦‚æœç®¡ç†å™¨å¯ç”¨åˆ™å¤„ç†è¯­ä¹‰çŸ¥è¯†
        if self.semantic_manager and isinstance(data, ConversationData):
            original_content = data.get_conversation_text(include_timestamps=True)
            
            # Discover and store semantic knowledge
            # å‘ç°å¹¶å­˜å‚¨è¯­ä¹‰çŸ¥è¯†
            semantic_nodes = await self.semantic_manager.process_episode_for_semantics(
                episode, original_content
            )
            
            # Add semantic metadata to episode
            # å‘æƒ…æ™¯æ·»åŠ è¯­ä¹‰å…ƒæ•°æ®
            if semantic_nodes:
                episode.metadata.custom_fields["discovered_semantics"] = len(semantic_nodes)
                episode.metadata.custom_fields["semantic_node_ids"] = [
                    node.node_id for node in semantic_nodes
                ]
                
        return episode
```

#### 2. Memory Retrieval Enhancement | è®°å¿†æ£€ç´¢å¢å¼º

```python
class EnhancedRetrievalService:
    """
    Enhanced retrieval combining episodic and semantic memory.
    ç»“åˆæƒ…æ™¯å’Œè¯­ä¹‰è®°å¿†çš„å¢å¼ºæ£€ç´¢æœåŠ¡ã€‚
    """
    
    def __init__(
        self, 
        episodic_retrieval: RetrievalService,
        semantic_retrieval: SemanticRetrievalService
    ):
        self.episodic_retrieval = episodic_retrieval
        self.semantic_retrieval = semantic_retrieval
        
    async def enhanced_query(
        self, 
        owner_id: str, 
        query: str, 
        include_semantic: bool = True
    ) -> dict[str, Any]:
        """
        Enhanced query combining episodic and semantic results.
        ç»“åˆæƒ…æ™¯å’Œè¯­ä¹‰ç»“æœçš„å¢å¼ºæŸ¥è¯¢ã€‚
        """
        results = {
            "episodes": await self.episodic_retrieval.search_episodes(owner_id, query),
            "semantic_knowledge": []
        }
        
        if include_semantic:
            results["semantic_knowledge"] = await self.semantic_retrieval.query_semantic_knowledge(
                owner_id, query
            )
            
        return results
```


---

## ä¸šåŠ¡ä½¿ç”¨ç¤ºä¾‹ | Business Usage Examples

### å®Œæ•´çš„è¯­ä¹‰è®°å¿†ç”Ÿå‘½å‘¨æœŸ | Complete Semantic Memory Lifecycle

**åœºæ™¯ï¼šç”¨æˆ· John çš„ç ”ç©¶æ–¹å‘æ¼”å˜**

#### ç¬¬ä¸€æ¬¡å¯¹è¯ | First Conversation
```python
# åŸå§‹å¯¹è¯
original_conversation = """
[2024-01-15 10:00] John: æˆ‘æœ€è¿‘åœ¨ç ”ç©¶å¤§è¯­è¨€æ¨¡å‹çš„æç¤ºå·¥ç¨‹
[2024-01-15 10:01] Assistant: å¾ˆæœ‰è¶£çš„æ–¹å‘ï¼ä½ ä¸»è¦å…³æ³¨å“ªäº›æ–¹é¢ï¼Ÿ
[2024-01-15 10:02] John: ä¸»è¦æ˜¯å¦‚ä½•è®©LLMæ›´å¥½åœ°ç†è§£å¤æ‚æŒ‡ä»¤
"""

# æƒ…æ™¯è®°å¿†ï¼ˆå‹ç¼©åï¼‰
episode_1 = Episode(
    title="Johnè®¨è®ºå¤§è¯­è¨€æ¨¡å‹æç¤ºå·¥ç¨‹ç ”ç©¶",
    content="Johnè¡¨ç¤ºæœ€è¿‘åœ¨ç ”ç©¶å¤§è¯­è¨€æ¨¡å‹çš„æç¤ºå·¥ç¨‹ï¼Œç‰¹åˆ«å…³æ³¨å¦‚ä½•è®©LLMæ›´å¥½åœ°ç†è§£å¤æ‚æŒ‡ä»¤ã€‚",
    owner_id="john"
)

# è¯­ä¹‰å‘ç°è¿‡ç¨‹
# LLMé‡å»ºï¼šJohnæåˆ°ç ”ç©¶æŸç§AIæŠ€æœ¯ï¼Œå¯èƒ½æ˜¯æœºå™¨å­¦ä¹ ç›¸å…³
# å·®åˆ†å‘ç°ï¼š
semantic_node_1 = SemanticNode(
    key="Johnçš„ç ”ç©¶æ–¹å‘",
    value="å¤§è¯­è¨€æ¨¡å‹æç¤ºå·¥ç¨‹",
    context="ä¸“æ³¨äºè®©LLMç†è§£å¤æ‚æŒ‡ä»¤",
    discovery_episode_id=episode_1.episode_id,
    linked_episode_ids=[episode_1.episode_id]
)
```

#### ä¸‰ä¸ªæœˆåçš„å¯¹è¯ | Conversation After 3 Months
```python
# åŸå§‹å¯¹è¯
original_conversation_2 = """
[2024-04-20 14:30] John: æˆ‘ç°åœ¨è½¬å‘AI Agentçš„è¡Œä¸ºè§„åˆ’äº†
[2024-04-20 14:31] Assistant: ä»LLMè½¬å‘Agentäº†ï¼Ÿ
[2024-04-20 14:32] John: å¯¹ï¼Œå‘ç°Agentçš„å†³ç­–æœºåˆ¶æ›´æœ‰æŒ‘æˆ˜æ€§
"""

# æƒ…æ™¯è®°å¿†
episode_2 = Episode(
    title="Johnè½¬å‘AI Agentè¡Œä¸ºè§„åˆ’ç ”ç©¶",
    content="Johnè¡¨ç¤ºå·²ä»ä¹‹å‰çš„LLMç ”ç©¶è½¬å‘AI Agentçš„è¡Œä¸ºè§„åˆ’ï¼Œè®¤ä¸ºAgentçš„å†³ç­–æœºåˆ¶æ›´æœ‰æŒ‘æˆ˜æ€§ã€‚",
    owner_id="john"
)

# è¯­ä¹‰æ¼”å˜è¿‡ç¨‹
# å‘ç°åŒä¸€ä¸ªkeyçš„ä¸åŒvalue
evolved_semantic_node = SemanticNode(
    key="Johnçš„ç ”ç©¶æ–¹å‘",
    value="AI Agentè¡Œä¸ºè§„åˆ’",  # æ–°å€¼
    context="ä¸“æ³¨äºAgentå†³ç­–æœºåˆ¶",
    version=2,  # ç‰ˆæœ¬å‡çº§
    evolution_history=["å¤§è¯­è¨€æ¨¡å‹æç¤ºå·¥ç¨‹"],  # ä¿å­˜å†å²
    evolution_episode_ids=[episode_2.episode_id],
    linked_episode_ids=[episode_1.episode_id, episode_2.episode_id]
)
```

### ä¸šåŠ¡æ£€ç´¢åº”ç”¨åœºæ™¯ | Business Retrieval Scenarios

#### åœºæ™¯1ï¼šç‹¬ç«‹ç›¸ä¼¼åº¦æœç´¢ | Independent Similarity Search

```python
# ç”¨æˆ·æŸ¥è¯¢ï¼šJohnç°åœ¨åœ¨ç ”ç©¶ä»€ä¹ˆï¼Ÿ
query = "Johnç°åœ¨åœ¨ç ”ç©¶ä»€ä¹ˆ"

# 1. æœç´¢è¯­ä¹‰è®°å¿†
semantic_results = await retrieval_service.search_semantic_memories(
    owner_id="john", 
    query=query
)
# è¿”å›ï¼šå½“å‰ç‰ˆæœ¬çš„"AI Agentè¡Œä¸ºè§„åˆ’"

# 2. æœç´¢æƒ…æ™¯è®°å¿†  
episodic_results = await retrieval_service.search_episodic_memories(
    owner_id="john",
    query=query
)
# è¿”å›ï¼šç›¸å…³çš„å¯¹è¯æƒ…æ™¯

# ä¸šåŠ¡å¯ä»¥é€‰æ‹©ä½¿ç”¨å“ªç§è®°å¿†ç±»å‹
if need_factual_knowledge:
    return semantic_results  # ç›´æ¥çš„äº‹å®çŸ¥è¯†
elif need_conversation_context:
    return episodic_results  # å¯¹è¯ä¸Šä¸‹æ–‡
else:
    return combine(semantic_results, episodic_results)  # ç»„åˆä½¿ç”¨
```

#### åœºæ™¯2ï¼šåŒå‘IDå…³è”æŸ¥è¯¢ | Bidirectional ID Association Query

```python
# æŸ¥è¯¢ç‰¹å®šè¯­ä¹‰èŠ‚ç‚¹çš„æ¼”å˜å†å²
evolution_history = await evolution_manager.get_semantic_evolution_history(
    semantic_node_id="john_research_direction_node_id"
)

# è¿”å›å®Œæ•´æ¼”å˜å†å²
{
    "node": current_semantic_node,
    "evolution_timeline": [
        {
            "version": 1,
            "value": "å¤§è¯­è¨€æ¨¡å‹æç¤ºå·¥ç¨‹",
            "episode": episode_1,  # åŒ…å«å®Œæ•´æƒ…æ™¯ä¿¡æ¯
            "timestamp": "2024-01-15T10:00:00"
        },
        {
            "version": 2, 
            "value": "AI Agentè¡Œä¸ºè§„åˆ’",
            "episode": episode_2,
            "timestamp": "2024-04-20T14:30:00"
        }
    ],
    "linked_episodes": [episode_1, episode_2],  # æ‰€æœ‰å…³è”æƒ…æ™¯
    "evolution_episodes": [episode_2]  # å¯¼è‡´æ¼”å˜çš„æƒ…æ™¯
}
```

#### åœºæ™¯3ï¼šä¸Šä¸‹æ–‡æ„ŸçŸ¥ç”Ÿæˆ | Context-Aware Generation

```python
# æ–°å¯¹è¯åˆ°æ¥æ—¶ï¼Œå‘ç°è¿‡ç¨‹ä¼šåˆ©ç”¨å†å²ä¿¡æ¯
new_episode = Episode(
    title="Johnè®¨è®ºAgentæ¶æ„è®¾è®¡",
    content="Johnåˆ†äº«äº†ä»–åœ¨Agentæ¶æ„è®¾è®¡æ–¹é¢çš„æ–°æƒ³æ³•...",
    owner_id="john"
)

# ä¸Šä¸‹æ–‡æ”¶é›†è¿‡ç¨‹
context = await discovery_engine._gather_discovery_context(new_episode)
# è‡ªåŠ¨æ‰¾åˆ°ï¼š
{
    "related_semantic_memories": [
        john_research_direction_node,  # Johnçš„ç ”ç©¶æ–¹å‘
        agent_related_nodes  # å…¶ä»–Agentç›¸å…³çŸ¥è¯†
    ],
    "related_historical_episodes": [
        episode_2,  # ä¹‹å‰å…³äºAgentç ”ç©¶çš„å¯¹è¯
        other_agent_episodes  # å…¶ä»–ç›¸å…³æƒ…æ™¯
    ]
}

# åˆ©ç”¨è¿™äº›ä¸Šä¸‹æ–‡è¿›è¡Œæ›´å‡†ç¡®çš„è¯­ä¹‰å‘ç°
```

### æŒ‰éœ€è®°å¿†è´¨é‡é€‰æ‹© | On-Demand Memory Quality Selection

```python
class AdaptiveMemoryService:
    """
    Adaptive service that provides different memory qualities based on business needs.
    æ ¹æ®ä¸šåŠ¡éœ€æ±‚æä¾›ä¸åŒè´¨é‡è®°å¿†çš„è‡ªé€‚åº”æœåŠ¡ã€‚
    """
    
    async def get_memory_for_query(
        self, 
        owner_id: str, 
        query: str, 
        quality_preference: str = "balanced"
    ) -> dict[str, Any]:
        """
        Get memory with specified quality preference.
        è·å–æŒ‡å®šè´¨é‡åå¥½çš„è®°å¿†ã€‚
        """
        if quality_preference == "factual":
            # éœ€è¦å‡†ç¡®äº‹å®ï¼šä¼˜å…ˆè¯­ä¹‰è®°å¿†
            return {
                "primary": await self.retrieval_service.search_semantic_memories(owner_id, query),
                "secondary": []
            }
            
        elif quality_preference == "contextual":
            # éœ€è¦ä¸°å¯Œä¸Šä¸‹æ–‡ï¼šä¼˜å…ˆæƒ…æ™¯è®°å¿†
            return {
                "primary": await self.retrieval_service.search_episodic_memories(owner_id, query),
                "secondary": []
            }
            
        elif quality_preference == "comprehensive":
            # éœ€è¦å…¨é¢ä¿¡æ¯ï¼šç»„åˆä¸¤ç§è®°å¿†
            semantic_results = await self.retrieval_service.search_semantic_memories(owner_id, query)
            episodic_results = await self.retrieval_service.search_episodic_memories(owner_id, query)
            
            # å¯¹äºæ¯ä¸ªè¯­ä¹‰èŠ‚ç‚¹ï¼Œè·å–å…¶å…³è”çš„æƒ…æ™¯
            enriched_results = []
            for semantic_node in semantic_results:
                associated_episodes = await self.retrieval_service.get_semantic_episodes(semantic_node.node_id)
                enriched_results.append({
                    "semantic_knowledge": semantic_node,
                    "supporting_episodes": associated_episodes["linked_episodes"],
                    "evolution_context": associated_episodes["evolution_episodes"]
                })
            
            return {
                "primary": enriched_results,
                "secondary": episodic_results
            }
            
        else:  # balanced
            # å¹³è¡¡æ¨¡å¼ï¼šåŸºäºæŸ¥è¯¢ç±»å‹è‡ªåŠ¨é€‰æ‹©
            return await self._smart_memory_selection(owner_id, query)
```

---

## æ€§èƒ½è€ƒè™‘ | Performance Considerations

### Scalability | å¯æ‰©å±•æ€§

**English Considerations:**

1. **Storage Optimization**
   - Use database indexes for frequent queries
   - Implement connection pooling for concurrent access
   - Consider read replicas for query-heavy workloads

2. **Memory Management**
   - Limit semantic node cache size
   - Implement LRU eviction for relationship cache
   - Monitor memory usage in production

3. **Discovery Efficiency**
   - Batch process multiple episodes for semantic discovery
   - Implement async processing for non-blocking discovery
   - Use sampling for large conversation datasets

**ä¸­æ–‡è€ƒè™‘ï¼š**

1. **å­˜å‚¨ä¼˜åŒ–**
   - ä¸ºé¢‘ç¹æŸ¥è¯¢ä½¿ç”¨æ•°æ®åº“ç´¢å¼•
   - ä¸ºå¹¶å‘è®¿é—®å®ç°è¿æ¥æ± 
   - ä¸ºæŸ¥è¯¢å¯†é›†å‹å·¥ä½œè´Ÿè½½è€ƒè™‘åªè¯»å‰¯æœ¬

2. **å†…å­˜ç®¡ç†**
   - é™åˆ¶è¯­ä¹‰èŠ‚ç‚¹ç¼“å­˜å¤§å°
   - ä¸ºå…³ç³»ç¼“å­˜å®ç° LRU æ·˜æ±°
   - åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ç›‘æ§å†…å­˜ä½¿ç”¨

3. **å‘ç°æ•ˆç‡**
   - æ‰¹é‡å¤„ç†å¤šä¸ªæƒ…æ™¯è¿›è¡Œè¯­ä¹‰å‘ç°
   - ä¸ºéé˜»å¡å‘ç°å®ç°å¼‚æ­¥å¤„ç†
   - ä¸ºå¤§å‹å¯¹è¯æ•°æ®é›†ä½¿ç”¨é‡‡æ ·

### Monitoring & Analytics | ç›‘æ§ä¸åˆ†æ

**Key Metrics | å…³é”®æŒ‡æ ‡:**

- **Discovery Rate**: Semantic nodes discovered per episode | æ¯ä¸ªæƒ…æ™¯å‘ç°çš„è¯­ä¹‰èŠ‚ç‚¹æ•°
- **Evolution Frequency**: How often knowledge evolves | çŸ¥è¯†æ¼”å˜é¢‘ç‡
- **Relationship Density**: Average relationships per node | æ¯ä¸ªèŠ‚ç‚¹çš„å¹³å‡å…³ç³»æ•°
- **Query Performance**: Response time for semantic queries | è¯­ä¹‰æŸ¥è¯¢å“åº”æ—¶é—´
- **Storage Growth**: Rate of semantic knowledge accumulation | è¯­ä¹‰çŸ¥è¯†ç§¯ç´¯é€Ÿåº¦

---

## å®‰å…¨ä¸éšç§ | Security & Privacy

### Privacy Protection | éšç§ä¿æŠ¤

**English Measures:**

1. **Data Isolation**: Semantic knowledge strictly scoped to owner
2. **Encryption**: Sensitive knowledge encrypted at rest
3. **Access Control**: Role-based access to semantic data
4. **Audit Logging**: Track access to private domain knowledge

**ä¸­æ–‡æªæ–½ï¼š**

1. **æ•°æ®éš”ç¦»**ï¼šè¯­ä¹‰çŸ¥è¯†ä¸¥æ ¼é™å®šäºæ‰€æœ‰è€…
2. **åŠ å¯†**ï¼šæ•æ„ŸçŸ¥è¯†é™æ€åŠ å¯†
3. **è®¿é—®æ§åˆ¶**ï¼šåŸºäºè§’è‰²çš„è¯­ä¹‰æ•°æ®è®¿é—®
4. **å®¡è®¡æ—¥å¿—**ï¼šè·Ÿè¸ªç§åŸŸçŸ¥è¯†è®¿é—®

### Data Retention | æ•°æ®ä¿ç•™

**Policies | æ”¿ç­–:**

- **Automatic Cleanup**: Remove unused semantic nodes after configurable period
- **Version Limits**: Limit evolution history to prevent unbounded growth
- **User Control**: Allow users to manually delete sensitive knowledge

---

## æ€»ç»“ | Conclusion

### Innovation Summary | åˆ›æ–°æ€»ç»“

**English**: This semantic memory design introduces a novel approach to private domain knowledge extraction using episodic memory as a "knowledge mask." The differential reconstruction technique automatically identifies information gaps between LLM world knowledge and user-specific knowledge, creating an evolving semantic knowledge base with relationship mapping.

**Key Benefits:**
- **Automatic Discovery**: No manual knowledge entry required
- **Evolutionary Design**: Knowledge evolves with new information
- **Relationship Mapping**: Simple but effective knowledge connections
- **Integration**: Seamless integration with existing Nemori architecture

**ä¸­æ–‡**: è¯¥è¯­ä¹‰è®°å¿†è®¾è®¡å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„ç§åŸŸçŸ¥è¯†æå–æ–¹æ³•ï¼Œä½¿ç”¨æƒ…æ™¯è®°å¿†ä½œä¸º"çŸ¥è¯†æ©ç "ã€‚å·®åˆ†é‡å»ºæŠ€æœ¯è‡ªåŠ¨è¯†åˆ«å¤§è¯­è¨€æ¨¡å‹ä¸–ç•ŒçŸ¥è¯†ä¸ç”¨æˆ·ç‰¹å®šçŸ¥è¯†ä¹‹é—´çš„ä¿¡æ¯å·®è·ï¼Œåˆ›å»ºå¸¦å…³ç³»æ˜ å°„çš„æ¼”è¿›è¯­ä¹‰çŸ¥è¯†åº“ã€‚

**ä¸»è¦ä¼˜åŠ¿ï¼š**
- **è‡ªåŠ¨å‘ç°**ï¼šæ— éœ€æ‰‹åŠ¨çŸ¥è¯†è¾“å…¥
- **æ¼”è¿›è®¾è®¡**ï¼šçŸ¥è¯†éšæ–°ä¿¡æ¯æ¼”å˜
- **å…³ç³»æ˜ å°„**ï¼šç®€å•ä½†æœ‰æ•ˆçš„çŸ¥è¯†è¿æ¥
- **é›†æˆæ€§**ï¼šä¸ç°æœ‰ Nemori æ¶æ„æ— ç¼é›†æˆ

### Future Enhancements | æœªæ¥å¢å¼º

**Potential Improvements | æ½œåœ¨æ”¹è¿›:**

1. **Advanced NLP**: Use semantic similarity models for better relationship detection
2. **Knowledge Graphs**: Evolve to more sophisticated graph structures
3. **Multi-modal Support**: Extend to images, audio, and video content
4. **Federated Learning**: Enable privacy-preserving knowledge sharing
5. **Active Learning**: LLM-guided discovery of missing knowledge gaps

---

**Document Version**: 1.0  
**Last Updated**: 2024-01-XX  
**Authors**: Nemori Development Team  

*Nemori - èµ‹äºˆ AI æ™ºèƒ½ä½“é•¿æœŸè®°å¿†ä»¥é©±åŠ¨å…¶è‡ªæˆ‘è¿›åŒ– ğŸš€*