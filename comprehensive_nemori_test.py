"""
Comprehensive Nemori System Test

This script demonstrates the complete Nemori functionality including:
1. Episodic Memory: Building episodes from conversation data
2. Semantic Memory: Knowledge discovery and evolution  
3. Unified Retrieval: Cross-memory search capabilities
4. Bidirectional Linking: Episode â†” Semantic associations
5. LLM Integration: Semantic discovery through differential analysis
"""

import asyncio
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Any

from nemori.core.data_types import RawEventData, DataType, TemporalInfo
from nemori.core.episode import Episode
from nemori.storage.storage_types import StorageConfig
from nemori.storage.duckdb_storage import DuckDBEpisodicMemoryRepository
from nemori.storage.duckdb_semantic_storage import DuckDBSemanticMemoryRepository
from nemori.llm.providers.openai_provider import OpenAIProvider
from nemori.retrieval.unified_service import UnifiedRetrievalService
from nemori.retrieval.providers.bm25_provider import BM25RetrievalProvider
from nemori.retrieval.providers.embedding_provider import EmbeddingRetrievalProvider
from nemori.episode_manager import EpisodeManager


class ComprehensiveNemoriTest:
    def __init__(self):
        self.db_path = "comprehensive_test.duckdb"
        self.episode_repo = None
        self.semantic_repo = None
        self.episode_manager = None
        self.unified_retrieval = None
        
    async def initialize(self):
        """Initialize all Nemori components"""
        print("ğŸ”§ Initializing Nemori System Components")
        print("=" * 60)
        
        # Storage configuration
        storage_config = StorageConfig(
            connection_string=self.db_path, 
            backend_type="duckdb"
        )
        
        # Add embedding configuration (using your API settings)
        storage_config.openai_api_key = "sk-NuO4dbNTkXXi5zWYkLFnhyug1kkqjeysvrb74pA9jTGfz8cm"
        storage_config.openai_base_url = "https://jeniya.cn/v1"
        
        # Initialize repositories
        self.episode_repo = DuckDBEpisodicMemoryRepository(storage_config)
        self.semantic_repo = DuckDBSemanticMemoryRepository(storage_config)
        
        await self.episode_repo.initialize()
        await self.semantic_repo.initialize()
        
        # Initialize LLM provider for semantic discovery
        llm_provider = OpenAIProvider(
            model="gpt-4o-mini",
            api_key="sk-NuO4dbNTkXXi5zWYkLFnhyug1kkqjeysvrb74pA9jTGfz8cm",
            base_url="https://jeniya.cn/v1"
        )
        
        # Initialize episode manager with semantic discovery
        from nemori.core.builders import EpisodeBuilderRegistry
        from nemori.storage.duckdb_storage import DuckDBRawDataRepository
        
        builder_registry = EpisodeBuilderRegistry()
        raw_data_repo = DuckDBRawDataRepository(storage_config)
        await raw_data_repo.initialize()
        
        self.episode_manager = EpisodeManager(
            raw_data_repo=raw_data_repo,
            episode_repo=self.episode_repo,
            builder_registry=builder_registry,
            retrieval_service=None
        )
        from nemori.retrieval.retrieval_types import RetrievalConfig
        # Initialize unified retrieval service
        embedding_provider = EmbeddingRetrievalProvider(
            episode_repo=self.episode_repo,
            api_key="sk-NuO4dbNTkXXi5zWYkLFnhyug1kkqjeysvrb74pA9jTGfz8cm",
            base_url="https://jeniya.cn/v1"
        )
        
        self.unified_retrieval = UnifiedRetrievalService(
            episode_repo=self.episode_repo,
            semantic_repo=self.semantic_repo,
            retrieval_providers=embedding_provider
        )
        
        print("âœ… All Nemori components initialized successfully")
        print("   ğŸ“– Episodic Memory: DuckDB repository")
        print("   ğŸ§  Semantic Memory: DuckDB repository with embeddings")
        print("   ğŸ¤– LLM Provider: OpenAI compatible API")
        print("   ğŸ”— Unified Retrieval: BM25 + Embedding search")
        
    def create_sample_conversations(self) -> List[Dict[str, Any]]:
        """Create comprehensive sample conversation data"""
        conversations = [
            {
                "conversation_id": "conv_001",
                "participants": ["Alice", "Bob"],
                "timestamp": datetime.now() - timedelta(days=5),
                "messages": [
                    {"speaker": "Alice", "content": "æˆ‘æœ€è¿‘åœ¨ç ”ç©¶å¤§è¯­è¨€æ¨¡å‹çš„å¾®è°ƒæŠ€æœ¯ï¼Œä½ æœ‰ä»€ä¹ˆç»éªŒåˆ†äº«å—ï¼Ÿ", "timestamp": datetime.now() - timedelta(days=5, hours=1)},
                    {"speaker": "Bob", "content": "æˆ‘ç”¨è¿‡LoRAå’ŒQLoRAæ–¹æ³•ï¼Œæ•ˆæœä¸é”™ã€‚ä½ å…·ä½“æƒ³å¾®è°ƒä»€ä¹ˆç±»å‹çš„ä»»åŠ¡ï¼Ÿ", "timestamp": datetime.now() - timedelta(days=5, hours=1, minutes=2)},
                    {"speaker": "Alice", "content": "ä¸»è¦æ˜¯æƒ³åšä¸­æ–‡å¯¹è¯ç³»ç»Ÿï¼Œæˆ‘åœ¨ç”¨ChatGLM-6Bä½œä¸ºåŸºç¡€æ¨¡å‹", "timestamp": datetime.now() - timedelta(days=5, hours=1, minutes=5)},
                    {"speaker": "Bob", "content": "ChatGLMä¸é”™ï¼Œå»ºè®®ä½ è¯•è¯•P-Tuning v2ï¼Œå¯¹ä¸­æ–‡æ•ˆæœå¾ˆå¥½ã€‚æ•°æ®é›†å‡†å¤‡äº†å¤šå°‘ï¼Ÿ", "timestamp": datetime.now() - timedelta(days=5, hours=1, minutes=8)},
                    {"speaker": "Alice", "content": "å¤§æ¦‚æœ‰10ä¸‡æ¡å¯¹è¯æ•°æ®ï¼Œéƒ½æ˜¯å®¢æœé¢†åŸŸçš„ã€‚æˆ‘æ‹…å¿ƒè¿‡æ‹Ÿåˆé—®é¢˜", "timestamp": datetime.now() - timedelta(days=5, hours=1, minutes=12)},
                    {"speaker": "Bob", "content": "10ä¸‡æ¡ä¸ç®—å¤šï¼Œå»ºè®®åŠ å…¥ä¸€äº›æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œæ¯”å¦‚Dropoutå’Œæƒé‡è¡°å‡", "timestamp": datetime.now() - timedelta(days=5, hours=1, minutes=15)}
                ]
            },
            {
                "conversation_id": "conv_002", 
                "participants": ["Alice", "Charlie"],
                "timestamp": datetime.now() - timedelta(days=3),
                "messages": [
                    {"speaker": "Charlie", "content": "Aliceï¼Œå¬è¯´ä½ åœ¨åšAIé¡¹ç›®ï¼Ÿæˆ‘ä»¬å…¬å¸ä¹Ÿæƒ³å¼•å…¥AIæŠ€æœ¯", "timestamp": datetime.now() - timedelta(days=3, hours=2)},
                    {"speaker": "Alice", "content": "æ˜¯çš„ï¼Œæˆ‘ä»¬åœ¨å¼€å‘æ™ºèƒ½å®¢æœç³»ç»Ÿã€‚ä½ ä»¬å…¬å¸æ˜¯ä»€ä¹ˆè¡Œä¸šï¼Ÿ", "timestamp": datetime.now() - timedelta(days=3, hours=2, minutes=3)},
                    {"speaker": "Charlie", "content": "æˆ‘ä»¬åšç”µå•†ï¼Œæ¯å¤©å®¢æœå’¨è¯¢é‡å¾ˆå¤§ï¼Œæƒ³ç”¨AIæ¥é™ä½äººå·¥æˆæœ¬", "timestamp": datetime.now() - timedelta(days=3, hours=2, minutes=6)},
                    {"speaker": "Alice", "content": "ç”µå•†å®¢æœå¾ˆé€‚åˆç”¨AIï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿå°±æ˜¯é’ˆå¯¹è¿™ä¸ªåœºæ™¯ã€‚ä¸»è¦ç”¨äº†æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯", "timestamp": datetime.now() - timedelta(days=3, hours=2, minutes=9)},
                    {"speaker": "Charlie", "content": "RAGæŠ€æœ¯ï¼Ÿå¬èµ·æ¥å¾ˆé«˜çº§ï¼Œéƒ¨ç½²æˆæœ¬é«˜å—ï¼Ÿ", "timestamp": datetime.now() - timedelta(days=3, hours=2, minutes=12)},
                    {"speaker": "Alice", "content": "æˆæœ¬å¯æ§ï¼Œä¸»è¦æ˜¯å‘é‡æ•°æ®åº“å’ŒAPIè°ƒç”¨è´¹ç”¨ã€‚æˆ‘ä»¬ç”¨çš„æ˜¯å¼€æºæ–¹æ¡ˆ", "timestamp": datetime.now() - timedelta(days=3, hours=2, minutes=15)},
                    {"speaker": "Charlie", "content": "èƒ½è¯¦ç»†ä»‹ç»ä¸€ä¸‹æŠ€æœ¯æ¶æ„å—ï¼Ÿæˆ‘æƒ³äº†è§£å…·ä½“å®ç°", "timestamp": datetime.now() - timedelta(days=3, hours=2, minutes=18)}
                ]
            },
            {
                "conversation_id": "conv_003",
                "participants": ["Bob", "David"],
                "timestamp": datetime.now() - timedelta(days=1),
                "messages": [
                    {"speaker": "David", "content": "Bobï¼Œä½ å¯¹å¤šæ¨¡æ€AIæœ‰äº†è§£å—ï¼Ÿæˆ‘åœ¨ç ”ç©¶è§†è§‰é—®ç­”ç³»ç»Ÿ", "timestamp": datetime.now() - timedelta(days=1, hours=3)},
                    {"speaker": "Bob", "content": "æœ‰ä¸€äº›ï¼Œæˆ‘ç”¨è¿‡CLIPå’ŒBLIPæ¨¡å‹ã€‚ä½ çš„VQAç³»ç»Ÿä¸»è¦è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿ", "timestamp": datetime.now() - timedelta(days=1, hours=3, minutes=2)},
                    {"speaker": "David", "content": "æˆ‘ä»¬æƒ³åšåŒ»å­¦å½±åƒçš„æ™ºèƒ½è¯Šæ–­è¾…åŠ©ï¼Œè®©AIèƒ½ç†è§£Xå…‰ç‰‡å¹¶å›ç­”åŒ»ç”Ÿçš„é—®é¢˜", "timestamp": datetime.now() - timedelta(days=1, hours=3, minutes=5)},
                    {"speaker": "Bob", "content": "åŒ»å­¦å½±åƒå¾ˆæœ‰æŒ‘æˆ˜æ€§ï¼Œæ•°æ®æ ‡æ³¨è´¨é‡è¦æ±‚å¾ˆé«˜ã€‚ä½ ä»¬æœ‰ä¸“ä¸šçš„åŒ»ç”Ÿå›¢é˜Ÿå—ï¼Ÿ", "timestamp": datetime.now() - timedelta(days=1, hours=3, minutes=8)},
                    {"speaker": "David", "content": "æœ‰çš„ï¼Œæˆ‘ä»¬å’Œä¸‰ç”²åŒ»é™¢åˆä½œï¼Œæœ‰æ”¾å°„ç§‘åŒ»ç”Ÿå¸®åŠ©æ ‡æ³¨ã€‚æ•°æ®è´¨é‡åº”è¯¥æ²¡é—®é¢˜", "timestamp": datetime.now() - timedelta(days=1, hours=3, minutes=11)},
                    {"speaker": "Bob", "content": "é‚£å¾ˆå¥½ï¼Œå»ºè®®ä½ çœ‹çœ‹æœ€æ–°çš„åŒ»å­¦å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ¯”å¦‚Med-CLIP", "timestamp": datetime.now() - timedelta(days=1, hours=3, minutes=14)}
                ]
            }
        ]
        return conversations
    
    async def test_episodic_memory(self, conversations: List[Dict[str, Any]]):
        """Test episodic memory functionality"""
        print("\nğŸ“– Testing Episodic Memory")
        print("-" * 40)
        
        episodes_created = []
        
        for conv in conversations:
            # Convert conversation to RawEventData
            conversation_text = "\n".join([
                f"{msg['speaker']}: {msg['content']}" 
                for msg in conv['messages']
            ])
            
            raw_data = RawEventData(
                data_type=DataType.CONVERSATION,
                content=conversation_text,
                temporal_info=TemporalInfo(timestamp=conv['timestamp']),
                metadata={
                    "conversation_id": conv['conversation_id'],
                    "participants": conv['participants'],
                    "message_count": len(conv['messages'])
                }
            )
            
            # Process through episode manager
            await self.episode_manager.ingest_event(raw_data, conv['participants'][0])
            # Then build episode from the stored raw data
            episode = await self.episode_manager.build_episode(raw_data.data_id, conv['participants'][0])
            episodes_created.append(episode)
            
            print(f"âœ… Created episode: {episode.title[:50]}...")
            print(f"   Owner: {episode.owner_id}")
            print(f"   Content length: {len(episode.content)} chars")
            print(f"   Importance: {episode.importance_score:.2f}")
        
        print(f"\nğŸ“Š Total episodes created: {len(episodes_created)}")
        return episodes_created
    
    async def test_semantic_memory(self, episodes: List[Episode]):
        """Test semantic memory functionality"""
        print("\nğŸ§  Testing Semantic Memory")
        print("-" * 40)
        
        # Check what semantic knowledge was discovered during episode creation
        owners = list(set(ep.owner_id for ep in episodes))
        total_semantic_nodes = 0
        
        for owner in owners:
            nodes = await self.semantic_repo.get_all_semantic_nodes_for_owner(owner)
            total_semantic_nodes += len(nodes)
            
            print(f"ğŸ” {owner}: {len(nodes)} semantic concepts discovered")
            
            # Show sample concepts
            for node in nodes[:3]:  # Show first 3
                print(f"   â€¢ {node.key}: {node.value[:40]}...")
                print(f"     Confidence: {node.confidence:.2f}, Version: {node.version}")
        
        print(f"\nğŸ“Š Total semantic concepts: {total_semantic_nodes}")
        
        # Test semantic search
        if total_semantic_nodes > 0:
            print("\nğŸ” Testing semantic search:")
            search_queries = ["AI", "æŠ€æœ¯", "æ¨¡å‹", "ç³»ç»Ÿ"]
            
            for query in search_queries:
                similar_nodes = await self.semantic_repo.similarity_search_semantic_nodes(
                    owners[0], query, limit=3
                )
                print(f"   Query '{query}': {len(similar_nodes)} results")
                for node in similar_nodes[:2]:
                    print(f"     - {node.key}: {node.value[:30]}...")
        
        return total_semantic_nodes
    
    async def test_unified_retrieval(self, episodes: List[Episode]):
        """Test unified retrieval functionality"""
        print("\nğŸ”— Testing Unified Retrieval")
        print("-" * 40)
        
        # Test queries that should find both episodic and semantic content
        test_queries = [
            "å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒ",
            "æ™ºèƒ½å®¢æœç³»ç»Ÿ",
            "å¤šæ¨¡æ€AIè§†è§‰",
            "æŠ€æœ¯æ¶æ„è®¾è®¡"
        ]
        
        for query in test_queries:
            print(f"\nğŸ” Query: '{query}'")
            
            # Search episodes
            episode_results = await self.unified_retrieval.search_episodes(
                query=query,
                owner_id=episodes[0].owner_id,
                limit=3
            )
            print(f"   ğŸ“– Episodes found: {len(episode_results)}")
            for ep in episode_results[:2]:
                print(f"     - {ep.title[:40]}...")
            
            # Search semantic knowledge
            semantic_results = await self.unified_retrieval.search_semantic_knowledge(
                query=query,
                owner_id=episodes[0].owner_id,
                limit=3
            )
            print(f"   ğŸ§  Semantic concepts found: {len(semantic_results)}")
            for sem in semantic_results[:2]:
                print(f"     - {sem.key}: {sem.value[:30]}...")
    
    async def test_bidirectional_linking(self, episodes: List[Episode]):
        """Test bidirectional episode-semantic linking"""
        print("\nğŸ”— Testing Bidirectional Linking")
        print("-" * 40)
        
        if not episodes:
            print("   No episodes to test linking")
            return
        
        sample_episode = episodes[0]
        
        # Get semantic concepts linked to this episode
        episode_semantics = await self.unified_retrieval.get_episode_semantics(
            sample_episode.episode_id
        )
        print(f"âœ… Episode '{sample_episode.title[:30]}...' links to {len(episode_semantics)} semantic concepts")
        
        if episode_semantics:
            # Test reverse linking
            sample_semantic = episode_semantics[0]
            semantic_episodes = await self.unified_retrieval.get_semantic_episodes(
                sample_semantic.node_id
            )
            print(f"âœ… Semantic concept '{sample_semantic.key}' links to {len(semantic_episodes)} episodes")
            print("   ğŸ”— Bidirectional linking verified!")
        
        return len(episode_semantics)
    
    async def test_knowledge_evolution(self):
        """Test knowledge evolution functionality"""
        print("\nğŸ”„ Testing Knowledge Evolution")
        print("-" * 40)
        
        # Find a semantic node to evolve
        alice_nodes = await self.semantic_repo.get_all_semantic_nodes_for_owner("Alice")
        
        if alice_nodes:
            original_node = alice_nodes[0]
            print(f"ğŸ“ Original: {original_node.key} = {original_node.value}")
            print(f"   Version: {original_node.version}, Confidence: {original_node.confidence:.2f}")
            
            # Evolve the knowledge
            evolved_node = original_node.evolve(
                new_value=f"{original_node.value} - å·²ç»å®é™…éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ",
                new_context=f"{original_node.context} é¡¹ç›®å·²æˆåŠŸä¸Šçº¿è¿è¡Œ",
                evolution_episode_id="evolution_test"
            )
            
            await self.semantic_repo.update_semantic_node(evolved_node)
            
            print(f"ğŸ”„ Evolved: {evolved_node.key} = {evolved_node.value}")
            print(f"   Version: {evolved_node.version}, History: {len(evolved_node.evolution_history)} changes")
            
            return True
        
        print("   No semantic nodes available for evolution test")
        return False
    
    async def generate_comprehensive_report(self, episodes: List[Episode], semantic_count: int):
        """Generate a comprehensive test report"""
        print("\nğŸ“Š Comprehensive Test Report")
        print("=" * 60)
        
        # Episode statistics
        owners = list(set(ep.owner_id for ep in episodes))
        print(f"ğŸ“– EPISODIC MEMORY:")
        print(f"   â€¢ Total Episodes: {len(episodes)}")
        print(f"   â€¢ Unique Owners: {len(owners)}")
        
        for owner in owners:
            owner_episodes = [ep for ep in episodes if ep.owner_id == owner]
            avg_importance = sum(ep.importance_score for ep in owner_episodes) / len(owner_episodes)
            print(f"   â€¢ {owner}: {len(owner_episodes)} episodes (avg importance: {avg_importance:.2f})")
        
        # Semantic memory statistics
        print(f"\nğŸ§  SEMANTIC MEMORY:")
        print(f"   â€¢ Total Concepts: {semantic_count}")
        
        total_nodes_by_owner = {}
        for owner in owners:
            nodes = await self.semantic_repo.get_all_semantic_nodes_for_owner(owner)
            total_nodes_by_owner[owner] = len(nodes)
            if nodes:
                avg_confidence = sum(node.confidence for node in nodes) / len(nodes)
                print(f"   â€¢ {owner}: {len(nodes)} concepts (avg confidence: {avg_confidence:.2f})")
        
        # Unified system statistics
        print(f"\nğŸ”— UNIFIED SYSTEM:")
        total_memory_items = len(episodes) + semantic_count
        print(f"   â€¢ Total Memory Items: {total_memory_items}")
        print(f"   â€¢ Memory Distribution: {len(episodes)} episodes + {semantic_count} concepts")
        
        # Test various retrieval scenarios
        print(f"\nğŸ¯ RETRIEVAL CAPABILITIES:")
        test_passed = 0
        total_tests = 4
        
        # Test 1: Episode search
        try:
            results = await self.unified_retrieval.search_episodes("AI", owners[0], limit=5)
            print(f"   âœ“ Episode search: {len(results)} results")
            test_passed += 1
        except Exception as e:
            print(f"   âœ— Episode search failed: {e}")
        
        # Test 2: Semantic search
        try:
            results = await self.unified_retrieval.search_semantic_knowledge("æŠ€æœ¯", owners[0], limit=5)
            print(f"   âœ“ Semantic search: {len(results)} results")
            test_passed += 1
        except Exception as e:
            print(f"   âœ— Semantic search failed: {e}")
        
        # Test 3: Cross-memory retrieval
        try:
            if episodes and semantic_count > 0:
                episode_semantics = await self.unified_retrieval.get_episode_semantics(episodes[0].episode_id)
                print(f"   âœ“ Cross-memory linking: {len(episode_semantics)} links")
                test_passed += 1
            else:
                print(f"   - Cross-memory linking: No data to test")
        except Exception as e:
            print(f"   âœ— Cross-memory linking failed: {e}")
        
        # Test 4: Statistics
        try:
            stats = await self.semantic_repo.get_semantic_statistics(owners[0])
            print(f"   âœ“ Statistics generation: {stats['node_count']} nodes tracked")
            test_passed += 1
        except Exception as e:
            print(f"   âœ— Statistics generation failed: {e}")
        
        print(f"\nâœ¨ TEST SUMMARY:")
        print(f"   Tests Passed: {test_passed}/{total_tests}")
        print(f"   System Status: {'âœ… HEALTHY' if test_passed >= 3 else 'âš ï¸ ISSUES DETECTED'}")
        
        return test_passed >= 3
    
    async def cleanup(self):
        """Cleanup resources"""
        print("\nğŸ§¹ Cleaning up resources...")
        
        if self.episode_repo:
            await self.episode_repo.close()
        if self.semantic_repo:
            await self.semantic_repo.close()
        
        # Remove test database
        db_path = Path(self.db_path)
        if db_path.exists():
            db_path.unlink()
            print("âœ… Test database cleaned up")
    

async def main():
    """Main test function"""
    print("ğŸš€ Comprehensive Nemori System Test")
    print("=" * 60)
    print("Testing complete Nemori functionality:")
    print("  ğŸ“– Episodic Memory: Episode creation and storage")
    print("  ğŸ§  Semantic Memory: Knowledge discovery and evolution")
    print("  ğŸ”— Unified Retrieval: Cross-memory search capabilities")
    print("  ğŸ¤– LLM Integration: Semantic discovery via differential analysis")
    print("  ğŸ¯ Bidirectional Linking: Episode â†” Semantic associations")
    print()
    
    test_runner = ComprehensiveNemoriTest()
    
    try:
        # Initialize system
        await test_runner.initialize()
        
        # Create sample data
        print("\nğŸ“ Creating sample conversation data...")
        conversations = test_runner.create_sample_conversations()
        print(f"âœ… Created {len(conversations)} sample conversations")
        
        # Test episodic memory
        episodes = await test_runner.test_episodic_memory(conversations)
        
        # Test semantic memory
        semantic_count = await test_runner.test_semantic_memory(episodes)
        
        # Test unified retrieval
        await test_runner.test_unified_retrieval(episodes)
        
        # Test bidirectional linking
        await test_runner.test_bidirectional_linking(episodes)
        
        # Test knowledge evolution
        await test_runner.test_knowledge_evolution()
        
        # Generate comprehensive report
        success = await test_runner.generate_comprehensive_report(episodes, semantic_count)
        
        if success:
            print(f"\nğŸ‰ ALL TESTS PASSED! Nemori system is working correctly.")
        else:
            print(f"\nâš ï¸ Some tests failed. Check the output above for details.")
    
    except Exception as e:
        print(f"\nâŒ Test failed with error: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        await test_runner.cleanup()
        

if __name__ == "__main__":
    asyncio.run(main())