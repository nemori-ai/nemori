#!/usr/bin/env python3
"""
Standalone Semantic Index Builder

This script continues from where the ingestion left off, building only the semantic indices
without re-running the entire episode processing pipeline.
"""

import asyncio
from pathlib import Path
from dotenv import load_dotenv

from nemori.storage.storage_types import StorageConfig
from nemori.storage.duckdb_storage import DuckDBEpisodicMemoryRepository
from nemori.storage.duckdb_semantic_storage import DuckDBSemanticMemoryRepository


async def build_semantic_indices_only(version: str = "default"):
    """Build semantic indices from existing processed data."""
    load_dotenv()
    
    print("üß† Standalone Semantic Index Builder")
    print("=" * 60)
    print(f"üìä Version: {version}")
    
    # Setup storage paths (same as used in ingestion)
    db_dir = Path(f"results/locomo/nemori-{version}/storages")
    db_path = db_dir / "nemori_memory.duckdb"
    
    if not db_path.exists():
        print(f"‚ùå Database not found at {db_path}")
        print("Please check the version name and ensure ingestion was completed")
        return
    
    print(f"‚úÖ Found database: {db_path}")
    
    # Setup storage configurations (same as ingestion)
    episode_storage_config = StorageConfig(
        backend_type="duckdb",
        connection_string=str(db_path),
        batch_size=100,
        cache_size=1000,
        enable_semantic_search=False,
    )
    
    semantic_storage_config = StorageConfig(
        backend_type="duckdb",
        connection_string=str(db_path),
        batch_size=100,
        cache_size=1000,
    )
    semantic_storage_config.openai_api_key = "EMPTY"
    semantic_storage_config.openai_base_url = "http://localhost:6007/v1"
    semantic_storage_config.openai_embed_model = "qwen3-emb"
    
    # Initialize repositories
    print("üîó Connecting to existing database...")
    episode_repo = DuckDBEpisodicMemoryRepository(episode_storage_config)
    semantic_repo = DuckDBSemanticMemoryRepository(semantic_storage_config)
    
    await episode_repo.initialize()
    await semantic_repo.initialize()
    
    # Get all episodes to extract owner IDs
    print("üìä Analyzing existing data...")
    
    # Get all owner IDs from episodes using a different approach
    owner_ids = set()
    total_episodes = 0
    
    try:
        # Get all episodes with an empty query (gets all episodes)
        from nemori.storage.storage_types import EpisodeQuery, SortBy
        
        print("   üìñ Loading all episodes from database...")
        
        # Search all episodes with no filters to get complete dataset
        all_episodes_query = EpisodeQuery(
            limit=10000,  # Large limit to get all episodes
            sort_by=SortBy.TIMESTAMP
        )
        
        result = await episode_repo.search_episodes(all_episodes_query)
        all_episodes = result.episodes if hasattr(result, "episodes") else result
        
        # Extract unique owner IDs
        for episode in all_episodes:
            owner_ids.add(episode.owner_id)
        
        total_episodes = len(all_episodes)
        
        print(f"   ‚úÖ Found {len(owner_ids)} unique owners in database")
        print(f"   üìñ Total episodes: {total_episodes}")
        
        # Count episodes for each owner
        for owner_id in sorted(owner_ids):
            result = await episode_repo.get_episodes_by_owner(owner_id)
            owner_episodes = result.episodes if hasattr(result, "episodes") else result
            episode_count = len(owner_episodes)
            print(f"     üë§ {owner_id}: {episode_count} episodes")
            
    except Exception as e:
        print(f"‚ùå Error analyzing existing data: {e}")
        print(f"   üí° Trying alternative approach...")
        
        # Fallback: Try to get episodes for common owner patterns
        common_patterns = [
            "Andrew", "Audrey", "Calvin", "Caroline", "Dave", "Deborah", 
            "Evan", "Gina", "James", "Joanna", "John", "Jolene", 
            "Jon", "Maria", "Melanie", "Nate", "Sam", "Tim"
        ]
        
        for pattern in common_patterns:
            try:
                result = await episode_repo.get_episodes_by_owner(pattern)
                owner_episodes = result.episodes if hasattr(result, "episodes") else result
                if owner_episodes:
                    owner_ids.add(pattern)
                    episode_count = len(owner_episodes)
                    total_episodes += episode_count
                    print(f"     üë§ {pattern}: {episode_count} episodes")
            except:
                continue
    
    print(f"üìä Total: {total_episodes} episodes from {len(owner_ids)} owners")
    
    if not owner_ids:
        print("‚ö†Ô∏è No owner IDs found - nothing to index")
        return
    
    print(f"üéØ Building semantic indices for {len(owner_ids)} owners.")
        
    # Build semantic memory embedding indices
    print(f"üß† Triggering semantic memory embedding indices...")
    all_semantic_nodes_count = 0
    semantic_owners_with_nodes = set()
    nodes_with_embeddings = 0
    
    # Initialize semantic embedding provider for efficient indexing
    from nemori.retrieval.providers.semantic_embedding_provider import SemanticEmbeddingProvider
    
    semantic_embedding_provider = SemanticEmbeddingProvider(
        semantic_storage=semantic_repo,
        api_key="EMPTY",
        base_url="http://localhost:6007/v1",
        embed_model="qwen3-emb",
        persistence_dir=db_dir,
        enable_persistence=True
    )
    await semantic_embedding_provider.initialize()
    print("   ‚úÖ Semantic embedding provider initialized")
    
    for owner_id in owner_ids:
        try:
            # Get all semantic nodes for this owner
            semantic_nodes = await semantic_repo.get_all_semantic_nodes_for_owner(owner_id)
            if semantic_nodes:
                semantic_owners_with_nodes.add(owner_id)
                all_semantic_nodes_count += len(semantic_nodes)
                
                # Rebuild semantic embedding index for this owner
                # This will create persistent JSON index files similar to episodic memory
                await semantic_embedding_provider._rebuild_semantic_index(owner_id)
                
                # Count nodes with embeddings
                for node in semantic_nodes:
                    if node.embedding_vector or True:  # Assume all will have embeddings after rebuild
                        nodes_with_embeddings += 1
                
                print(f"   ‚úÖ Semantic index built for {owner_id}: {len(semantic_nodes)} nodes")
            else:
                print(f"   üìù No semantic nodes found for {owner_id}")
                
        except Exception as e:
            print(f"   ‚ö†Ô∏è Semantic index building failed for {owner_id}: {e}")
    
    print(f"\nüìä Semantic Index Building Summary:")
    print(f"   üß† Semantic Memory: {all_semantic_nodes_count} nodes indexed")
    print(f"   üë• Owners with semantic knowledge: {len(semantic_owners_with_nodes)}/{len(owner_ids)}")
    print(f"   üéØ Semantic indices ready for retrieval")
    
    # Cleanup
    await episode_repo.close()
    await semantic_repo.close()
    
    print("\n‚úÖ Semantic indexing verification complete!")
    print("üéâ Your semantic memory system is ready for search!")


async def main():
    """Main entry point"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Build semantic indices from existing data")
    parser.add_argument(
        "--version",
        type=str,
        default="default",
        help="Version identifier used during ingestion (default: default)",
    )
    
    args = parser.parse_args()
    
    await build_semantic_indices_only(args.version)


if __name__ == "__main__":
    asyncio.run(main())