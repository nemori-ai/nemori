import argparse
import asyncio
import json
import traceback
from typing import Dict, Any

import pandas as pd

from dotenv import load_dotenv
from nemori_eval.experiment import NemoriExperiment, RetrievalStrategy
from nemori.retrieval import RetrievalQuery
from nemori_eval.search import get_nemori_unified_client, nemori_unified_search
NEMORI_AVAILABLE = True


def create_test_locomo_data():
    """åˆ›å»ºæµ‹è¯•ç”¨çš„locomoæ ¼å¼æ•°æ®"""
    test_conversations = [
        {
            "sample_id": "test_001",
            "user_id": "77777",
            "conversation": {
                "speaker_a": "å¼ ä¸‰",
                "speaker_b": "æå››",
                "session_1": [
                    {
                        "speaker": "å¼ ä¸‰",
                        "dia_id": "D1:1", 
                        "text": "æœ€è¿‘æˆ‘åœ¨å­¦ä¹ Pythonæœºå™¨å­¦ä¹ ï¼Œä½ è§‰å¾—å“ªäº›åº“æ¯”è¾ƒé‡è¦ï¼Ÿ",
                        "timestamp": "2024-01-20T10:00:00Z"
                    },
                    {
                        "speaker": "æå››",
                        "dia_id": "D1:2",
                        "text": "scikit-learnæ˜¯å…¥é—¨å¿…å¤‡ï¼Œpandasç”¨äºæ•°æ®å¤„ç†ï¼Œnumpyå¤„ç†æ•°å€¼è®¡ç®—ã€‚æ·±åº¦å­¦ä¹ æ¨èPyTorchã€‚",
                        "timestamp": "2024-01-20T10:02:00Z"
                    },
                    {
                        "speaker": "å¼ ä¸‰", 
                        "dia_id": "D1:3",
                        "text": "PyTorchå’ŒTensorFlowæ¯”è¾ƒï¼Œå“ªä¸ªæ›´é€‚åˆåˆå­¦è€…ï¼Ÿ",
                        "timestamp": "2024-01-20T10:04:00Z"
                    },
                    {
                        "speaker": "æå››",
                        "dia_id": "D1:4", 
                        "text": "PyTorchçš„åŠ¨æ€å›¾æ›´ç›´è§‚ï¼Œè°ƒè¯•å®¹æ˜“ã€‚TensorFlowé€‚åˆç”Ÿäº§éƒ¨ç½²ï¼Œä½†å­¦ä¹ æ›²çº¿é™¡å³­ã€‚",
                        "timestamp": "2024-01-20T10:06:00Z"
                    }
                ],
                "session_1_date_time": "10:00 AM on 20 January, 2024",
                "session_2": [
                    {
                        "speaker": "å¼ ä¸‰",
                        "dia_id": "D2:1",
                        "text": "æ•°æ®é¢„å¤„ç†æ–¹é¢æœ‰ä»€ä¹ˆå»ºè®®å—ï¼Ÿæˆ‘çš„æ•°æ®é›†æœ‰å¾ˆå¤šç¼ºå¤±å€¼ã€‚",
                        "timestamp": "2024-01-20T14:00:00Z"
                    },
                    {
                        "speaker": "æå››", 
                        "dia_id": "D2:2",
                        "text": "ç¼ºå¤±å€¼å¯ä»¥ç”¨å‡å€¼å¡«å……ã€å‰å‘å¡«å……ï¼Œæˆ–è€…ç›´æ¥åˆ é™¤ã€‚è¦çœ‹å…·ä½“ä¸šåŠ¡åœºæ™¯ï¼Œæ—¶é—´åºåˆ—æ•°æ®å»ºè®®å‰å‘å¡«å……ã€‚",
                        "timestamp": "2024-01-20T14:02:00Z"
                    }
                ],
                "session_2_date_time": "2:00 PM on 20 January, 2024"
            },
            "qa": [
                {
                    "question": "å¼ ä¸‰åœ¨å­¦ä¹ ä»€ä¹ˆæŠ€æœ¯ï¼Ÿ",
                    "answer": "Pythonæœºå™¨å­¦ä¹ ", 
                    "evidence": ["D1:1"],
                    "category": 1
                },
                {
                    "question": "æå››æ¨èäº†å“ªäº›Pythonåº“ï¼Ÿ",
                    "answer": "scikit-learn, pandas, numpy, PyTorch",
                    "evidence": ["D1:2"],
                    "category": 1
                }
            ]
        },
        {
            "sample_id": "test_002",
            "user_id": "88888",
            "conversation": {
                "speaker_a": "ç‹åšå£«",
                "speaker_b": "åˆ˜æ•™æˆ", 
                "session_1": [
                    {
                        "speaker": "ç‹åšå£«",
                        "dia_id": "D1:1",
                        "text": "æˆ‘ä»¬çš„å¤§è¯­è¨€æ¨¡å‹å¯¹é½ç ”ç©¶è¿›å±•å¦‚ä½•ï¼ŸRLHFçš„æ•ˆæœæ€ä¹ˆæ ·ï¼Ÿ",
                        "timestamp": "2024-01-21T09:00:00Z"
                    },
                    {
                        "speaker": "åˆ˜æ•™æˆ",
                        "dia_id": "D1:2", 
                        "text": "RLHFåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šæå‡äº†15%å‡†ç¡®ç‡ã€‚æˆ‘ä»¬è¿˜åœ¨å°è¯•Constitutional AIçš„æ–¹æ³•ã€‚",
                        "timestamp": "2024-01-21T09:02:00Z"
                    },
                    {
                        "speaker": "ç‹åšå£«",
                        "dia_id": "D1:3",
                        "text": "Constitutional AIåœ¨å®‰å…¨æ€§æ–¹é¢è¡¨ç°å¦‚ä½•ï¼Ÿ",
                        "timestamp": "2024-01-21T09:04:00Z"
                    },
                    {
                        "speaker": "åˆ˜æ•™æˆ",
                        "dia_id": "D1:4",
                        "text": "åœ¨é¿å…æœ‰å®³è¾“å‡ºæ–¹é¢æ•ˆæœå¾ˆå¥½ï¼Œæˆ‘ä»¬è€ƒè™‘å°†ä¸¤ç§æ–¹æ³•ç»“åˆèµ·æ¥ä½¿ç”¨ã€‚",
                        "timestamp": "2024-01-21T09:06:00Z"
                    }
                ],
                "session_1_date_time": "9:00 AM on 21 January, 2024",
                "session_2": [
                    {
                        "speaker": "ç‹åšå£«",
                        "dia_id": "D2:1",
                        "text": "è®ºæ–‡å‡†å¤‡æŠ•ç¨¿åˆ°å“ªä¸ªä¼šè®®ï¼Ÿ",
                        "timestamp": "2024-01-21T15:00:00Z"
                    },
                    {
                        "speaker": "åˆ˜æ•™æˆ",
                        "dia_id": "D2:2",
                        "text": "è€ƒè™‘ICMLï¼Œä»Šå¹´ä»–ä»¬å¯¹AIå®‰å…¨è®®é¢˜æ¯”è¾ƒé‡è§†ï¼Œç¬¦åˆæˆ‘ä»¬çš„ç ”ç©¶æ–¹å‘ã€‚",
                        "timestamp": "2024-01-21T15:02:00Z"
                    }
                ],
                "session_2_date_time": "3:00 PM on 21 January, 2024"
            },
            "qa": [
                {
                    "question": "RLHFåœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†æå‡ï¼Ÿ",
                    "answer": "æ•°å­¦æ¨ç†ä»»åŠ¡ï¼Œæå‡äº†15%å‡†ç¡®ç‡",
                    "evidence": ["D1:2"], 
                    "category": 1
                },
                {
                    "question": "ä»–ä»¬è€ƒè™‘æŠ•ç¨¿åˆ°å“ªä¸ªä¼šè®®ï¼Ÿ",
                    "answer": "ICML",
                    "evidence": ["D2:2"],
                    "category": 1
                }
            ]
        },
        {
            "sample_id": "test_003",
            "user_id": "99999",
            "conversation": {
                "speaker_a": "å°æ˜",
                "speaker_b": "å°çº¢",
                "session_1": [
                    {
                        "speaker": "å°æ˜",
                        "dia_id": "D1:1",
                        "text": "å‘¨æœ«æˆ‘æƒ³å»çˆ¬å±±ï¼Œä½ è¦ä¸è¦ä¸€èµ·ï¼Ÿæˆ‘çŸ¥é“ä¸€ä¸ªåœ°æ–¹é£æ™¯ç‰¹åˆ«å¥½ã€‚",
                        "timestamp": "2024-01-22T18:00:00Z"
                    },
                    {
                        "speaker": "å°çº¢",
                        "dia_id": "D1:2",
                        "text": "å¥½å•Šï¼æˆ‘æœ€è¿‘åœ¨å­¦æ‘„å½±ï¼Œæ­£å¥½å¯ä»¥å¸¦ç›¸æœºå»æ‹é£æ™¯ç…§ã€‚",
                        "timestamp": "2024-01-22T18:02:00Z"
                    },
                    {
                        "speaker": "å°æ˜",
                        "dia_id": "D1:3", 
                        "text": "å¤ªå¥½äº†ï¼é‚£ä¸ªå±±é¡¶å¯ä»¥çœ‹æ—¥å‡ºï¼Œæˆ‘ä»¬æ—©ä¸Š4ç‚¹åŠå‡ºå‘ï¼Œ6ç‚¹èƒ½çœ‹åˆ°æ—¥å‡ºã€‚",
                        "timestamp": "2024-01-22T18:04:00Z"
                    },
                    {
                        "speaker": "å°çº¢",
                        "dia_id": "D1:4",
                        "text": "æ—¥å‡ºæ‘„å½±æˆ‘è¿˜æ²¡è¯•è¿‡ï¼Œæ­£å¥½ç»ƒä¹ ä¸€ä¸‹é•¿ç„¦é•œå¤´ã€‚æˆ‘ä»¬éœ€è¦å‡†å¤‡æ—©é¤å—ï¼Ÿ",
                        "timestamp": "2024-01-22T18:06:00Z"
                    }
                ],
                "session_1_date_time": "6:00 PM on 22 January, 2024",
                "session_2": [
                    {
                        "speaker": "å°æ˜",
                        "dia_id": "D2:1",
                        "text": "æˆ‘å¯ä»¥å‡†å¤‡ä¸‰æ˜æ²»ï¼Œä½ è´Ÿè´£å’–å•¡æ€ä¹ˆæ ·ï¼Ÿè®°å¾—ä½ å¾ˆä¼šæ³¡å’–å•¡ã€‚",
                        "timestamp": "2024-01-22T19:00:00Z"
                    },
                    {
                        "speaker": "å°çº¢", 
                        "dia_id": "D2:2",
                        "text": "æ²¡é—®é¢˜ï¼æˆ‘å¸¦ä¾¿æºçš„æ‰‹å†²è®¾å¤‡ï¼Œç”¨V60æ»¤æ¯ï¼Œå±±é¡¶å–ç°ç£¨å’–å•¡ä¸€å®šå¾ˆæ£’ã€‚",
                        "timestamp": "2024-01-22T19:02:00Z"
                    }
                ],
                "session_2_date_time": "7:00 PM on 22 January, 2024"
            },
            "qa": [
                {
                    "question": "ä»–ä»¬è®¡åˆ’å‡ ç‚¹å‡ºå‘å»çˆ¬å±±ï¼Ÿ",
                    "answer": "æ—©ä¸Š4ç‚¹åŠ",
                    "evidence": ["D1:3"],
                    "category": 2
                },
                {
                    "question": "å°çº¢åœ¨å­¦ä¹ ä»€ä¹ˆæŠ€èƒ½ï¼Ÿ",
                    "answer": "æ‘„å½±",
                    "evidence": ["D1:2"],
                    "category": 1
                }
            ]
        }
    ]
    
    #return pd.DataFrame(test_conversations)
    return test_conversations#pd.DataFrame(test_conversations)

async def search_query_async(client, query, metadata, frame, reversed_client=None, top_k=10):
    """Async version of search_query for nemori with unified memory support."""
    speaker_a_user_id = metadata.get("speaker_a_user_id")
    speaker_b_user_id = metadata.get("speaker_b_user_id")

    if not NEMORI_AVAILABLE:
        raise ImportError("Nemori is not available. Please install nemori.")
    
    # Check if this is a unified semantic version
    version = metadata.get("version", "default")
    
    context, duration_ms = await nemori_unified_search(
        unified_retrieval_service=client[0],
        retrieval_service=client[1], 
        query=query,
        speaker_a_user_id=speaker_a_user_id,
        speaker_b_user_id=speaker_b_user_id,
        top_k=top_k
    )
    
    return context, duration_ms


async def main_nemori(version: str = "test") -> bool:
    """ä¸»å‡½æ•°ï¼šè¿è¡ŒNemoriå¤„ç†å’Œæµ‹è¯•"""
    load_dotenv()
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    print("ğŸ“‹ ç”Ÿæˆæµ‹è¯•æ•°æ®...")
    locomo_df = create_test_locomo_data()
    print(f"âœ… åˆ›å»ºäº† {len(locomo_df)} ä¸ªæµ‹è¯•å¯¹è¯")
    print("locomo_df: ",locomo_df)
    print("\nğŸš€ å¼€å§‹Nemoriå®éªŒï¼ˆä½¿ç”¨æµ‹è¯•æ•°æ®ï¼‰")
    print("=" * 60)

    # åˆ›å»ºNemoriå®éªŒ
    experiment = NemoriExperiment(
        version=version, 
        episode_mode="speaker", 
        retrievalstrategy=RetrievalStrategy.EMBEDDING, 
        max_concurrency=1
    )

    # Step 1: è®¾ç½®LLMæä¾›è€…
    print("\nğŸ¤– è®¾ç½®LLMæä¾›è€…...")
    llm_config = {
        "api_key": "sk-NuO4dbNTkXXi5zWYkLFnhyug1kkqjeysvrb74pA9jTGfz8cm",
        "base_url": "https://jeniya.cn/v1",
        "model": "gpt-4o-mini"
    }
    
    llm_available = await experiment.setup_llm_provider(**llm_config)
    if not llm_available:
        print("âš ï¸ LLMä¸å¯ç”¨ï¼Œç»ˆæ­¢æµ‹è¯•")
        return False
    print("âœ… LLMè®¾ç½®æˆåŠŸ")

    # Step 2: åŠ è½½æµ‹è¯•æ•°æ®
    print("\nğŸ“Š åŠ è½½æ•°æ®...")
    # experiment.load_locomo_data(locomo_df)
    experiment.conversations = locomo_df
    print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {len(experiment.conversations)} ä¸ªå¯¹è¯")
    
    # Step 3: è®¾ç½®å­˜å‚¨å’Œæ£€ç´¢
    print("\nğŸ—„ è®¾ç½®å­˜å‚¨å’Œæ£€ç´¢...")
    embed_config = {
        "emb_api_key": "EMPTY",
        "emb_base_url": "http://localhost:6007/v1",
        "embed_model": "qwen3-emb"
    }
    await experiment.setup_storage_and_retrieval(**embed_config)
    print("âœ… å­˜å‚¨å’Œæ£€ç´¢è®¾ç½®æˆåŠŸ")

    # Step 4: æ„å»ºepisodeså¹¶è¿›è¡Œè¯­ä¹‰å‘ç°
    print("\nğŸ  æ„å»ºepisodeså¹¶è¿›è¡Œè¯­ä¹‰å‘ç°...")
    await experiment.build_episodes_semantic()
    # Use unified client for semantic versions
    
    top_k =20
    group_idx = 0
    user_id = 77777
    frame = "nemori"
    query = "question" 
    conversation = locomo_df[group_idx]["conversation"]#.iloc[group_idx]
    speaker_a = conversation.get("speaker_a")
    speaker_b = conversation.get("speaker_b")
    # Generate dynamic user IDs like in locomo_search_emb2.py
    speaker_a_user_id = f"{speaker_a.lower().replace(' ', '_')}_{group_idx}"
    speaker_b_user_id = f"{speaker_b.lower().replace(' ', '_')}_{group_idx}"
    speaker_a_user_id = f"{speaker_a.lower().replace(' ', '_')}_{user_id}"
    speaker_b_user_id = f"{speaker_b.lower().replace(' ', '_')}_{user_id}"
    
    conv_id = f"locomo_exp_user_{user_id}"
    unified_retrieval, retrieval_service, episode_repo, semantic_repo = await get_nemori_unified_client(
        version=version,
        retrievalstrategy=RetrievalStrategy.EMBEDDING,  # é»˜è®¤ä½¿ç”¨embeddingæœç´¢
        emb_api_key="EMPTY",
        emb_base_url="http://localhost:6007/v1", 
        embed_model="qwen3-emb"
    )
    client = (unified_retrieval, retrieval_service, episode_repo, semantic_repo)
    print(f"   ğŸ‘¥ Original speakers: '{speaker_a}' & '{speaker_b}'")
    print(f"   ğŸ†” Generated IDs: '{speaker_a_user_id}' & '{speaker_b_user_id}'")
    print(f"   ğŸ“ Conversation ID: '{conv_id}'")
    metadata = {
    "speaker_a": speaker_a,
    "speaker_b": speaker_b,
    "speaker_a_user_id": speaker_a_user_id,
    "speaker_b_user_id": speaker_b_user_id,
    "conv_idx": user_id,
    "conv_id": conv_id,
    "version": version,
}
    context, duration_ms = await search_query_async(client, query, metadata, frame, top_k=top_k)
    print(context, duration_ms)
    # æ˜¾ç¤ºç»“æœ
    print("\nğŸ‰ Nemoriæµ‹è¯•æˆåŠŸå®Œæˆ")
    print(f"âœ… æˆåŠŸå¤„ç† {len(experiment.conversations)} ä¸ªå¯¹è¯")
    print(f"âœ… åˆ›å»º {len(experiment.episodes)} ä¸ªepisodes")
    
    semantic_count = getattr(experiment, 'actual_semantic_count', 0)
    print(f"âœ… å‘ç° {semantic_count} ä¸ªè¯­ä¹‰æ¦‚å¿µ")
    
    if semantic_count > 0 and len(experiment.episodes) > 0:
        avg_concepts = semantic_count / len(experiment.episodes)
        print(f"ğŸ“Š æ¯ä¸ªepisodeå¹³å‡è¯­ä¹‰æ¦‚å¿µ: {avg_concepts:.1f}")

    # è¿›è¡ŒåŠŸèƒ½æµ‹è¯•
    print("\nğŸ§ª å¼€å§‹åŠŸèƒ½æµ‹è¯•...")
    
    # æµ‹è¯•è¯­ä¹‰å‘ç°
    # await show_semantic_discoveries(experiment)
    
    # æµ‹è¯•æ£€ç´¢åŠŸèƒ½
    # await test_retrieval_functionality(experiment)
    
    # æ€»ç»“
    print("\nğŸ‰ å…¨åŠŸèƒ½æµ‹è¯•å®Œæˆ!")
    print("=" * 60)
    print("æµ‹è¯•åœºæ™¯åŒ…æ‹¬:")
    print("  1. ğŸ¤– æœºå™¨å­¦ä¹ æŠ€æœ¯è®¨è®º (å¼ ä¸‰ & æå››)")
    print("  2. ğŸ“ AIç ”ç©¶å­¦æœ¯å¯¹è¯ (ç‹åšå£« & åˆ˜æ•™æˆ)")
    print("  3. ğŸ”ï¸ æˆ·å¤–æ‘„å½±æ´»åŠ¨è®¡åˆ’ (å°æ˜ & å°çº¢)")
    print("\næµ‹è¯•åŠŸèƒ½:")
    print("  âœ… Episodic Memory: å¯¹è¯åˆ†å‰²å’Œepisodesåˆ›å»º")
    print("  âœ… Semantic Memory: éšå«çŸ¥è¯†å‘ç°å’ŒæŠ½å–")
    print("  âœ… Unified Retrieval: å¤šç­–ç•¥æ£€ç´¢ç³»ç»Ÿ")
    print("  âœ… å®æ—¶åŠŸèƒ½éªŒè¯å’Œç»“æœå±•ç¤º")
    
    return True
        
 


def main(frame, version="test"):
    load_dotenv()
    if frame == "nemori":
        # Run async main for nemori
        return asyncio.run(main_nemori(version))
    else:
        print(f"âŒ ä¸æ”¯æŒçš„æ¡†æ¶: {frame}")
        return False


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Nemoriå…¨åŠŸèƒ½æµ‹è¯•è„šæœ¬ - å®Œå–„ç‰ˆ")
    parser.add_argument(
        "--lib",
        type=str,
        choices=["nemori"],
        default="nemori",
        help="ä½¿ç”¨nemoriæ¡†æ¶è¿›è¡Œæµ‹è¯•",
    )
    parser.add_argument(
        "--version",
        type=str,
        default="test",
        help="ç‰ˆæœ¬æ ‡è¯†ç¬¦ (ä¾‹å¦‚: test, demo, production)",
    )
    
    args = parser.parse_args()
    lib = args.lib
    version = args.version
    
    print(f"ğŸš€ å¼€å§‹è¿è¡Œ {lib} æµ‹è¯•ï¼Œç‰ˆæœ¬: {version}")
    success = main(lib, version)
    
    if success:
        print(f"\nâœ… {lib} æµ‹è¯•æˆåŠŸå®Œæˆï¼")
    else:
        print(f"\nâŒ {lib} æµ‹è¯•å¤±è´¥ï¼")
        exit(1)