import asyncio
from dotenv import load_dotenv
from sklearn import base

# ä»æˆ‘ä»¬åˆšåˆšåˆ›å»ºçš„æ–‡ä»¶ä¸­å¯¼å…¥ NemoriExperiment ç±»
from nemori_eval.experiment import NemoriExperiment, RetrievalQuery, RetrievalStrategy, RetrievalConfig

# --- å®šä¹‰æˆ‘ä»¬çš„ä¸­æ–‡å¯¹è¯æ ·ä¾‹æ•°æ® ---
# è¿™æ˜¯ä¸€ä¸ªåŒ…å«å•ä¸ªå¯¹è¯çš„åˆ—è¡¨
# æ¯ä¸ªå¯¹è¯æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å« conversation_id å’Œ messages åˆ—è¡¨
my_conversations = [
    {
        "conversation_id": "beijing_trip_plan",
        "messages": [
            {"speaker": "å°æ˜", "text": "ä½ å¥½å•Šï¼Œå°çº¢ï¼æœ€è¿‘åœ¨å¿™ä»€ä¹ˆï¼Ÿ", "timestamp": "2024-05-20T10:00:00Z"},
            {
                "speaker": "å°çº¢",
                "text": "æˆ‘æœ€è¿‘åœ¨å‡†å¤‡å»åŒ—äº¬æ—…æ¸¸ï¼Œæ­£åœ¨çœ‹æ”»ç•¥å‘¢ã€‚æ•…å®«å’Œé•¿åŸæ˜¯å¿…å»çš„ï¼",
                "timestamp": "2024-05-20T10:00:30Z",
            },
            {
                "speaker": "å°æ˜",
                "text": "å¬èµ·æ¥ä¸é”™ï¼åŒ—äº¬çƒ¤é¸­ä¹Ÿä¸€å®šè¦å°å°ã€‚ä½ æ‰“ç®—ä»€ä¹ˆæ—¶å€™å»ï¼Ÿ",
                "timestamp": "2024-05-20T10:01:00Z",
            },
            {
                "speaker": "å°çº¢",
                "text": "è®¡åˆ’ä¸‹ä¸ªæœˆï¼Œå¤§æ¦‚6æœˆ15å·å·¦å³å‡ºå‘ã€‚å¸Œæœ›èƒ½è®¢åˆ°åˆé€‚çš„æœºç¥¨å’Œé…’åº—ã€‚",
                "timestamp": "2024-05-20T10:01:30Z",
            },
        ],
    }
]
my_conversations = [
    {
        "conversation_id": "beijing_trip_plan",
        "messages": [
            {"speaker": "å°æ˜", "text": "ä½ å¥½å•Šï¼Œå°çº¢ï¼æœ€è¿‘åœ¨å¿™ä»€ä¹ˆï¼Ÿ", "timestamp": "2024-05-20T10:00:00Z"},
            {
                "speaker": "å°çº¢",
                "text": "æˆ‘æœ€è¿‘åœ¨å‡†å¤‡å»åŒ—äº¬æ—…æ¸¸ï¼Œæ­£åœ¨çœ‹æ”»ç•¥å‘¢ã€‚æ•…å®«å’Œé•¿åŸæ˜¯å¿…å»çš„ï¼",
                "timestamp": "2024-05-20T10:00:30Z",
            },
            {
                "speaker": "å°æ˜",
                "text": "å¬èµ·æ¥ä¸é”™ï¼åŒ—äº¬çƒ¤é¸­ä¹Ÿä¸€å®šè¦å°å°ã€‚ä½ æ‰“ç®—ä»€ä¹ˆæ—¶å€™å»ï¼Ÿ",
                "timestamp": "2024-05-20T10:01:00Z",
            },
            {
                "speaker": "å°çº¢",
                "text": "è®¡åˆ’ä¸‹ä¸ªæœˆï¼Œå¤§æ¦‚6æœˆ15å·å·¦å³å‡ºå‘ã€‚å¸Œæœ›èƒ½è®¢åˆ°åˆé€‚çš„æœºç¥¨å’Œé…’åº—ã€‚",
                "timestamp": "2024-05-20T10:01:30Z",
            },
        ],
    }
]

my_conversations = [
    {
        "user_id": "123456",
        "conversation": {
            "speaker_a": "å°æ˜",
            "speaker_b": "å°çº¢",
            "session_1_date_time": "1:56 pm on 8 May, 2023",
            "session_1": [
                {"speaker": "å°æ˜", "text": "ä½ å¥½å•Šï¼Œå°çº¢ï¼æœ€è¿‘åœ¨å¿™ä»€ä¹ˆï¼Ÿ", "timestamp": "2024-05-20T10:00:00Z"},
                {
                    "speaker": "å°çº¢",
                    "text": "æˆ‘æœ€è¿‘åœ¨å‡†å¤‡å»åŒ—äº¬æ—…æ¸¸ï¼Œæ­£åœ¨çœ‹æ”»ç•¥å‘¢ã€‚æ•…å®«å’Œé•¿åŸæ˜¯å¿…å»çš„ï¼",
                    "timestamp": "2024-05-20T10:00:30Z",
                },
                {
                    "speaker": "å°æ˜",
                    "text": "å¬èµ·æ¥ä¸é”™ï¼åŒ—äº¬çƒ¤é¸­ä¹Ÿä¸€å®šè¦å°å°ã€‚ä½ æ‰“ç®—ä»€ä¹ˆæ—¶å€™å»ï¼Ÿ",
                    "timestamp": "2024-05-20T10:01:00Z",
                },
                {
                    "speaker": "å°çº¢",
                    "text": "è®¡åˆ’ä¸‹ä¸ªæœˆï¼Œå¤§æ¦‚6æœˆ15å·å·¦å³å‡ºå‘ã€‚å¸Œæœ›èƒ½è®¢åˆ°åˆé€‚çš„æœºç¥¨å’Œé…’åº—ã€‚",
                    "timestamp": "2024-05-20T10:01:30Z",
                },
            ],
        },
    }
]


async def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    load_dotenv()
    print("ğŸš€ Starting Nemori Experiment...")
    print("==================================================")

    # 1. åˆå§‹åŒ–å®éªŒ
    # retrieval_config = RetrievalConfig.from_env(strategy=RetrievalStrategy.EMBEDDING)
    experiment = NemoriExperiment(
        version="my-first-run", episode_mode="speaker", retrievalstrategy=RetrievalStrategy.EMBEDDING
    )

    # 2. å®šä¹‰æŸ¥è¯¢
    query_text = "å°çº¢çš„åŒ—äº¬æ—…æ¸¸è®¡åˆ’æ˜¯ä»€ä¹ˆï¼Ÿ"
    # æˆ‘ä»¬æƒ³ä»å°çº¢çš„è§†è§’æ¥æŸ¥è¯¢
    query_owner_name = "å°çº¢"
    api_key = "sk-NuO4dbNTkXXi5zWYkLFnhyug1kkqjeysvrb74pA9jTGfz8cm"
    base_url = "https://jeniya.cn/v1"
    model = "gpt-4o-mini"
    llm_available = await experiment.setup_llm_provider(model=model, api_key=api_key, base_url=base_url)
    if not llm_available:
        print("âš ï¸ Continuing with fallback mode (no LLM)")

    # Step 2: Load data
    experiment.load_conversation_data(my_conversations)
    api_key = "EMPTY"
    base_url = "http://localhost:6007/v1"
    model = "qwen3-emb"
    # Step 3: Setup storage and retrieval
    await experiment.setup_storage_and_retrieval(emb_api_key=api_key, emb_base_url=base_url, embed_model=model)
    # Create tasks for all conversations
    # tasks = [
    #     experiment.process_conversation(i, conv_data) for i, conv_data in enumerate(my_conversations)
    # ]
    # print(f"   ğŸ“‹ Created {len(tasks)} concurrent tasks")

    # # Wait for all tasks to complete
    # print("   â³ Starting concurrent execution...")
    # results = await asyncio.gather(*tasks)
    # print("   ğŸ All conversations completed")

    # # Collect all episodes
    # for _, episodes in results:
    #     experiment.episodes.extend(episodes)

    # print("\nğŸ“Š Episode Building Complete")
    # print(f"âœ… Successfully created {len(experiment.episodes)} episodes")
    # #await experiment.build_bm25_indices()
    # await experiment.build_embedding_indices()
    # Step 4: Build episodes
    await experiment.build_episodes()

    print("\nğŸ‰ Nemori Ingestion Complete")
    print(f"âœ… Successfully processed {len(experiment.conversations)} conversations")
    print(f"âœ… Created {len(experiment.episodes)} episodes")
    for i in experiment.episodes:
        print(i.owner_id)
    print("\nğŸ‰ Experiment finished.")
    # --- Part 2: Querying ---
    conversation_id = "123456"  # my_conversations[0]["conversation_id"]
    # Query 1: From å°çº¢'s perspective
    query_owner_id_xiao_hong = f"{query_owner_name}_{conversation_id}"
    print("\n" + "=" * 50)
    print(f"ğŸ” Running Query from '{query_owner_id_xiao_hong}' perspective")
    print(f"   â“ Query: '{query_text}'")
    print("=" * 50)

    # 1. Create a retrieval query object
    retrieval_query = RetrievalQuery(
        text=query_text,
        owner_id=query_owner_id_xiao_hong,
        limit=25,  # Retrieve top 5 relevant episodes
        strategy=RetrievalStrategy.EMBEDDING,
    )

    # 2. Perform the search
    result_a = await experiment.retrieval_service.search(retrieval_query)

    print(result_a)
    # --- FIX 2: Get the length of the .results list ---
    print(f"   âœ… Found {len(result_a.episodes)} relevant episode(s):")

    # 3. Display the results
    for i, episode in enumerate(result_a.episodes[:2]):
        print(f"     {i+1}. Title: '{episode.title}'")
        print(f"        Content: '{episode.content}...'")
        print(f"        Summary: '{episode.summary}'")


if __name__ == "__main__":

    asyncio.run(main())
