"""
Semantic discovery engine for identifying private domain knowledge.

This module implements the core semantic discovery functionality using differential
analysis between episodic compression and LLM reconstruction capabilities.
"""

import asyncio
import json
from typing import Any, Dict, List

from ..core.data_types import SemanticNode
from ..core.episode import Episode
from ..llm.protocol import LLMProvider
from ..retrieval import RetrievalStrategy
from .unified_retrieval import UnifiedRetrievalService


class ContextAwareSemanticDiscoveryEngine:
    """
    Context-aware engine for discovering semantic knowledge through differential analysis.
    é€šè¿‡å·®åˆ†åˆ†æå‘ç°è¯­ä¹‰çŸ¥è¯†çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥å¼•æ“ã€‚
    """

    def __init__(self, llm_provider: LLMProvider, retrieval_service: UnifiedRetrievalService):
        self.llm_provider = llm_provider
        self.retrieval_service = retrieval_service
        self.topk = 10

    async def discover_semantic_knowledge(self, episode: Episode, original_content: str) -> list[SemanticNode]:
        """
        Discover semantic knowledge with context from related memories.
        åˆ©ç”¨ç›¸å…³è®°å¿†çš„ä¸Šä¸‹æ–‡å‘ç°è¯­ä¹‰çŸ¥è¯†ã€‚

        Process | æµç¨‹:
        1. Gather related semantic memories and historical episodes | æ”¶é›†ç›¸å…³è¯­ä¹‰è®°å¿†å’Œå†å²æƒ…æ™¯
        2. Use episode as mask to reconstruct original | ä½¿ç”¨æƒ…æ™¯ä½œä¸ºæ©ç é‡å»ºåŸå§‹å†…å®¹
        3. Compare reconstructed vs original with context | ç»“åˆä¸Šä¸‹æ–‡æ¯”è¾ƒé‡å»ºä¸åŸå§‹å†…å®¹
        4. Extract knowledge gaps as semantic nodes | æå–çŸ¥è¯†å·®è·ä½œä¸ºè¯­ä¹‰èŠ‚ç‚¹
        """
        # Step 1: Gather context from related memories
        # æ­¥éª¤1ï¼šä»ç›¸å…³è®°å¿†ä¸­æ”¶é›†ä¸Šä¸‹æ–‡
        context = await self._gather_discovery_context(episode)
        #context = {}
        # Step 2: Context-aware reconstruction
        # æ­¥éª¤2ï¼šä¸Šä¸‹æ–‡æ„ŸçŸ¥é‡å»º
        reconstructed_content = await self._reconstruct_with_context(episode, context)
        print("reconstructed_content:",reconstructed_content)
        # Step 3: Perform differential analysis
        # æ­¥éª¤3ï¼šæ‰§è¡Œå·®åˆ†åˆ†æ
        knowledge_gaps = await self._analyze_knowledge_gaps(
            original=original_content, reconstructed=reconstructed_content, episode=episode, context=context
        )

        # Step 4: Create semantic nodes with bidirectional links
        # æ­¥éª¤4ï¼šåˆ›å»ºå¸¦åŒå‘é“¾æ¥çš„è¯­ä¹‰èŠ‚ç‚¹
        
        # Clean owner_id to remove segment suffix for consistent indexing
        # æ¸…ç†owner_idï¼Œå»æ‰segmentåç¼€ä»¥ç¡®ä¿ç´¢å¼•ä¸€è‡´æ€§
        clean_owner_id = self._clean_owner_id(episode.owner_id)
        print(f"   ğŸ”§ Cleaning owner_id: '{episode.owner_id}' -> '{clean_owner_id}'")
        
        semantic_nodes = []
        for gap in knowledge_gaps:
            node = SemanticNode(
                owner_id=clean_owner_id,  # Use cleaned owner_id
                key=gap.get("key", ""),
                value=gap.get("value", ""),
                context=gap.get("context", ""),
                discovery_episode_id=episode.episode_id,
                linked_episode_ids=[episode.episode_id],  # Initial linking
                confidence=gap.get("confidence", 1.0),
                search_keywords=self._extract_keywords(gap),
            )
            semantic_nodes.append(node)

        return semantic_nodes

    async def _gather_discovery_context(self, episode: Episode) -> dict[str, Any]:
        """
        Gather related semantic memories and historical episodes for context.
        æ”¶é›†ç›¸å…³è¯­ä¹‰è®°å¿†å’Œå†å²æƒ…æ™¯ä½œä¸ºä¸Šä¸‹æ–‡ã€‚
        """
        print(f"ğŸ” Gathering discovery context for episode {episode.episode_id} (owner: {episode.owner_id})")
        
        # Search for related semantic memories
        # æœç´¢ç›¸å…³è¯­ä¹‰è®°å¿†
        query_text = f"{episode.title} {episode.content}"
        print(f"   ğŸ§  Searching semantic memories with query: '{query_text[:100]}...'")
        related_semantics = await self.retrieval_service.search_semantic_memories(
            owner_id=episode.owner_id, query=query_text, limit=5 #self.topk#self.topk * 2
        )
        print(f"   âœ… Found {len(related_semantics)} related semantic memories")

        # Search for related historical episodes (excluding current episode)
        # æœç´¢ç›¸å…³å†å²æƒ…æ™¯ï¼ˆæ’é™¤å½“å‰æƒ…æ™¯ï¼‰
        episode_query = f"{episode.content}"
        print(f"   ğŸ“š Searching episodic memories with query: '{episode_query[:100]}...'")
        
        related_episodes_result = await self.retrieval_service.search_episodic_memories(
            owner_id=episode.owner_id, query=episode_query, limit=3 #self.topk // 2# Increase limit to get more candidates
        )
        # æ­¥éª¤ 3: æå–å¹¶ç»Ÿä¸€æ•°æ®ç±»å‹ (è¿™æ˜¯æ ¸å¿ƒä¿®å¤)
        # ----------------------------------------------------
        # V V V V V V V  æ²»æœ¬çš„å…³é”®ä¿®æ”¹  V V V V V V V
        # ----------------------------------------------------
        
        raw_episodes = []
        if hasattr(related_episodes_result, 'episodes'):
            raw_episodes = related_episodes_result.episodes
        elif related_episodes_result:
            raw_episodes = related_episodes_result

        uniform_episodes = []
        if raw_episodes:
            for item in raw_episodes:
                if isinstance(item, Episode):
                    # å¦‚æœå·²ç»æ˜¯ Episode å¯¹è±¡ï¼Œç›´æ¥æ·»åŠ 
                    uniform_episodes.append(item)
                elif isinstance(item, dict):
                    # å¦‚æœæ˜¯å­—å…¸ï¼Œåˆ™å®ä¾‹åŒ–ä¸º Episode å¯¹è±¡
                    try:
                        # ä½¿ç”¨å­—å…¸è§£åŒ…æ¥åˆ›å»º Episode å®ä¾‹
                        # è¿™è¦æ±‚å­—å…¸çš„é”®ä¸ Episode çš„ __init__ å‚æ•°ååŒ¹é…
                        uniform_episodes.append(Episode(**item))
                    except TypeError as e:
                        print(f"   âš ï¸ Warning: Failed to instantiate Episode from dict due to TypeError: {e}. Dict keys: {item.keys()}. Skipping item.")
                else:
                    print(f"   âš ï¸ Warning: Found an unexpected type in episodic search results: {type(item)}. Skipping item.")
        
        print(f"   ğŸ› ï¸  Unified {len(uniform_episodes)} items into Episode objects.")

        # æ­¥éª¤ 4: è¿‡æ»¤æ‰å½“å‰æ­£åœ¨å¤„ç†çš„ episode
        # è¿™ä¸€æ­¥ä¹Ÿå¾ˆé‡è¦ï¼Œä¸€ä¸ªç‰‡æ®µçš„ä¸Šä¸‹æ–‡ä¸åº”è¯¥åŒ…å«å®ƒè‡ªå·±
        filtered_episodes = [
            ep for ep in uniform_episodes if ep.episode_id != episode.episode_id
        ]
        
        print(f"   âœ… Found {len(uniform_episodes)} total episodes, returning {len(filtered_episodes)} historical episodes for context.")
        
        # ----------------------------------------------------
        # ^ ^ ^ ^ ^ ^ ^  æ²»æœ¬çš„å…³é”®ä¿®æ”¹ç»“æŸ  ^ ^ ^ ^ ^ ^ ^
        # ----------------------------------------------------
        
        return {
            "related_semantic_memories": related_semantics,
            "related_historical_episodes": filtered_episodes, # è¿”å›ç»è¿‡å¤„ç†å’Œè¿‡æ»¤çš„åˆ—è¡¨
            "current_episode": episode,
        }
        # # Handle the search result properly - it might be EpisodeSearchResult or direct episodes list
        # if hasattr(related_episodes_result, 'episodes'):
        #     related_episodes = related_episodes_result.episodes
        # else:
        #     related_episodes = related_episodes_result if related_episodes_result else []

        # # # Only filter out current episode if we actually found some episodes
        # # # åªæœ‰åœ¨å®é™…æ‰¾åˆ°episodesçš„æƒ…å†µä¸‹æ‰è¿‡æ»¤å½“å‰episode
        # # filtered_episodes = []
        # # if related_episodes:
        # #     for ep in related_episodes:
        # #         if ep.episode_id != episode.episode_id:
        # #             filtered_episodes.append(ep)
        # #     print(f"   âœ… Found {len(related_episodes)} total episodes, {len(filtered_episodes)} historical episodes (excluding current)")
            
        # #     # If we filtered out everything, it means we only found the current episode
        # #     if len(related_episodes) > 0 and len(filtered_episodes) == 0:
        # #         print(f"   âš ï¸ Only found current episode in search results - no historical episodes available")
        # # else:
        # #     print(f"   âœ… Found {len(related_episodes)} episodes (none to filter)")
        
        # # # Log some details about found episodes for debugging
        # # if filtered_episodes:
        # #     print(f"   ğŸ“ Sample historical episodes:")
        # #     for i, ep in enumerate(filtered_episodes[:2]):  # Show first 2
        # #         content_preview = ep.content[:50] + "..." if len(ep.content) > 50 else ep.content
        # #         print(f"      {i+1}. {ep.episode_id}: {content_preview}")
        # # else:
        # #     print(f"   âš ï¸ No historical episodes available for context")

        # return {
        #     "related_semantic_memories": related_semantics,
        #     "related_historical_episodes": related_episodes,
        #     "current_episode": episode,
        # }


    async def _reconstruct_with_context(self, episode: Episode, context: dict[str, Any]) -> str:
        """
        Reconstruct original conversation from episodic summary using context.
        åˆ©ç”¨ä¸Šä¸‹æ–‡ä»æƒ…æ™¯æ‘˜è¦é‡å»ºåŸå§‹å¯¹è¯ã€‚
        """
        reconstruction_prompt = self._build_reconstruction_prompt(episode, context)
        print("reconstruction_prompt:",reconstruction_prompt)
        response = await self.llm_provider.generate(prompt=reconstruction_prompt, temperature=0.3)
        return response.strip()

    async def _analyze_knowledge_gaps(
        self, original: str, reconstructed: str, episode: Episode, context: dict[str, Any]
    ) -> list[dict[str, Any]]:
        """
        Analyze differences between original and reconstructed content to identify knowledge gaps.
        åˆ†æåŸå§‹å†…å®¹ä¸é‡å»ºå†…å®¹çš„å·®å¼‚ä»¥è¯†åˆ«çŸ¥è¯†å·®è·ã€‚
        """
        analysis_prompt = self._build_knowledge_gap_analysis_prompt(original, reconstructed, episode, context)
        print("analysis_prompt:",analysis_prompt)
        response = await self.llm_provider.generate(prompt=analysis_prompt, temperature=0.1)
        print("analysis_gaps:",response)
        #print(self.llm_provider.model,response)
        # æå– JSON å­—ç¬¦ä¸²éƒ¨åˆ†
        start_idx = response.find('{')
        end_idx = response.rfind('}') + 1
        if start_idx == -1 or end_idx == 0:
            raise ValueError("å“åº”ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„ JSON éƒ¨åˆ†")
        judge_string = response[start_idx:end_idx]
        analysis_result = json.loads(judge_string.strip())
        return analysis_result.get("knowledge_gaps", [])

    def _build_reconstruction_prompt(self, episode: Episode, context: dict[str, Any]) -> str:
        """Build prompt for content reconstruction."""
        related_context = ""
        try:
            if context:
                # Convert context to JSON-serializable format
                serializable_context = {
                    "related_semantic_memories": [
                        {"key": node.key, "value": node.value, "context": node.context} for node in context.get("related_semantic_memories", [])
                    ],
                    "related_episodes_memories": [
                        {"title": ep.title, "content": ep.content} for ep in context.get("related_historical_episodes", [])
                    ],
                    "current_episode": (
                        {"title": context["current_episode"].title, "content": context["current_episode"].content}
                        if "current_episode" in context
                        else None
                    ),
                }
                related_context = (
                    f"\n{json.dumps(serializable_context, indent=2, ensure_ascii=False)}\n"
                )
        except:
            print("error!!!!!! context:",context)
#         return f"""You are an expert at reconstructing original conversations from episodic summaries.
# æ‚¨æ˜¯ä»æƒ…æ™¯æ‘˜è¦é‡å»ºåŸå§‹å¯¹è¯çš„ä¸“å®¶ã€‚

# Given this related episodes memories and related semantic memories:
# ç»™å®šä»¥ä¸‹ç›¸å…³çš„æƒ…æ™¯è®°å¿†å’Œè¯­ä¹‰è®°å¿†ï¼š
# {related_context}

# Please reconstruct what the original conversation might have looked like, using your general world knowledge.
# è¯·ä½¿ç”¨æ‚¨çš„é€šç”¨ä¸–ç•ŒçŸ¥è¯†é‡å»ºåŸå§‹å¯¹è¯å¯èƒ½çš„æ ·å­ã€‚

# Important guidelines | é‡è¦å‡†åˆ™:
# 1. Use only common knowledge that a typical LLM would know | åªä½¿ç”¨å…¸å‹å¤§è¯­è¨€æ¨¡å‹ä¼šçŸ¥é“çš„å¸¸è¯†
# 2. Make reasonable assumptions for missing details | å¯¹ç¼ºå¤±ç»†èŠ‚åšåˆç†å‡è®¾
# 3. Focus on factual reconstruction, not creative interpretation | ä¸“æ³¨äºäº‹å®é‡å»ºï¼Œè€Œéåˆ›æ„è§£é‡Š
# 4. Maintain the same conversation structure and flow | ä¿æŒç›¸åŒçš„å¯¹è¯ç»“æ„å’Œæµç¨‹

# Return the reconstructed conversation:
# è¿”å›é‡å»ºçš„å¯¹è¯ï¼š"""
#         return f"""You are an expert at reconstructing original conversations from episodic summaries.
# æ‚¨æ˜¯ä»æƒ…æ™¯æ‘˜è¦é‡å»ºåŸå§‹å¯¹è¯çš„ä¸“å®¶ã€‚

# Given this episodic memory:
# ç»™å®šä»¥ä¸‹æƒ…æ™¯è®°å¿†ï¼š
# Title: {episode.title}
# Summary: {episode.summary}
# Content: {episode.content}{related_context}

# Please reconstruct what the original conversation might have looked like, using your general world knowledge.
# è¯·ä½¿ç”¨æ‚¨çš„é€šç”¨ä¸–ç•ŒçŸ¥è¯†é‡å»ºåŸå§‹å¯¹è¯å¯èƒ½çš„æ ·å­ã€‚

# Important guidelines | é‡è¦å‡†åˆ™:
# 1. Use only common knowledge that a typical LLM would know | åªä½¿ç”¨å…¸å‹å¤§è¯­è¨€æ¨¡å‹ä¼šçŸ¥é“çš„å¸¸è¯†
# 2. Make reasonable assumptions for missing details | å¯¹ç¼ºå¤±ç»†èŠ‚åšåˆç†å‡è®¾
# 3. Focus on factual reconstruction, not creative interpretation | ä¸“æ³¨äºäº‹å®é‡å»ºï¼Œè€Œéåˆ›æ„è§£é‡Š
# 4. Maintain the same conversation structure and flow | ä¿æŒç›¸åŒçš„å¯¹è¯ç»“æ„å’Œæµç¨‹

# Return the reconstructed conversation:
# è¿”å›é‡å»ºçš„å¯¹è¯ï¼š"""
        return f"""You are an expert at reconstructing original conversations from episodic summaries.

Given this episodic memory:
Title: {episode.title}
Summary: {episode.summary}
Content: {episode.content}{related_context}

Please reconstruct what the original conversation might have looked like, using your general world knowledge.

Important guidelines 
1. Use only common knowledge that a typical LLM would know 
2. Make reasonable assumptions for missing details 
3. Focus on factual reconstruction, not creative interpretation 
4. Maintain the same conversation structure and flow 

Return the reconstructed conversation:
è¿”å›é‡å»ºçš„å¯¹è¯ï¼š"""
    def _build_knowledge_gap_analysis_prompt(
        self, original: str, reconstructed: str, episode: Episode, context: dict[str, Any]
    ) -> str:
        """Build prompt for knowledge gap analysis."""
#         return f"""
# You are a meticulous, high-fidelity knowledge analyst. Your mission is to perform a detailed comparison between an 'Original Content' block, which represents private domain truth, and a 'Reconstructed Content' block, which is an attempt to summarize or infer that truth.

# æ‚¨æ˜¯ä¸€åä¸€ä¸ä¸è‹Ÿçš„é«˜ä¿çœŸçŸ¥è¯†åˆ†æå¸ˆã€‚æ‚¨çš„ä»»åŠ¡æ˜¯è¯¦ç»†æ¯”è¾ƒâ€œåŸå§‹å†…å®¹â€ï¼ˆä»£è¡¨ç§åŸŸçœŸç†ï¼‰å’Œâ€œé‡å»ºå†…å®¹â€ï¼ˆå°è¯•æ€»ç»“æˆ–æ¨æ–­è¯¥çœŸç†ï¼‰ä¹‹é—´çš„å·®å¼‚ã€‚

# Your goal is to precisely identify pieces of private, non-public knowledge that are present in the original but are either missing, incorrectly stated, over-generalized, or inferred without sufficient evidence in the reconstruction.

# æ‚¨çš„ç›®æ ‡æ˜¯ç²¾ç¡®è¯†åˆ«å‡ºåŸå§‹å†…å®¹ä¸­å­˜åœ¨çš„ï¼Œä½†åœ¨é‡å»ºå†…å®¹ä¸­ç¼ºå¤±ã€é™ˆè¿°é”™è¯¯ã€è¿‡åº¦æ³›åŒ–æˆ–åœ¨è¯æ®ä¸è¶³çš„æƒ…å†µä¸‹è¢«æ¨æ–­å‡ºæ¥çš„ç§åŸŸã€éå…¬å¼€çŸ¥è¯†ã€‚

# Original content | åŸå§‹å†…å®¹:
# {original}

# Reconstructed content | é‡å»ºå†…å®¹:
# {reconstructed}
# Please identify specific pieces of information that exist in the original but are missing or incorrectly assumed in the reconstruction. These represent private domain knowledge.
# è¯·è¯†åˆ«åŸå§‹å†…å®¹ä¸­å­˜åœ¨ä½†åœ¨é‡å»ºä¸­ç¼ºå¤±æˆ–é”™è¯¯å‡è®¾çš„å…·ä½“ä¿¡æ¯ã€‚è¿™äº›ä»£è¡¨ç§åŸŸçŸ¥è¯†ã€‚

# Focus on | å…³æ³¨:
# 1. Proper names, project names, specific terminology | ä¸“æœ‰åè¯ã€é¡¹ç›®åç§°ã€ç‰¹å®šæœ¯è¯­
# 2. Personal preferences, habits, and characteristics | ä¸ªäººåå¥½ã€ä¹ æƒ¯å’Œç‰¹å¾
# 3. Specific facts, dates, numbers that differ | å…·ä½“çš„äº‹å®ã€æ—¥æœŸã€æ•°å­—å·®å¼‚
# 4. Context-specific meanings and interpretations | ä¸Šä¸‹æ–‡ç‰¹å®šçš„å«ä¹‰å’Œè§£é‡Š
# 53.  Focus on the loss of fidelity and the introduction of assumptions. | ä¸“æ³¨äºä¿çœŸåº¦çš„æŸå¤±å’Œå‡è®¾çš„å¼•å…¥ã€‚

# Return your analysis in JSON format:
# ä»¥ JSON æ ¼å¼è¿”å›åˆ†æï¼š
# {{
#     "knowledge_gaps": [
#         {{  
#             "analysis": "A brief explanation of WHY this is a knowledge gap. Explain the nature of the mismatch (e.g., generalization, assumption, factual error).",
#             "key": "specific identifier or topic",
#             "value": "the correct private knowledge",
#             "context": "surrounding context from original",
#             "gap_type": "proper_noun|personal_fact|specific_detail|contextual_meaning",
#             "confidence": 0.0-1.0
#         }}
#     ]
# }}"""
#         return f"""You are an expert at identifying private domain knowledge gaps.
# æ‚¨æ˜¯è¯†åˆ«ç§åŸŸçŸ¥è¯†å·®è·çš„ä¸“å®¶ã€‚

# Original content | åŸå§‹å†…å®¹:
# {original}

# Reconstructed content (using general LLM knowledge) | é‡å»ºå†…å®¹ï¼ˆä½¿ç”¨é€šç”¨å¤§è¯­è¨€æ¨¡å‹çŸ¥è¯†ï¼‰:
# {reconstructed}

# Please identify specific pieces of information that exist in the original but are missing or incorrectly assumed in the reconstruction. These represent private domain knowledge.
# è¯·è¯†åˆ«åŸå§‹å†…å®¹ä¸­å­˜åœ¨ä½†åœ¨é‡å»ºä¸­ç¼ºå¤±æˆ–é”™è¯¯å‡è®¾çš„å…·ä½“ä¿¡æ¯ã€‚è¿™äº›ä»£è¡¨ç§åŸŸçŸ¥è¯†ã€‚

# Focus on | å…³æ³¨:
# 1. Proper names, project names, specific terminology | ä¸“æœ‰åè¯ã€é¡¹ç›®åç§°ã€ç‰¹å®šæœ¯è¯­
# 2. Personal preferences, habits, and characteristics | ä¸ªäººåå¥½ã€ä¹ æƒ¯å’Œç‰¹å¾
# 3. Specific facts, dates, numbers that differ | å…·ä½“çš„äº‹å®ã€æ—¥æœŸã€æ•°å­—å·®å¼‚
# 4. Context-specific meanings and interpretations | ä¸Šä¸‹æ–‡ç‰¹å®šçš„å«ä¹‰å’Œè§£é‡Š

# Return your analysis in JSON format:
# ä»¥ JSON æ ¼å¼è¿”å›åˆ†æï¼š
# {{
#     "knowledge_gaps": [
#         {{
#             "key": "specific identifier or topic",
#             "value": "the correct private knowledge",
#             "context": "surrounding context from original",
#             "gap_type": "proper_noun|personal_fact|specific_detail|contextual_meaning",
#             "confidence": 0.0-1.0
#         }}
#     ]
# }}"""
    
        return f"""You are an expert at identifying private domain knowledge gaps. Your primary goal is to find what specific, private information the general LLM is missing, based only on the provided original content.
æ‚¨æ˜¯è¯†åˆ«ç§åŸŸçŸ¥è¯†å·®è·çš„ä¸“å®¶ã€‚æ‚¨çš„é¦–è¦ç›®æ ‡æ˜¯ä»…æ ¹æ®æä¾›çš„åŸå§‹å†…å®¹ï¼Œæ‰¾å‡ºé€šç”¨å¤§è¯­è¨€æ¨¡å‹æ‰€ç¼ºä¹çš„ç‰¹å®šç§åŸŸä¿¡æ¯ã€‚

Original content 
{original}

Reconstructed content (using general LLM knowledge) 
{reconstructed}

Task | ä»»åŠ¡:
Please identify specific pieces of information that exist in the original but are missing or incorrectly assumed in the reconstruction. These represent private domain knowledge.
è¯·è¯†åˆ«åŸå§‹å†…å®¹ä¸­å­˜åœ¨ä½†åœ¨é‡å»ºä¸­ç¼ºå¤±æˆ–é”™è¯¯å‡è®¾çš„å…·ä½“ä¿¡æ¯ã€‚è¿™äº›ä»£è¡¨ç§åŸŸçŸ¥è¯†ã€‚


å…³é”®ï¼šä»…å…³æ³¨é«˜ä»·å€¼çŸ¥è¯†

ä»…æå–ç¬¦åˆä»¥ä¸‹æ ‡å‡†çš„çŸ¥è¯†ï¼š

   æŒä¹…æ€§æµ‹è¯•ï¼šè¯¥ä¿¡æ¯åœ¨6ä¸ªæœˆåæ˜¯å¦ä¾ç„¶æœ‰æ•ˆï¼Ÿ
   å…·ä½“æ€§æµ‹è¯•ï¼šå®ƒæ˜¯å¦åŒ…å«å…·ä½“ã€å¯æœç´¢çš„ä¿¡æ¯ï¼Ÿ
   å®ç”¨æ€§æµ‹è¯•ï¼šå®ƒæ˜¯å¦æœ‰åŠ©äºé¢„æµ‹æœªæ¥çš„ç”¨æˆ·éœ€æ±‚æˆ–åå¥½ï¼Ÿ
   ç‹¬ç«‹æ€§æµ‹è¯•ï¼šè„±ç¦»å¯¹è¯ä¸Šä¸‹æ–‡åï¼Œè¯¥ä¿¡æ¯æ˜¯å¦ä»èƒ½è¢«ç†è§£ï¼Ÿ

é«˜ä»·å€¼çŸ¥è¯†ç±»åˆ«ï¼ˆæå–è¿™äº›ï¼‰ï¼š

1.  èº«ä»½ä¸èƒŒæ™¯ï¼šå§“åã€èŒä¸šã€å…¬å¸ã€æ•™è‚²èƒŒæ™¯
2.  é•¿æœŸåå¥½ï¼šå–œæ¬¢çš„ä¹¦ç±/ç”µå½±/å·¥å…·ï¼Œé•¿æœŸçš„å¥½æ¶
3.  æŠ€æœ¯ç»†èŠ‚ï¼šæŠ€æœ¯ã€ç‰ˆæœ¬ã€æ–¹æ³•è®ºã€æ¶æ„
4.  äººé™…å…³ç³»ï¼šå®¶åº­ã€åŒäº‹ã€å›¢é˜Ÿæˆå‘˜ã€å¯¼å¸ˆ
5.  ç›®æ ‡ä¸è®¡åˆ’ï¼šèŒä¸šç›®æ ‡ã€å­¦ä¹ ç›®æ ‡ã€é¡¹ç›®è®¡åˆ’
6.  ä¿¡å¿µä¸ä»·å€¼è§‚ï¼šåŸåˆ™ã€ç†å¿µã€é²œæ˜è§‚ç‚¹
7.  ä¹ æƒ¯ä¸æ¨¡å¼ï¼šå¸¸è§„æ´»åŠ¨ã€å·¥ä½œæµç¨‹ã€æ—¥ç¨‹å®‰æ’

ä½ä»·å€¼çŸ¥è¯†ï¼ˆå¿½ç•¥è¿™äº›ï¼‰ï¼š

   æš‚æ—¶æ€§çš„æƒ…ç»ªæˆ–ååº”
   å•æ¬¡å¯¹è¯ä¸­çš„ç¡®è®¤æˆ–å›åº”
   ç¼ºä¹ç»†èŠ‚çš„æ¨¡ç³Šé™ˆè¿°
   ä¾èµ–äºä¸Šä¸‹æ–‡çš„ä¿¡æ¯

æŒ‡å¯¼åŸåˆ™ï¼š

1.  æ¯æ¡çŸ¥è¯†é™ˆè¿°åº”è‡ªæˆä¸€ä½“ä¸”ä¸å¯å†åˆ†ã€‚
2.  åŒ…å«æ‰€æœ‰å…·ä½“ç»†èŠ‚ï¼ˆå§“åã€ç‰ˆæœ¬ã€æ ‡é¢˜ç­‰ï¼‰ã€‚
3.  å¯¹äºé•¿æœŸä¸å˜çš„äº‹å®ï¼Œä½¿ç”¨ç°åœ¨æ—¶æ€ã€‚
4.  ä¸“æ³¨äºæœ‰åŠ©äºé•¿æœŸç†è§£ç”¨æˆ·çš„äº‹å®ã€‚
5.  é™ˆè¿°ä¸­ä¸è¦åŒ…å«æ—¶é—´/æ—¥æœŸä¿¡æ¯ã€‚
6.  è´¨é‡ä¼˜äºæ•°é‡â€”â€”å°‘æ•°å‡ æ¡æœ‰ä»·å€¼çš„é™ˆè¿°èƒœè¿‡å¤§é‡æ— ä»·å€¼çš„é™ˆè¿°ã€‚

ç¤ºä¾‹ï¼š

å¥½çš„ç¤ºä¾‹ï¼šâ€œå¡ç½—ç³æœ€å–œæ¬¢çš„ä¹¦æ˜¯è‰¾ç±³Â·åŸƒåˆ©æ–¯Â·çº³ç‰¹ï¼ˆAmy Ellis Nuttï¼‰çš„ã€Šæˆä¸ºå¦®å¯ã€‹ï¼ˆBecoming Nicoleï¼‰â€
å¥½çš„ç¤ºä¾‹ï¼šâ€œè¯¥ç”¨æˆ·åœ¨å­—èŠ‚è·³åŠ¨ï¼ˆByteDanceï¼‰æ‹…ä»»é«˜çº§æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆâ€
ä¸å¥½çš„ç¤ºä¾‹ï¼šâ€œç”¨æˆ·æ„Ÿè°¢äº†åŠ©æ‰‹â€
ä¸å¥½çš„ç¤ºä¾‹ï¼šâ€œç”¨æˆ·å¯¹å›å¤æ„Ÿåˆ°æ»¡æ„â€

Return your analysis in JSON format:
{{
    "knowledge_gaps": [
        {{
            "key": "specific identifier or topic",
            "value": "the correct private knowledge",
            "context": "surrounding context from original",
            "gap_type": "proper_noun|personal_fact|specific_detail|contextual_meaning",
            "confidence": 0.0-1.0
        }}
    ]
}}"""

    def _extract_gaps_from_text(self, response: str) -> list[dict[str, Any]]:
        """Extract knowledge gaps from text response when JSON parsing fails."""
        gaps = []
        lines = response.split("\n")

        current_gap = {}
        for line in lines:
            line = line.strip()
            if line.startswith("- ") or line.startswith("* "):
                # New gap item
                if current_gap:
                    gaps.append(current_gap)
                current_gap = {
                    "key": line[2:].split(":")[0] if ":" in line else line[2:],
                    "value": line[2:].split(":", 1)[1].strip() if ":" in line else "",
                    "context": "",
                    "gap_type": "specific_detail",
                    "confidence": 0.7,
                }

        if current_gap:
            gaps.append(current_gap)

        return gaps

    def _extract_keywords(self, gap: dict[str, Any]) -> list[str]:
        """Extract search keywords from a knowledge gap."""
        keywords = []

        # Extract from key
        key_words = gap.get("key", "").split()
        keywords.extend([word.lower() for word in key_words if len(word) > 2])

        # Extract from value
        value_words = gap.get("value", "").split()[:5]  # First 5 words
        keywords.extend([word.lower() for word in value_words if len(word) > 2])

        # Remove duplicates and common stop words
        stop_words = {"the", "and", "or", "but", "in", "on", "at", "to", "for", "of", "with", "by"}
        keywords = [kw for kw in set(keywords) if kw not in stop_words]

        return keywords[:10]  # Limit to 10 keywords

    def _clean_owner_id(self, owner_id: str) -> str:
        """
        Clean owner_id by removing segment suffix (_segment_X_Y) for consistent indexing.
        æ¸…ç†owner_idï¼Œå»æ‰segmentåç¼€ä»¥ç¡®ä¿ç´¢å¼•ä¸€è‡´æ€§ã€‚
        
        Examples:
        - "å¼ ä¸‰_0_segment_0_5" -> "å¼ ä¸‰_0"
        - "æå››_1_segment_2_3" -> "æå››_1"  
        - "ç‹åšå£«_0" -> "ç‹åšå£«_0" (no change)
        """
        import re
        
        # Remove _segment_X_Y pattern
        cleaned = re.sub(r'_segment_\d+_\d+$', '', owner_id)
        
        return cleaned
