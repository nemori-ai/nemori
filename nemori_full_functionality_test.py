#!/usr/bin/env python3
"""
Nemoriå…¨åŠŸèƒ½æµ‹è¯•è„šæœ¬

è¿™ä¸ªè„šæœ¬åˆ›å»ºä¸€ä¸ªåŒ…å«å¤šç§åœºæ™¯çš„å¯¹è¯æ•°æ®æ ·ä¾‹ï¼Œç”¨äºæµ‹è¯•nemoriçš„å…¨éƒ¨åŠŸèƒ½ï¼š
- Episodic Memory: å¯¹è¯ç‰‡æ®µçš„åˆ†å‰²å’Œè®°å¿†
- Semantic Memory: éšå«çŸ¥è¯†çš„å‘ç°å’ŒæŠ½å–
- Unified Retrieval: ç»Ÿä¸€çš„æ£€ç´¢ç³»ç»Ÿ

æµ‹è¯•åœºæ™¯åŒ…æ‹¬ï¼š
1. æŠ€æœ¯è®¨è®º - è½¯ä»¶å¼€å‘å’ŒæŠ€æœ¯é€‰å‹
2. å­¦æœ¯ç ”ç©¶ - è®ºæ–‡å†™ä½œå’Œç ”ç©¶æ–¹å‘
3. ç”Ÿæ´»æ—¥å¸¸ - æ—¥å¸¸æ´»åŠ¨å’Œä¸ªäººå…´è¶£
4. å·¥ä½œåä½œ - é¡¹ç›®ç®¡ç†å’Œå›¢é˜Ÿåä½œ
5. ä¸“ä¸šçŸ¥è¯† - æœºå™¨å­¦ä¹ å’Œäººå·¥æ™ºèƒ½
"""

import asyncio
import json
from datetime import datetime, timedelta
from pathlib import Path
import pandas as pd
from dotenv import load_dotenv

# Import nemori components
import sys
from pathlib import Path

# æ·»åŠ è·¯å¾„ä»¥å¯¼å…¥nemoriæ¨¡å—
current_dir = Path(__file__).parent
nemori_root = current_dir / "nemori" 
sys.path.insert(0, str(nemori_root))
sys.path.insert(0, str(current_dir / "evaluation/memos/evaluation/scripts/locomo"))
from nemori_eval.experiment import NemoriExperiment, RetrievalStrategy
from nemori.retrieval import RetrievalQuery


def create_comprehensive_test_data():
    """åˆ›å»ºåŒ…å«å¤šç§åœºæ™¯çš„ç»¼åˆæµ‹è¯•æ•°æ®"""
    
    # å®šä¹‰ä¸åŒçš„å¯¹è¯åœºæ™¯
    conversations = [
        {
            "user_id": "tech_discussion_001",
            "conversation": {
                "speaker_a": "å¼ ä¸‰",
                "speaker_b": "æå››",
                "session_1": [
                    {
                        "speaker": "å¼ ä¸‰",
                        "text": "æœ€è¿‘æˆ‘ä»¬é¡¹ç›®éœ€è¦é€‰æ‹©å‰ç«¯æ¡†æ¶ï¼Œä½ è§‰å¾—Reactå’ŒVueå“ªä¸ªæ›´é€‚åˆï¼Ÿ",
                        "timestamp": "2024-01-15T09:00:00Z"
                    },
                    {
                        "speaker": "æå››", 
                        "text": "è¿™è¦çœ‹å…·ä½“éœ€æ±‚ã€‚Reactçš„ç”Ÿæ€ç³»ç»Ÿæ›´æˆç†Ÿï¼Œä½†Vueçš„å­¦ä¹ æ›²çº¿æ›´å¹³ç¼“ã€‚æˆ‘ä»¬çš„å›¢é˜Ÿå¤§éƒ¨åˆ†äººå¯¹JavaScriptåŸºç¡€æ¯”è¾ƒæ‰å®ã€‚",
                        "timestamp": "2024-01-15T09:02:00Z"
                    },
                    {
                        "speaker": "å¼ ä¸‰",
                        "text": "ç¡®å®ï¼Œæˆ‘è€ƒè™‘åˆ°æˆ‘ä»¬éœ€è¦å¼€å‘ä¸€ä¸ªæ•°æ®å¯è§†åŒ–çš„Dashboardï¼Œæ€§èƒ½è¦æ±‚æ¯”è¾ƒé«˜ã€‚",
                        "timestamp": "2024-01-15T09:04:00Z"
                    },
                    {
                        "speaker": "æå››",
                        "text": "é‚£å»ºè®®ç”¨Reacté…åˆD3.jsï¼ŒReactçš„è™šæ‹ŸDOMåœ¨å¤„ç†å¤§é‡æ•°æ®æ›´æ–°æ—¶è¡¨ç°å¾ˆå¥½ã€‚æˆ‘ä¹‹å‰ç”¨è¿™å¥—æ–¹æ¡ˆåšè¿‡ç±»ä¼¼é¡¹ç›®ã€‚",
                        "timestamp": "2024-01-15T09:06:00Z"
                    }
                ],
                "session_1_date_time": "9:00 AM on 15 January, 2024",
                "session_2": [
                    {
                        "speaker": "å¼ ä¸‰",
                        "text": "å¥½çš„ï¼Œé‚£çŠ¶æ€ç®¡ç†ä½ æ¨èç”¨ä»€ä¹ˆï¼ŸReduxè¿˜æ˜¯Context APIï¼Ÿ",
                        "timestamp": "2024-01-15T09:30:00Z"
                    },
                    {
                        "speaker": "æå››",
                        "text": "å¯¹äºä¸­ç­‰è§„æ¨¡çš„é¡¹ç›®ï¼Œæˆ‘å»ºè®®ç”¨Zustandï¼Œæ¯”Reduxç®€å•å¾ˆå¤šï¼Œè€Œä¸”TypeScriptæ”¯æŒå¾ˆå¥½ã€‚",
                        "timestamp": "2024-01-15T09:32:00Z"
                    },
                    {
                        "speaker": "å¼ ä¸‰", 
                        "text": "å¬èµ·æ¥ä¸é”™ï¼Œæˆ‘å»ç ”ç©¶ä¸€ä¸‹Zustandçš„æ–‡æ¡£ã€‚å¦å¤–ï¼Œæˆ‘ä»¬è¿˜éœ€è¦è€ƒè™‘å›½é™…åŒ–çš„éœ€æ±‚ã€‚",
                        "timestamp": "2024-01-15T09:34:00Z"
                    }
                ],
                "session_2_date_time": "9:30 AM on 15 January, 2024"
            }
        },
        
        {
            "user_id": "academic_research_002", 
            "conversation": {
                "speaker_a": "ç‹æ•™æˆ",
                "speaker_b": "åˆ˜åšå£«",
                "session_1": [
                    {
                        "speaker": "ç‹æ•™æˆ",
                        "text": "æˆ‘ä»¬çš„è®ºæ–‡å…³äºå¤§è¯­è¨€æ¨¡å‹çš„å¯¹é½ç ”ç©¶è¿›å±•å¦‚ä½•ï¼Ÿ",
                        "timestamp": "2024-01-16T14:00:00Z"
                    },
                    {
                        "speaker": "åˆ˜åšå£«",
                        "text": "ç›®å‰æˆ‘ä»¬å·²ç»å®Œæˆäº†RLHFçš„åŸºç¡€å®éªŒï¼Œå‘ç°åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šæå‡äº†15%çš„å‡†ç¡®ç‡ã€‚",
                        "timestamp": "2024-01-16T14:02:00Z"
                    },
                    {
                        "speaker": "ç‹æ•™æˆ",
                        "text": "å¾ˆå¥½ï¼é‚£Constitutional AIçš„å¯¹æ¯”å®éªŒå‘¢ï¼Ÿæˆ‘è®°å¾—ä½ æåˆ°è¿‡è¿™ä¸ªæ–¹å‘ã€‚",
                        "timestamp": "2024-01-16T14:04:00Z"
                    },
                    {
                        "speaker": "åˆ˜åšå£«",
                        "text": "Constitutional AIåœ¨å®‰å…¨æ€§è¯„ä¼°ä¸Šè¡¨ç°æ›´å¥½ï¼Œç‰¹åˆ«æ˜¯åœ¨é¿å…æœ‰å®³è¾“å‡ºæ–¹é¢ã€‚æˆ‘ä»¬å¯ä»¥ç»“åˆä¸¤ç§æ–¹æ³•ã€‚",
                        "timestamp": "2024-01-16T14:06:00Z"
                    }
                ],
                "session_1_date_time": "2:00 PM on 16 January, 2024",
                "session_2": [
                    {
                        "speaker": "ç‹æ•™æˆ",
                        "text": "è®ºæ–‡æŠ•ç¨¿çš„è¯ï¼Œä½ è§‰å¾—ICMLå’ŒNeurIPSå“ªä¸ªæ›´åˆé€‚ï¼Ÿ",
                        "timestamp": "2024-01-16T15:00:00Z"
                    },
                    {
                        "speaker": "åˆ˜åšå£«",
                        "text": "è€ƒè™‘åˆ°æˆ‘ä»¬çš„å·¥ä½œåå‘åº”ç”¨ï¼ŒICMLå¯èƒ½æ›´åˆé€‚ã€‚è€Œä¸”ICMLä»Šå¹´å¯¹AIå®‰å…¨çš„è®®é¢˜æ¯”è¾ƒé‡è§†ã€‚",
                        "timestamp": "2024-01-16T15:02:00Z"
                    }
                ],
                "session_2_date_time": "3:00 PM on 16 January, 2024"
            }
        },
        
        {
            "user_id": "daily_life_003",
            "conversation": {
                "speaker_a": "å°æ˜",
                "speaker_b": "å°çº¢", 
                "session_1": [
                    {
                        "speaker": "å°æ˜",
                        "text": "å‘¨æœ«ä½ æœ‰ä»€ä¹ˆè®¡åˆ’å—ï¼Ÿæˆ‘æƒ³å»çˆ¬å±±ã€‚",
                        "timestamp": "2024-01-17T18:00:00Z"
                    },
                    {
                        "speaker": "å°çº¢",
                        "text": "çˆ¬å±±å¬èµ·æ¥ä¸é”™ï¼æˆ‘æœ€è¿‘åœ¨å­¦æ‘„å½±ï¼Œå¯ä»¥å¸¦ç›¸æœºå»æ‹é£æ™¯ã€‚",
                        "timestamp": "2024-01-17T18:02:00Z"
                    },
                    {
                        "speaker": "å°æ˜",
                        "text": "å¤ªå¥½äº†ï¼æˆ‘çŸ¥é“ä¸€ä¸ªå±±ä¸Šæœ‰å¾ˆç¾çš„æ—¥å‡ºï¼Œæ—©ä¸Š6ç‚¹æ—¥å‡ºï¼Œ4ç‚¹åŠå°±è¦å‡ºå‘ã€‚",
                        "timestamp": "2024-01-17T18:04:00Z"
                    },
                    {
                        "speaker": "å°çº¢",
                        "text": "å¥½çš„ï¼Œæˆ‘æœ€è¿‘åœ¨ç»ƒä¹ é•¿ç„¦é•œå¤´çš„ä½¿ç”¨ï¼Œæ­£å¥½å¯ä»¥æ‹æ—¥å‡ºã€‚ä½ ä¼šåšé¥­å—ï¼Ÿæˆ‘ä»¬å¯ä»¥å¸¦ç‚¹ç®€å•çš„æ—©é¤ã€‚",
                        "timestamp": "2024-01-17T18:06:00Z"
                    }
                ],
                "session_1_date_time": "6:00 PM on 17 January, 2024",
                "session_2": [
                    {
                        "speaker": "å°æ˜",
                        "text": "æˆ‘ä¼šåšç®€å•çš„ä¸‰æ˜æ²»ï¼Œä½ è´Ÿè´£å’–å•¡æ€ä¹ˆæ ·ï¼Ÿæˆ‘çŸ¥é“ä½ æ˜¯å’–å•¡çˆ±å¥½è€…ã€‚",
                        "timestamp": "2024-01-17T19:00:00Z"
                    },
                    {
                        "speaker": "å°çº¢",
                        "text": "æ²¡é—®é¢˜ï¼æˆ‘æœ€è¿‘åœ¨ç ”ç©¶æ‰‹å†²å’–å•¡ï¼Œç”¨V60æ»¤æ¯ï¼Œå¯ä»¥å¸¦ä¾¿æºè®¾å¤‡ä¸Šå±±ã€‚",
                        "timestamp": "2024-01-17T19:02:00Z"
                    }
                ],
                "session_2_date_time": "7:00 PM on 17 January, 2024"
            }
        },
        
        {
            "user_id": "work_collaboration_004",
            "conversation": {
                "speaker_a": "é¡¹ç›®ç»ç†",
                "speaker_b": "å¼€å‘å·¥ç¨‹å¸ˆ",
                "session_1": [
                    {
                        "speaker": "é¡¹ç›®ç»ç†",
                        "text": "æˆ‘ä»¬çš„AIæ¨èç³»ç»Ÿé¡¹ç›®è¿›åº¦å¦‚ä½•ï¼Ÿå®¢æˆ·å¸Œæœ›ä¸‹ä¸ªæœˆä¸Šçº¿ã€‚",
                        "timestamp": "2024-01-18T10:00:00Z"
                    },
                    {
                        "speaker": "å¼€å‘å·¥ç¨‹å¸ˆ",
                        "text": "æ ¸å¿ƒç®—æ³•å·²ç»å®Œæˆï¼Œæˆ‘ä»¬ä½¿ç”¨äº†collaborative filteringé…åˆdeep learning embeddingã€‚ç›®å‰å‡†ç¡®ç‡è¾¾åˆ°äº†85%ã€‚",
                        "timestamp": "2024-01-18T10:02:00Z"
                    },
                    {
                        "speaker": "é¡¹ç›®ç»ç†",
                        "text": "å¾ˆå¥½ï¼é‚£A/Bæµ‹è¯•çš„ç»“æœå‘¢ï¼Ÿç”¨æˆ·ä½“éªŒæœ‰æå‡å—ï¼Ÿ",
                        "timestamp": "2024-01-18T10:04:00Z"
                    },
                    {
                        "speaker": "å¼€å‘å·¥ç¨‹å¸ˆ",
                        "text": "A/Bæµ‹è¯•æ˜¾ç¤ºç‚¹å‡»ç‡æå‡äº†23%ï¼Œç”¨æˆ·åœç•™æ—¶é—´å¢åŠ äº†18%ã€‚ä¸è¿‡å†·å¯åŠ¨é—®é¢˜è¿˜éœ€è¦ä¼˜åŒ–ã€‚",
                        "timestamp": "2024-01-18T10:06:00Z"
                    }
                ],
                "session_1_date_time": "10:00 AM on 18 January, 2024",
                "session_2": [
                    {
                        "speaker": "é¡¹ç›®ç»ç†",
                        "text": "å†·å¯åŠ¨é—®é¢˜ç¡®å®é‡è¦ã€‚æ–°ç”¨æˆ·çš„æ¨èå‡†ç¡®ç‡æ€ä¹ˆæ ·ï¼Ÿ",
                        "timestamp": "2024-01-18T14:00:00Z"
                    },
                    {
                        "speaker": "å¼€å‘å·¥ç¨‹å¸ˆ",
                        "text": "æˆ‘ä»¬é‡‡ç”¨äº†content-based filteringä½œä¸ºè¡¥å……ï¼Œç»“åˆç”¨æˆ·çš„åŸºç¡€ä¿¡æ¯å’Œçƒ­é—¨å†…å®¹ã€‚æ–°ç”¨æˆ·çš„è½¬åŒ–ç‡è¾¾åˆ°äº†12%ã€‚",
                        "timestamp": "2024-01-18T14:02:00Z"
                    }
                ],
                "session_2_date_time": "2:00 PM on 18 January, 2024"
            }
        },
        
        {
            "user_id": "ml_knowledge_005",
            "conversation": {
                "speaker_a": "æ•°æ®ç§‘å­¦å®¶",
                "speaker_b": "MLå·¥ç¨‹å¸ˆ",
                "session_1": [
                    {
                        "speaker": "æ•°æ®ç§‘å­¦å®¶",
                        "text": "æˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤„ç†imbalanced datasetæ—¶è¡¨ç°ä¸å¥½ï¼Œä½ æœ‰ä»€ä¹ˆå»ºè®®ï¼Ÿ",
                        "timestamp": "2024-01-19T09:00:00Z"
                    },
                    {
                        "speaker": "MLå·¥ç¨‹å¸ˆ",
                        "text": "å¯ä»¥å°è¯•SMOTEè¿›è¡Œæ•°æ®å¢å¼ºï¼Œæˆ–è€…è°ƒæ•´class weightsã€‚æˆ‘ä¹‹å‰ç”¨focal lossåœ¨ç±»ä¼¼é—®é¢˜ä¸Šæ•ˆæœå¾ˆå¥½ã€‚",
                        "timestamp": "2024-01-19T09:02:00Z"
                    },
                    {
                        "speaker": "æ•°æ®ç§‘å­¦å®¶",
                        "text": "Focal losså¬èµ·æ¥ä¸é”™ï¼Œæ˜¯åœ¨object detectionä¸­ç”¨çš„é‚£ä¸ªå—ï¼Ÿ",
                        "timestamp": "2024-01-19T09:04:00Z"
                    },
                    {
                        "speaker": "MLå·¥ç¨‹å¸ˆ",
                        "text": "å¯¹çš„ï¼Œå®ƒå¯ä»¥åŠ¨æ€è°ƒæ•´éš¾æ˜“æ ·æœ¬çš„æƒé‡ã€‚å¯¹äºæˆ‘ä»¬çš„äºŒåˆ†ç±»é—®é¢˜ï¼Œå¯ä»¥è®©æ¨¡å‹æ›´å…³æ³¨å›°éš¾çš„minority classã€‚",
                        "timestamp": "2024-01-19T09:06:00Z"
                    }
                ],
                "session_1_date_time": "9:00 AM on 19 January, 2024",
                "session_2": [
                    {
                        "speaker": "æ•°æ®ç§‘å­¦å®¶",
                        "text": "é‚£evaluation metricså‘¢ï¼Ÿaccuracyæ˜æ˜¾ä¸é€‚ç”¨ï¼Œä½ æ¨èç”¨ä»€ä¹ˆï¼Ÿ",
                        "timestamp": "2024-01-19T10:00:00Z"
                    },
                    {
                        "speaker": "MLå·¥ç¨‹å¸ˆ",
                        "text": "å»ºè®®ç”¨precision, recallå’ŒF1-scoreçš„ç»„åˆï¼Œæˆ–è€…ç›´æ¥ç”¨AUC-ROCã€‚æˆ‘ä»¬è¿˜å¯ä»¥çœ‹confusion matrixæ¥åˆ†æå…·ä½“çš„é”™è¯¯ç±»å‹ã€‚",
                        "timestamp": "2024-01-19T10:02:00Z"
                    }
                ],
                "session_2_date_time": "10:00 AM on 19 January, 2024"
            }
        }
    ]
    
    # å°†æ•°æ®è½¬æ¢ä¸ºDataFrameæ ¼å¼ï¼Œå…¼å®¹ç°æœ‰çš„åŠ è½½æ–¹å¼
    return pd.DataFrame(conversations)


async def test_episodic_memory(experiment):
    """æµ‹è¯•episodic memoryåŠŸèƒ½"""
    print("\nğŸ§  æµ‹è¯• Episodic Memory åŠŸèƒ½")
    print("=" * 60)
    
    # æ£€æŸ¥åˆ›å»ºçš„episodes
    if experiment.episodes:
        print(f"âœ… æˆåŠŸåˆ›å»º {len(experiment.episodes)} ä¸ªepisodes")
        
        # æ˜¾ç¤ºä¸€äº›episodeçš„è¯¦ç»†ä¿¡æ¯
        for i, episode in enumerate(experiment.episodes[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
            print(f"\nğŸ“ Episode {i+1}: {episode.episode_id}")
            print(f"   ğŸ‘¤ Owner: {episode.owner_id}")
            print(f"   ğŸ• æ—¶é—´: {episode.temporal_info.timestamp}")
            print(f"   ğŸ“„ å†…å®¹é•¿åº¦: {len(episode.content)} characters")
            print(f"   ğŸ·ï¸ æ ‡ç­¾: {episode.tags}")
            
            # æ˜¾ç¤ºepisodeçš„éƒ¨åˆ†å†…å®¹
            content_preview = episode.content[:200] + "..." if len(episode.content) > 200 else episode.content
            print(f"   ğŸ“– å†…å®¹é¢„è§ˆ: {content_preview}")
    else:
        print("âŒ æ²¡æœ‰åˆ›å»ºä»»ä½•episodes")


async def test_semantic_memory(experiment):
    """æµ‹è¯•semantic memoryåŠŸèƒ½"""
    print("\nğŸ” æµ‹è¯• Semantic Memory åŠŸèƒ½")  
    print("=" * 60)
    
    if not experiment.semantic_repo:
        print("âŒ Semantic repository æœªåˆå§‹åŒ–")
        return
        
    # è·å–æ‰€æœ‰ownersçš„semantic knowledge
    owner_ids = {episode.owner_id for episode in experiment.episodes} if experiment.episodes else set()
    
    total_concepts = 0
    for owner_id in owner_ids:
        try:
            semantic_nodes = await experiment.semantic_repo.get_all_semantic_nodes_for_owner(owner_id)
            if semantic_nodes:
                print(f"\nğŸ‘¤ {owner_id} çš„è¯­ä¹‰çŸ¥è¯†:")
                for i, node in enumerate(semantic_nodes[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
                    print(f"   ğŸ”‘ {node.key}: {node.value}")
                    print(f"   ğŸ¯ ä¿¡å¿ƒåº¦: {node.confidence:.2f}")
                    print(f"   ğŸ“ ä¸Šä¸‹æ–‡: {node.context[:100]}...")
                    print(f"   ğŸ”— å…³è”episodes: {len(node.linked_episode_ids)}")
                    print()
                total_concepts += len(semantic_nodes)
                if len(semantic_nodes) > 3:
                    print(f"   ... è¿˜æœ‰ {len(semantic_nodes) - 3} ä¸ªæ¦‚å¿µ")
            else:
                print(f"\nğŸ‘¤ {owner_id}: æœªå‘ç°è¯­ä¹‰çŸ¥è¯†")
        except Exception as e:
            print(f"\nâŒ è·å– {owner_id} çš„è¯­ä¹‰çŸ¥è¯†æ—¶å‡ºé”™: {e}")
    
    print(f"\nğŸ“Š æ€»å…±å‘ç° {total_concepts} ä¸ªè¯­ä¹‰æ¦‚å¿µ")


async def test_unified_retrieval(experiment):
    """æµ‹è¯•unified retrievalåŠŸèƒ½"""
    print("\nğŸ” æµ‹è¯• Unified Retrieval åŠŸèƒ½")
    print("=" * 60)
    
    if not experiment.retrieval_service:
        print("âŒ Retrieval service æœªåˆå§‹åŒ–")
        return
        
    # å®šä¹‰æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "Reactå’ŒVueçš„é€‰æ‹©",
        "å¤§è¯­è¨€æ¨¡å‹å¯¹é½ç ”ç©¶", 
        "æ‘„å½±å’Œçˆ¬å±±",
        "æ¨èç³»ç»Ÿç®—æ³•",
        "imbalanced datasetå¤„ç†"
    ]
    
    # è·å–ä¸€ä¸ªæµ‹è¯•ç”¨çš„owner_id
    owner_ids = list({episode.owner_id for episode in experiment.episodes}) if experiment.episodes else []
    if not owner_ids:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„owner_idè¿›è¡Œæ£€ç´¢æµ‹è¯•")
        return
        
    test_owner = owner_ids[0]
    print(f"ğŸ¯ ä½¿ç”¨owner_id: {test_owner}")
    
    for query_text in test_queries:
        print(f"\nğŸ” æŸ¥è¯¢: '{query_text}'")
        
        try:
            # åˆ›å»ºæ£€ç´¢æŸ¥è¯¢
            query = RetrievalQuery(
                text=query_text,
                owner_id=test_owner,
                limit=3,
                strategy=experiment.retrievalstrategy
            )
            
            # æ‰§è¡Œæ£€ç´¢
            results = await experiment.retrieval_service.search(query)
            
            if results and results.episodes:
                print(f"   âœ… æ‰¾åˆ° {len(results.episodes)} ä¸ªç›¸å…³episodes:")
                for i, result in enumerate(results.episodes[:2]):  # åªæ˜¾ç¤ºå‰2ä¸ª
                    print(f"      ğŸ“„ {i+1}. Episode ID: {result.episode_id}")
                    print(f"         ğŸ‘¤ Owner: {result.owner_id}")
                    print(f"         ğŸ“Š ç›¸å…³åº¦åˆ†æ•°: {getattr(result, 'score', 'N/A')}")
                    content_preview = result.content[:150] + "..." if len(result.content) > 150 else result.content
                    print(f"         ğŸ“– å†…å®¹: {content_preview}")
                    print()
            else:
                print("   âŒ æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")
                
        except Exception as e:
            print(f"   âŒ æ£€ç´¢å‡ºé”™: {e}")


async def test_search_functionality(experiment):
    """æµ‹è¯•æœç´¢åŠŸèƒ½çš„ç»¼åˆæ€§èƒ½"""
    print("\nğŸ¯ æµ‹è¯•æœç´¢åŠŸèƒ½ç»¼åˆæ€§èƒ½")
    print("=" * 60)
    
    # é’ˆå¯¹ä¸åŒownerè¿›è¡Œæœç´¢æµ‹è¯•
    owner_ids = list({episode.owner_id for episode in experiment.episodes}) if experiment.episodes else []
    
    for owner_id in owner_ids[:3]:  # åªæµ‹è¯•å‰3ä¸ªowner
        print(f"\nğŸ‘¤ æµ‹è¯• Owner: {owner_id}")
        
        # è·å–è¯¥ownerçš„episodeä¿¡æ¯
        try:
            result = await experiment.episode_repo.get_episodes_by_owner(owner_id)
            owner_episodes = result.episodes if hasattr(result, "episodes") else result
            print(f"   ğŸ“Š è¯¥ç”¨æˆ·å…±æœ‰ {len(owner_episodes)} ä¸ªepisodes")
            
            # é’ˆå¯¹è¯¥ç”¨æˆ·çš„å†…å®¹è¿›è¡Œç›¸å…³æœç´¢
            if owner_episodes:
                # ä»ç¬¬ä¸€ä¸ªepisodeä¸­æå–å…³é”®è¯è¿›è¡Œæœç´¢
                first_episode = owner_episodes[0]
                # ç®€å•æå–å‰å‡ ä¸ªè¯ä½œä¸ºæœç´¢å…³é”®è¯
                words = first_episode.content.split()[:3]
                search_query = " ".join(words)
                
                print(f"   ğŸ” ä½¿ç”¨å…³é”®è¯æœç´¢: '{search_query}'")
                
                query = RetrievalQuery(
                    text=search_query,
                    owner_id=owner_id,
                    limit=5,
                    strategy=experiment.retrievalstrategy
                )
                
                search_results = await experiment.retrieval_service.search(query)
                
                if search_results and search_results.episodes:
                    print(f"   âœ… æ‰¾åˆ° {len(search_results.episodes)} ä¸ªç›¸å…³episodes")
                    print(f"   ğŸ“ˆ æœç´¢å¬å›ç‡: {len(search_results.episodes)}/{len(owner_episodes)} = {len(search_results.episodes)/len(owner_episodes)*100:.1f}%")
                else:
                    print("   âŒ æœç´¢æœªè¿”å›ç»“æœ")
                    
        except Exception as e:
            print(f"   âŒ æœç´¢æµ‹è¯•å‡ºé”™: {e}")


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ Nemori å…¨åŠŸèƒ½æµ‹è¯•")
    print("=" * 80)
    print("æµ‹è¯•åœºæ™¯åŒ…æ‹¬:")
    print("  1. æŠ€æœ¯è®¨è®º - å‰ç«¯æ¡†æ¶é€‰æ‹©å’ŒæŠ€æœ¯æ ˆ")
    print("  2. å­¦æœ¯ç ”ç©¶ - AIæ¨¡å‹å¯¹é½å’Œè®ºæ–‡æŠ•ç¨¿") 
    print("  3. æ—¥å¸¸ç”Ÿæ´» - çˆ¬å±±æ‘„å½±å’Œå…´è¶£çˆ±å¥½")
    print("  4. å·¥ä½œåä½œ - AIæ¨èç³»ç»Ÿé¡¹ç›®ç®¡ç†")
    print("  5. ä¸“ä¸šçŸ¥è¯† - æœºå™¨å­¦ä¹ ç®—æ³•å’Œè¯„ä¼°æŒ‡æ ‡")
    print("=" * 80)
    
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    print("\nğŸ“‹ åˆ›å»ºæµ‹è¯•æ•°æ®...")
    test_data = create_comprehensive_test_data()
    print(f"âœ… åˆ›å»ºäº† {len(test_data)} ä¸ªå¯¹è¯åœºæ™¯")
    
    # åˆå§‹åŒ–nemoriå®éªŒ
    print("\nğŸ”§ åˆå§‹åŒ– Nemori å®éªŒ...")
    experiment = NemoriExperiment(
        version="full_test",
        episode_mode="speaker", 
        retrievalstrategy=RetrievalStrategy.EMBEDDING,
        max_concurrency=1  # ä¸²è¡Œå¤„ç†ï¼Œä¾¿äºè§‚å¯Ÿ
    )
    
    try:
        # Step 1: è®¾ç½®LLM provider
        print("\nğŸ¤– è®¾ç½® LLM Provider...")
        api_key = "sk-NuO4dbNTkXXi5zWYkLFnhyug1kkqjeysvrb74pA9jTGfz8cm"
        base_url = "https://jeniya.cn/v1"  
        model = "gpt-4o-mini"
        
        llm_available = await experiment.setup_llm_provider(
            model=model, 
            api_key=api_key, 
            base_url=base_url
        )
        
        if not llm_available:
            print("âš ï¸ LLMè¿æ¥å¤±è´¥ï¼Œå°†å½±å“è¯­ä¹‰å‘ç°åŠŸèƒ½")
            return
            
        # Step 2: åŠ è½½æµ‹è¯•æ•°æ®
        print("\nğŸ“Š åŠ è½½æµ‹è¯•æ•°æ®...")
        experiment.load_locomo_data(test_data)
        
        # Step 3: è®¾ç½®å­˜å‚¨å’Œæ£€ç´¢
        print("\nğŸ—„ï¸ è®¾ç½®å­˜å‚¨å’Œæ£€ç´¢æœåŠ¡...")
        emb_api_key = "EMPTY"
        emb_base_url = "http://localhost:6007/v1"
        emb_model = "qwen3-emb"
        
        await experiment.setup_storage_and_retrieval(
            emb_api_key=emb_api_key,
            emb_base_url=emb_base_url, 
            embed_model=emb_model
        )
        
        # Step 4: æ„å»ºepisodeså’Œè¯­ä¹‰è®°å¿†
        print("\nğŸ—ï¸ æ„å»ºEpisodeså’Œè¯­ä¹‰è®°å¿†...")
        await experiment.build_episodes_semantic()
        
        # åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
        print("\nğŸ“Š å¤„ç†ç»“æœç»Ÿè®¡:")
        print(f"âœ… å¤„ç†äº† {len(experiment.conversations)} ä¸ªå¯¹è¯")
        print(f"âœ… åˆ›å»ºäº† {len(experiment.episodes)} ä¸ªepisodes")
        semantic_count = getattr(experiment, 'actual_semantic_count', 0)
        print(f"âœ… å‘ç°äº† {semantic_count} ä¸ªè¯­ä¹‰æ¦‚å¿µ")
        
        if semantic_count > 0 and len(experiment.episodes) > 0:
            print(f"ğŸ“ˆ å¹³å‡æ¯ä¸ªepisodeå‘ç°è¯­ä¹‰æ¦‚å¿µ: {semantic_count/len(experiment.episodes):.1f}")
        
        # è¿è¡ŒåŠŸèƒ½æµ‹è¯•
        await test_episodic_memory(experiment)
        await test_semantic_memory(experiment) 
        await test_unified_retrieval(experiment)
        await test_search_functionality(experiment)
        
        print("\nğŸ‰ å…¨åŠŸèƒ½æµ‹è¯•å®Œæˆ!")
        print("=" * 80)
        print("æµ‹è¯•æ€»ç»“:")
        print(f"ğŸ“ Episodic Memory: {len(experiment.episodes)} episodes")
        print(f"ğŸ§  Semantic Memory: {semantic_count} concepts")
        print(f"ğŸ” Retrieval Strategy: {experiment.retrievalstrategy.value}")
        print(f"ğŸ’¾ å­˜å‚¨ä½ç½®: {experiment.db_dir}")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        # æ¸…ç†èµ„æº
        print("\nğŸ§¹ æ¸…ç†èµ„æº...")
        if hasattr(experiment, 'cleanup'):
            await experiment.cleanup()


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    asyncio.run(main())