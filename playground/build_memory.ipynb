{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete! 设置完成！\n",
      "Python version: 3.12.7 (main, Oct 16 2024, 07:12:08) [Clang 18.1.8 ]\n",
      "Current working directory: /Users/pandazki/Codes/nemori/playground\n",
      "✓ OpenAI API key found in environment\n",
      "✓ 在环境中找到 OpenAI API 密钥\n"
     ]
    }
   ],
   "source": [
    "# Memory Building and Testing with LoComo Dataset\n",
    "# 基于 LoComo 数据集的记忆构建和测试\n",
    "\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the parent directory to the path so we can import nemori\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "print(\"Setup complete! 设置完成！\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Current working directory: {Path.cwd()}\")\n",
    "\n",
    "# Check for OpenAI API key\n",
    "# 检查 OpenAI API 密钥\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if api_key:\n",
    "    print(\"✓ OpenAI API key found in environment\")\n",
    "    print(\"✓ 在环境中找到 OpenAI API 密钥\")\n",
    "else:\n",
    "    print(\"⚠ Warning: OPENAI_API_KEY not found in environment\")\n",
    "    print(\"⚠ 警告：在环境中未找到 OPENAI_API_KEY\")\n",
    "    print(\"Please set your OpenAI API key: export OPENAI_API_KEY=your_key_here\")\n",
    "    print(\"请设置您的 OpenAI API 密钥: export OPENAI_API_KEY=your_key_here\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "# 导入所需模块\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from nemori.builders.conversation_builder import ConversationEpisodeBuilder\n",
    "from nemori.core.data_types import DataType, RawUserData, TemporalInfo\n",
    "from nemori.core.episode import EpisodeLevel, EpisodeType\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 conversations, sampled 3\n",
      "加载了 10 个对话，采样了 3 个\n",
      "\n",
      "First conversation structure:\n",
      "第一个对话结构:\n",
      "Keys: ['qa', 'conversation', 'event_summary', 'observation', 'session_summary', 'sample_id']\n",
      "Speaker A: Tim\n",
      "Speaker B: John\n",
      "Number of sessions: 29\n",
      "会话数量: 29\n",
      "Total messages: 680\n",
      "消息总数: 680\n",
      "\n",
      "First session (session_1) - 7:48 pm on 21 May, 2023:\n",
      "第一个会话 (session_1) - 7:48 pm on 21 May, 2023:\n",
      "Message 1: John - Hey Tim, nice to meet you! What's up? Anything new happening?...\n",
      "Dialog ID: D1:1\n",
      "\n",
      "Message 2: Tim - Hey John! Great to meet you. Been discussing collaborations for a Harry Potter fan project I am work...\n",
      "Dialog ID: D1:2\n",
      "\n",
      "Message 3: John - That's great! I just signed with a new team - excited for the season!...\n",
      "Dialog ID: D1:3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load and Sample LoComo Dataset\n",
    "# 加载和采样 LoComo 数据集\n",
    "\n",
    "def load_locomo_data(file_path: str, sample_size: int = 5):\n",
    "    \"\"\"Load and sample conversation data from LoComo dataset.\n",
    "    从 LoComo 数据集加载和采样对话数据。\"\"\"\n",
    "\n",
    "    with open(file_path, encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Sample random conversations (data is now a list, not a dict)\n",
    "    # 随机采样对话（数据现在是列表，不是字典）\n",
    "    sampled_indices = random.sample(range(len(data)), min(sample_size, len(data)))\n",
    "    sampled_conversations = [data[i] for i in sampled_indices]\n",
    "\n",
    "    print(f\"Loaded {len(data)} conversations, sampled {len(sampled_conversations)}\")\n",
    "    print(f\"加载了 {len(data)} 个对话，采样了 {len(sampled_conversations)} 个\")\n",
    "\n",
    "    return sampled_conversations\n",
    "\n",
    "# Load sample data\n",
    "# 加载示例数据\n",
    "locomo_data = load_locomo_data(\"dataset/locomo10.json\", sample_size=3)\n",
    "\n",
    "# Display first conversation structure\n",
    "# 显示第一个对话的结构\n",
    "first_conversation = locomo_data[0]\n",
    "\n",
    "print(\"\\nFirst conversation structure:\")\n",
    "print(\"第一个对话结构:\")\n",
    "print(f\"Keys: {list(first_conversation.keys())}\")\n",
    "print(f\"Speaker A: {first_conversation['conversation']['speaker_a']}\")\n",
    "print(f\"Speaker B: {first_conversation['conversation']['speaker_b']}\")\n",
    "\n",
    "# Count total messages across all sessions\n",
    "# 统计所有会话中的消息总数\n",
    "conv = first_conversation['conversation']\n",
    "session_keys = [key for key in conv.keys() if key.startswith('session_') and not key.endswith('_date_time')]\n",
    "total_messages = sum(len(conv[session_key]) for session_key in session_keys)\n",
    "\n",
    "print(f\"Number of sessions: {len(session_keys)}\")\n",
    "print(f\"会话数量: {len(session_keys)}\")\n",
    "print(f\"Total messages: {total_messages}\")\n",
    "print(f\"消息总数: {total_messages}\")\n",
    "\n",
    "# Show first few messages from first session\n",
    "# 显示第一个会话的前几条消息\n",
    "if session_keys:\n",
    "    first_session = session_keys[0]\n",
    "    first_session_messages = conv[first_session]\n",
    "    first_session_time = conv[f\"{first_session}_date_time\"]\n",
    "\n",
    "    print(f\"\\nFirst session ({first_session}) - {first_session_time}:\")\n",
    "    print(f\"第一个会话 ({first_session}) - {first_session_time}:\")\n",
    "\n",
    "    for i, msg in enumerate(first_session_messages[:3]):\n",
    "        print(f\"Message {i+1}: {msg['speaker']} - {msg['text'][:100]}...\")\n",
    "        print(f\"Dialog ID: {msg['dia_id']}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion test successful! 转换测试成功！\n",
      "User ID: tim\n",
      "Data Type: DataType.CONVERSATION\n",
      "Message count: 680\n",
      "Session count: 29\n",
      "Source: locomo_dataset\n",
      "Duration: 8378310.0 seconds\n",
      "Has images: True\n",
      "\n",
      "First 3 converted messages:\n",
      "前3条转换后的消息:\n",
      "1. John (session_1): Hey Tim, nice to meet you! What's up? Anything new happening?...\n",
      "   Timestamp: 2023-05-21T19:48:00\n",
      "   Dialog ID: D1:1\n",
      "\n",
      "2. Tim (session_1): Hey John! Great to meet you. Been discussing collaborations for a Harry Potter f...\n",
      "   Timestamp: 2023-05-21T19:48:30\n",
      "   Dialog ID: D1:2\n",
      "\n",
      "3. John (session_1): That's great! I just signed with a new team - excited for the season!...\n",
      "   Timestamp: 2023-05-21T19:49:00\n",
      "   Dialog ID: D1:3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert LoComo Data to Nemori Format\n",
    "# 将 LoComo 数据转换为 Nemori 格式\n",
    "\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def parse_locomo_timestamp(timestamp_str: str) -> datetime:\n",
    "    \"\"\"Parse LoComo timestamp format to datetime object.\n",
    "    将 LoComo 时间戳格式解析为 datetime 对象。\"\"\"\n",
    "    # Example: \"1:56 pm on 8 May, 2023\"\n",
    "    try:\n",
    "        # Remove extra spaces and normalize\n",
    "        timestamp_str = re.sub(r'\\s+', ' ', timestamp_str.strip())\n",
    "\n",
    "        # Parse the timestamp\n",
    "        dt = datetime.strptime(timestamp_str, \"%I:%M %p on %d %B, %Y\")\n",
    "        return dt\n",
    "    except ValueError as e:\n",
    "        print(f\"Warning: Could not parse timestamp '{timestamp_str}': {e}\")\n",
    "        print(f\"警告：无法解析时间戳 '{timestamp_str}': {e}\")\n",
    "        return datetime.now()\n",
    "\n",
    "\n",
    "def convert_locomo_to_nemori(conversation_data: dict, conversation_id: str) -> RawUserData:\n",
    "    \"\"\"Convert LoComo conversation format to Nemori RawUserData format.\n",
    "    将 LoComo 对话格式转换为 Nemori RawUserData 格式。\"\"\"\n",
    "\n",
    "    messages = []\n",
    "    conv = conversation_data['conversation']\n",
    "\n",
    "    # Get all session keys in order\n",
    "    # 按顺序获取所有会话键\n",
    "    session_keys = sorted([key for key in conv.keys() if key.startswith('session_') and not key.endswith('_date_time')])\n",
    "\n",
    "    message_counter = 0\n",
    "\n",
    "    for session_key in session_keys:\n",
    "        session_messages = conv[session_key]\n",
    "        session_time_key = f\"{session_key}_date_time\"\n",
    "\n",
    "        if session_time_key in conv:\n",
    "            # Parse session timestamp\n",
    "            # 解析会话时间戳\n",
    "            session_time = parse_locomo_timestamp(conv[session_time_key])\n",
    "\n",
    "            # Process each message in this session\n",
    "            # 处理此会话中的每条消息\n",
    "            for i, msg in enumerate(session_messages):\n",
    "                # Generate timestamp for this message (session time + message offset)\n",
    "                # 为此消息生成时间戳（会话时间 + 消息偏移）\n",
    "                msg_timestamp = session_time + timedelta(seconds=i * 30)  # 30 seconds between messages\n",
    "                iso_timestamp = msg_timestamp.isoformat()\n",
    "\n",
    "                message = {\n",
    "                    'user_id': msg['speaker'].lower().replace(' ', '_'),\n",
    "                    'user_name': msg['speaker'],\n",
    "                    'content': msg['text'],\n",
    "                    'timestamp': iso_timestamp,\n",
    "                    'original_timestamp': conv[session_time_key],  # Keep original session timestamp\n",
    "                    'dia_id': msg['dia_id'],  # Keep dialog ID for reference\n",
    "                    'session': session_key\n",
    "                }\n",
    "\n",
    "                # Add optional fields if present\n",
    "                # 如果存在可选字段，则添加\n",
    "                if 'img_url' in msg:\n",
    "                    message['img_url'] = msg['img_url']\n",
    "                if 'blip_caption' in msg:\n",
    "                    message['blip_caption'] = msg['blip_caption']\n",
    "                if 'query' in msg:\n",
    "                    message['query'] = msg['query']\n",
    "\n",
    "                messages.append(message)\n",
    "                message_counter += 1\n",
    "\n",
    "    # Calculate total duration based on all messages\n",
    "    # 根据所有消息计算总持续时间\n",
    "    if messages:\n",
    "        first_time = datetime.fromisoformat(messages[0]['timestamp'])\n",
    "        last_time = datetime.fromisoformat(messages[-1]['timestamp'])\n",
    "        duration = (last_time - first_time).total_seconds() + 30  # Add 30s for last message\n",
    "    else:\n",
    "        duration = 0.0\n",
    "        first_time = datetime.now()\n",
    "\n",
    "    temporal_info = TemporalInfo(\n",
    "        timestamp=first_time,\n",
    "        duration=duration,\n",
    "        timezone=\"UTC\"\n",
    "    )\n",
    "\n",
    "    # Extract main user_id (use speaker_a as primary)\n",
    "    # 提取主要用户ID（使用 speaker_a 作为主要用户）\n",
    "    main_user_id = conv.get('speaker_a', 'unknown_user').lower().replace(' ', '_')\n",
    "\n",
    "    return RawUserData(\n",
    "        user_id=main_user_id,\n",
    "        data_type=DataType.CONVERSATION,\n",
    "        content=messages,\n",
    "        source=\"locomo_dataset\",\n",
    "        temporal_info=temporal_info,\n",
    "        metadata={\n",
    "            \"conversation_id\": conversation_id,\n",
    "            \"sample_id\": conversation_data.get('sample_id', 'unknown'),\n",
    "            \"speaker_a\": conv.get('speaker_a'),\n",
    "            \"speaker_b\": conv.get('speaker_b'),\n",
    "            \"participant_count\": 2,  # Always 2 speakers in LoComo\n",
    "            \"session_count\": len(session_keys),\n",
    "            \"message_count\": len(messages),\n",
    "            \"has_images\": any('img_url' in msg for msg in messages),\n",
    "            \"original_format\": \"locomo_multi_session\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Test conversion with first conversation\n",
    "# 使用第一个对话测试转换\n",
    "test_raw_data = convert_locomo_to_nemori(first_conversation, \"0\")\n",
    "\n",
    "print(\"Conversion test successful! 转换测试成功！\")\n",
    "print(f\"User ID: {test_raw_data.user_id}\")\n",
    "print(f\"Data Type: {test_raw_data.data_type}\")\n",
    "print(f\"Message count: {len(test_raw_data.content)}\")\n",
    "print(f\"Session count: {test_raw_data.metadata['session_count']}\")\n",
    "print(f\"Source: {test_raw_data.source}\")\n",
    "print(f\"Duration: {test_raw_data.temporal_info.duration:.1f} seconds\")\n",
    "print(f\"Has images: {test_raw_data.metadata['has_images']}\")\n",
    "\n",
    "# Show first few converted messages\n",
    "# 显示前几条转换后的消息\n",
    "print(\"\\nFirst 3 converted messages:\")\n",
    "print(\"前3条转换后的消息:\")\n",
    "for i, msg in enumerate(test_raw_data.content[:3]):\n",
    "    print(f\"{i+1}. {msg['user_name']} ({msg['session']}): {msg['content'][:80]}...\")\n",
    "    print(f\"   Timestamp: {msg['timestamp']}\")\n",
    "    print(f\"   Dialog ID: {msg['dia_id']}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up OpenAI provider...\n",
      "设置 OpenAI 提供程序...\n",
      "max_tokens: 4096\n",
      "Testing OpenAI connection...\n",
      "测试 OpenAI 连接...\n",
      "✓ OpenAI connection successful!\n",
      "✓ OpenAI 连接成功！\n",
      "Model: gpt-4o-mini\n",
      "模型: gpt-4o-mini\n",
      "Temperature: 0.3\n",
      "温度: 0.3\n",
      "Max tokens: 4096\n",
      "最大令牌数: 4096\n"
     ]
    }
   ],
   "source": [
    "# OpenAI LLM Provider for Testing\n",
    "# 用于测试的 OpenAI LLM 提供程序\n",
    "\n",
    "from nemori.llm.providers.openai_provider import OpenAIProvider\n",
    "\n",
    "# Create OpenAI provider for testing\n",
    "# 创建用于测试的 OpenAI 提供程序\n",
    "print(\"Setting up OpenAI provider...\")\n",
    "print(\"设置 OpenAI 提供程序...\")\n",
    "\n",
    "try:\n",
    "    # Use gpt-4o-mini for cost-effective testing\n",
    "    # 使用 gpt-4o-mini 进行经济高效的测试\n",
    "    openai_llm = OpenAIProvider(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0.3,\n",
    "        max_tokens=4096  # Reasonable limit for episode generation\n",
    "    )\n",
    "\n",
    "    print(\"Testing OpenAI connection...\")\n",
    "    print(\"测试 OpenAI 连接...\")\n",
    "\n",
    "    # Test the connection\n",
    "    # 测试连接\n",
    "    if openai_llm.test_connection():\n",
    "        print(\"✓ OpenAI connection successful!\")\n",
    "        print(\"✓ OpenAI 连接成功！\")\n",
    "        print(f\"Model: {openai_llm.model}\")\n",
    "        print(f\"模型: {openai_llm.model}\")\n",
    "        print(f\"Temperature: {openai_llm.temperature}\")\n",
    "        print(f\"温度: {openai_llm.temperature}\")\n",
    "        print(f\"Max tokens: {openai_llm.max_tokens}\")\n",
    "        print(f\"最大令牌数: {openai_llm.max_tokens}\")\n",
    "    else:\n",
    "        print(\"✗ OpenAI connection failed!\")\n",
    "        print(\"✗ OpenAI 连接失败！\")\n",
    "        openai_llm = None\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error creating OpenAI provider: {e}\")\n",
    "    print(f\"✗ 创建 OpenAI 提供程序时出错: {e}\")\n",
    "    print(\"Please check your API key and internet connection\")\n",
    "    print(\"请检查您的 API 密钥和网络连接\")\n",
    "    openai_llm = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing ConversationEpisodeBuilder without LLM (Fix Verification) ===\n",
      "=== 测试不使用 LLM 的 ConversationEpisodeBuilder（修复验证）===\n",
      "\n",
      "Re-converting test data with fixed timestamps...\n",
      "使用修复的时间戳重新转换测试数据...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'first_key' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRe-converting test data with fixed timestamps...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m使用修复的时间戳重新转换测试数据...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m test_raw_data_fixed = convert_locomo_to_nemori(first_conversation, \u001b[43mfirst_key\u001b[49m)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Show sample of fixed timestamps\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# 显示修复的时间戳样本\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFixed timestamp samples:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'first_key' is not defined"
     ]
    }
   ],
   "source": [
    "# Test ConversationEpisodeBuilder without LLM (Quick Fix Verification)\n",
    "# 测试不使用 LLM 的 ConversationEpisodeBuilder（快速修复验证）\n",
    "\n",
    "print(\"=== Testing ConversationEpisodeBuilder without LLM (Fix Verification) ===\")\n",
    "print(\"=== 测试不使用 LLM 的 ConversationEpisodeBuilder（修复验证）===\\n\")\n",
    "\n",
    "# Create builder without LLM (fallback mode)\n",
    "# 创建不使用 LLM 的构建器（回退模式）\n",
    "builder = ConversationEpisodeBuilder()\n",
    "\n",
    "# Re-convert test data with fixed timestamps\n",
    "# 使用修复的时间戳重新转换测试数据\n",
    "print(\"Re-converting test data with fixed timestamps...\")\n",
    "print(\"使用修复的时间戳重新转换测试数据...\")\n",
    "\n",
    "test_raw_data_fixed = convert_locomo_to_nemori(first_conversation, \"0\")\n",
    "\n",
    "# Show sample of fixed timestamps\n",
    "# 显示修复的时间戳样本\n",
    "print(\"\\nFixed timestamp samples:\")\n",
    "print(\"修复的时间戳样本:\")\n",
    "for i in range(min(3, len(test_raw_data_fixed.content))):\n",
    "    msg = test_raw_data_fixed.content[i]\n",
    "    print(f\"  Message {i+1}:\")\n",
    "    print(f\"    Original: {msg['original_timestamp']}\")\n",
    "    print(f\"    Fixed ISO: {msg['timestamp']}\")\n",
    "    print()\n",
    "\n",
    "# Test building episode with fixed data\n",
    "# 使用修复的数据测试构建情节\n",
    "print(\"Testing episode building with fixed timestamps...\")\n",
    "print(\"使用修复的时间戳测试情节构建...\")\n",
    "\n",
    "try:\n",
    "    episode = builder.build_episode(test_raw_data_fixed)\n",
    "\n",
    "    print(\"✓ Episode built successfully with fixed timestamps!\")\n",
    "    print(\"✓ 使用修复的时间戳成功构建情节！\")\n",
    "    print(f\"  Episode ID: {episode.episode_id}\")\n",
    "    print(f\"  情节 ID: {episode.episode_id}\")\n",
    "    print(f\"  Title: {episode.title}\")\n",
    "    print(f\"  标题: {episode.title}\")\n",
    "    print(f\"  Level: {episode.level}\")\n",
    "    print(f\"  级别: {episode.level}\")\n",
    "    print(f\"  Message count: {len(test_raw_data_fixed.content)}\")\n",
    "    print(f\"  消息数: {len(test_raw_data_fixed.content)}\")\n",
    "\n",
    "    print(\"\\n✓ Timestamp fix successful! Ready for OpenAI testing.\")\n",
    "    print(\"✓ 时间戳修复成功！准备进行 OpenAI 测试。\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error still persists: {e}\")\n",
    "    print(f\"✗ 错误仍然存在: {e}\")\n",
    "    episode = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ConversationEpisodeBuilder with OpenAI LLM\n",
    "# 使用 OpenAI LLM 测试 ConversationEpisodeBuilder\n",
    "\n",
    "print(\"=== Testing ConversationEpisodeBuilder with OpenAI LLM ===\")\n",
    "print(\"=== 使用 OpenAI LLM 测试 ConversationEpisodeBuilder ===\\n\")\n",
    "\n",
    "if openai_llm is not None:\n",
    "    # Create builder with OpenAI LLM\n",
    "    # 使用 OpenAI LLM 创建构建器\n",
    "    builder_with_llm = ConversationEpisodeBuilder(llm_provider=openai_llm)\n",
    "\n",
    "    # Build episode with LLM using fixed timestamp data\n",
    "    # 使用修复时间戳数据的 LLM 构建情节\n",
    "    print(\"Building episode with OpenAI LLM (using fixed timestamps)...\")\n",
    "    print(\"使用 OpenAI LLM 构建情节（使用修复的时间戳）...\")\n",
    "\n",
    "    try:\n",
    "        episode_with_llm = builder_with_llm.build_episode(test_raw_data_fixed)\n",
    "\n",
    "        print(\"✓ Episode built successfully with OpenAI!\")\n",
    "        print(\"✓ 使用 OpenAI 成功构建情节！\")\n",
    "\n",
    "        # Display episode details\n",
    "        # 显示情节详情\n",
    "        print(\"\\n=== Episode Details with OpenAI LLM ===\")\n",
    "        print(\"=== 使用 OpenAI LLM 的情节详情 ===\")\n",
    "        print(f\"Episode ID: {episode_with_llm.episode_id}\")\n",
    "        print(f\"情节 ID: {episode_with_llm.episode_id}\")\n",
    "        print(f\"Title: {episode_with_llm.title}\")\n",
    "        print(f\"标题: {episode_with_llm.title}\")\n",
    "        print(f\"Summary: {episode_with_llm.summary}\")\n",
    "        print(f\"总结: {episode_with_llm.summary}\")\n",
    "        print(f\"Content preview: {episode_with_llm.content[:300]}...\")\n",
    "        print(f\"内容预览: {episode_with_llm.content[:300]}...\")\n",
    "\n",
    "        # Compare episodes with and without LLM\n",
    "        # 比较使用和不使用 LLM 的情节\n",
    "        print(\"\\n=== Comparison: With vs Without LLM ===\")\n",
    "        print(\"=== 比较：使用 vs 不使用 LLM ===\")\n",
    "\n",
    "        print(f\"\\nWithout LLM Title: {episode.title}\")\n",
    "        print(f\"不使用 LLM 标题: {episode.title}\")\n",
    "        print(f\"With OpenAI Title: {episode_with_llm.title}\")\n",
    "        print(f\"使用 OpenAI 标题: {episode_with_llm.title}\")\n",
    "\n",
    "        print(f\"\\nWithout LLM Summary: {episode.summary}\")\n",
    "        print(f\"不使用 LLM 总结: {episode.summary}\")\n",
    "        print(f\"With OpenAI Summary: {episode_with_llm.summary}\")\n",
    "        print(f\"使用 OpenAI 总结: {episode_with_llm.summary}\")\n",
    "\n",
    "        print(\"\\nContent Length Comparison:\")\n",
    "        print(\"内容长度比较:\")\n",
    "        print(f\"Without LLM: {len(episode.content)} characters\")\n",
    "        print(f\"不使用 LLM: {len(episode.content)} 字符\")\n",
    "        print(f\"With OpenAI: {len(episode_with_llm.content)} characters\")\n",
    "        print(f\"使用 OpenAI: {len(episode_with_llm.content)} 字符\")\n",
    "\n",
    "        # Show quality improvements with OpenAI\n",
    "        # 显示 OpenAI 的质量改进\n",
    "        print(\"\\n=== Quality Analysis ===\")\n",
    "        print(\"=== 质量分析 ===\")\n",
    "        print(f\"OpenAI generated more natural title: {'✓' if len(episode_with_llm.title.split()) > 2 else '✗'}\")\n",
    "        print(f\"OpenAI 生成更自然的标题: {'✓' if len(episode_with_llm.title.split()) > 2 else '✗'}\")\n",
    "        print(f\"OpenAI summary is more detailed: {'✓' if len(episode_with_llm.summary) > len(episode.summary) else '✗'}\")\n",
    "        print(f\"OpenAI 摘要更详细: {'✓' if len(episode_with_llm.summary) > len(episode.summary) else '✗'}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error building episode with OpenAI: {e}\")\n",
    "        print(f\"✗ 使用 OpenAI 构建情节时出错: {e}\")\n",
    "        episode_with_llm = None\n",
    "\n",
    "else:\n",
    "    print(\"⚠ Skipping OpenAI LLM test (no valid provider)\")\n",
    "    print(\"⚠ 跳过 OpenAI LLM 测试（没有有效的提供程序）\")\n",
    "    episode_with_llm = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Testing: Multiple Conversations with OpenAI\n",
    "# 使用 OpenAI 进行多个对话的综合测试\n",
    "\n",
    "print(\"=== Comprehensive Testing: Multiple Conversations with OpenAI ===\")\n",
    "print(\"=== 使用 OpenAI 进行多个对话的综合测试 ===\\n\")\n",
    "\n",
    "# Test all sampled conversations\n",
    "# 测试所有采样的对话\n",
    "episodes_created = []\n",
    "\n",
    "# Use OpenAI if available, otherwise fallback to no LLM\n",
    "# 如果可用则使用 OpenAI，否则回退到不使用 LLM\n",
    "if openai_llm is not None:\n",
    "    print(\"Using OpenAI LLM for episode generation\")\n",
    "    print(\"使用 OpenAI LLM 生成情节\")\n",
    "    builder = ConversationEpisodeBuilder(llm_provider=openai_llm)\n",
    "else:\n",
    "    print(\"Using fallback mode (no LLM) for episode generation\")\n",
    "    print(\"使用回退模式（无 LLM）生成情节\")\n",
    "    builder = ConversationEpisodeBuilder()\n",
    "\n",
    "for i, conv_data in enumerate(locomo_data):\n",
    "    conv_id = str(i)  # Use index as conversation ID\n",
    "    print(f\"\\nProcessing conversation {conv_id} ({i+1}/{len(locomo_data)})...\")\n",
    "    print(f\"处理对话 {conv_id} ({i+1}/{len(locomo_data)})...\")\n",
    "\n",
    "    # Convert to Nemori format\n",
    "    # 转换为 Nemori 格式\n",
    "    raw_data = convert_locomo_to_nemori(conv_data, conv_id)\n",
    "\n",
    "    try:\n",
    "        # Build episode\n",
    "        # 构建情节\n",
    "        episode = builder.build_episode(raw_data)\n",
    "        episodes_created.append(episode)\n",
    "\n",
    "        print(\"  ✓ Episode created successfully\")\n",
    "        print(\"  ✓ 情节创建成功\")\n",
    "        print(f\"  Title: {episode.title}\")\n",
    "        print(f\"  标题: {episode.title}\")\n",
    "        print(f\"  Level: {episode.level}\")\n",
    "        print(f\"  级别: {episode.level}\")\n",
    "        print(f\"  Messages: {len(raw_data.content)}\")\n",
    "        print(f\"  消息数: {len(raw_data.content)}\")\n",
    "        print(f\"  Participants: {episode.metadata.custom_fields.get('unique_participants', 'N/A')}\")\n",
    "        print(f\"  参与者: {episode.metadata.custom_fields.get('unique_participants', 'N/A')}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error creating episode: {e}\")\n",
    "        print(f\"  ✗ 创建情节时出错: {e}\")\n",
    "        continue\n",
    "\n",
    "# Analyze episode levels distribution\n",
    "# 分析情节级别分布\n",
    "if episodes_created:\n",
    "    level_counts = {}\n",
    "    for episode in episodes_created:\n",
    "        level = episode.level\n",
    "        level_counts[level] = level_counts.get(level, 0) + 1\n",
    "\n",
    "    print(\"\\n=== Episode Level Distribution ===\")\n",
    "    print(\"=== 情节级别分布 ===\")\n",
    "    for level, count in level_counts.items():\n",
    "        print(f\"{level}: {count} episodes\")\n",
    "        print(f\"{level}: {count} 个情节\")\n",
    "\n",
    "    # Test episode validation\n",
    "    # 测试情节验证\n",
    "    print(\"\\n=== Episode Validation ===\")\n",
    "    print(\"=== 情节验证 ===\")\n",
    "\n",
    "    for i, episode in enumerate(episodes_created):\n",
    "        print(f\"\\nEpisode {i+1} Validation:\")\n",
    "        print(f\"情节 {i+1} 验证:\")\n",
    "\n",
    "        # Check required fields\n",
    "        # 检查必需字段\n",
    "        validations = {\n",
    "            \"Has Episode ID\": bool(episode.episode_id),\n",
    "            \"Has User ID\": bool(episode.user_id),\n",
    "            \"Has Title\": bool(episode.title),\n",
    "            \"Has Content\": bool(episode.content),\n",
    "            \"Has Summary\": bool(episode.summary),\n",
    "            \"Has Valid Type\": episode.episode_type in [EpisodeType.CONVERSATIONAL, EpisodeType.MIXED],\n",
    "            \"Has Valid Level\": episode.level in [EpisodeLevel.ATOMIC, EpisodeLevel.COMPOUND, EpisodeLevel.THEMATIC],\n",
    "            \"Has Metadata\": episode.metadata is not None,\n",
    "            \"Has Search Keywords\": len(episode.search_keywords) > 0,\n",
    "            \"Has Topics or Entities\": (len(episode.metadata.topics) > 0 or len(episode.metadata.entities) > 0) if episode.metadata else False\n",
    "        }\n",
    "\n",
    "        all_valid = all(validations.values())\n",
    "        print(f\"  Overall Valid: {all_valid}\")\n",
    "        print(f\"  整体有效: {all_valid}\")\n",
    "\n",
    "        for check, result in validations.items():\n",
    "            status = \"✓\" if result else \"✗\"\n",
    "            print(f\"  {status} {check}: {result}\")\n",
    "\n",
    "    print(\"\\n=== Testing Complete! ===\")\n",
    "    print(\"=== 测试完成！ ===\")\n",
    "    print(f\"Total episodes created: {len(episodes_created)}\")\n",
    "    print(f\"创建的情节总数: {len(episodes_created)}\")\n",
    "    print(f\"LLM used: {'OpenAI' if openai_llm else 'None (Fallback)'}\")\n",
    "    print(f\"使用的 LLM: {'OpenAI' if openai_llm else '无（回退模式）'}\")\n",
    "    print(\"All episodes validated successfully!\")\n",
    "    print(\"所有情节验证成功！\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠ No episodes were created successfully\")\n",
    "    print(\"⚠ 没有成功创建任何情节\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Episodes as Structured Data\n",
    "# 将情节保存为结构化数据\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def save_episodes_to_json(episodes: list, filename: str = \"generated_episodes.json\"):\n",
    "    \"\"\"Save episodes to JSON file with structured format.\n",
    "    将情节保存到结构化格式的JSON文件。\"\"\"\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    # 如果输出目录不存在则创建\n",
    "    output_dir = Path(\"output\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    output_data = {\n",
    "        \"metadata\": {\n",
    "            \"generation_timestamp\": datetime.now().isoformat(),\n",
    "            \"total_episodes\": len(episodes),\n",
    "            \"llm_provider\": \"OpenAI\" if openai_llm else \"None (Fallback)\",\n",
    "            \"dataset_source\": \"LoComo RAG Dataset\"\n",
    "        },\n",
    "        \"episodes\": []\n",
    "    }\n",
    "\n",
    "    for i, episode in enumerate(episodes):\n",
    "        episode_data = {\n",
    "            \"index\": i + 1,\n",
    "            \"episode_id\": episode.episode_id,\n",
    "            \"user_id\": episode.user_id,\n",
    "            \"title\": episode.title,\n",
    "            \"title_zh\": f\"情节 {i+1}: {episode.title}\",\n",
    "            \"summary\": episode.summary,\n",
    "            \"content_preview\": episode.content[:300] + \"...\" if len(episode.content) > 300 else episode.content,\n",
    "            \"content_length\": len(episode.content),\n",
    "            \"episode_type\": episode.episode_type.value,\n",
    "            \"level\": episode.level.value,\n",
    "            \"level_name\": episode.level.name,\n",
    "            \"timestamp\": episode.temporal_info.timestamp.isoformat(),\n",
    "            \"duration_seconds\": episode.temporal_info.duration,\n",
    "            \"metadata\": {\n",
    "                \"entities\": episode.metadata.entities,\n",
    "                \"topics\": episode.metadata.topics,\n",
    "                \"emotions\": episode.metadata.emotions,\n",
    "                \"key_points\": episode.metadata.key_points,\n",
    "                \"confidence_score\": episode.metadata.confidence_score,\n",
    "                \"custom_fields\": episode.metadata.custom_fields\n",
    "            },\n",
    "            \"search_keywords\": episode.search_keywords,\n",
    "            \"importance_score\": episode.importance_score\n",
    "        }\n",
    "        output_data[\"episodes\"].append(episode_data)\n",
    "\n",
    "    # Save to file in output directory\n",
    "    # 保存文件到输出目录\n",
    "    output_path = output_dir / filename\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"✓ Episodes saved to: {output_path.absolute()}\")\n",
    "    print(f\"✓ 情节已保存到: {output_path.absolute()}\")\n",
    "    print(f\"  Total episodes: {len(episodes)}\")\n",
    "    print(f\"  总情节数: {len(episodes)}\")\n",
    "    print(f\"  File size: {output_path.stat().st_size / 1024:.1f} KB\")\n",
    "    print(f\"  文件大小: {output_path.stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "    return output_path\n",
    "\n",
    "# Save episodes if they exist\n",
    "# 如果存在情节则保存\n",
    "if 'episodes_created' in globals() and episodes_created:\n",
    "    print(\"=== Saving Episodes to Structured Data ===\")\n",
    "    print(\"=== 将情节保存为结构化数据 ===\\n\")\n",
    "\n",
    "    saved_file = save_episodes_to_json(episodes_created)\n",
    "\n",
    "    # Display sample of saved data\n",
    "    # 显示保存数据的样本\n",
    "    print(\"\\n=== Sample of Saved Data ===\")\n",
    "    print(\"=== 保存数据样本 ===\")\n",
    "\n",
    "    with open(saved_file, encoding='utf-8') as f:\n",
    "        sample_data = json.load(f)\n",
    "\n",
    "    print(f\"Generation Time: {sample_data['metadata']['generation_timestamp']}\")\n",
    "    print(f\"生成时间: {sample_data['metadata']['generation_timestamp']}\")\n",
    "    print(f\"LLM Provider: {sample_data['metadata']['llm_provider']}\")\n",
    "    print(f\"LLM 提供商: {sample_data['metadata']['llm_provider']}\")\n",
    "    print(f\"Total Episodes: {sample_data['metadata']['total_episodes']}\")\n",
    "    print(f\"总情节数: {sample_data['metadata']['total_episodes']}\")\n",
    "\n",
    "    # Show first episode structure\n",
    "    if sample_data['episodes']:\n",
    "        first_episode = sample_data['episodes'][0]\n",
    "        print(\"\\nFirst Episode Structure | 第一个情节结构:\")\n",
    "        print(f\"  Title: {first_episode['title'][:60]}...\")\n",
    "        print(f\"  标题: {first_episode['title'][:60]}...\")\n",
    "        print(f\"  Level: {first_episode['level_name']} ({first_episode['level']})\")\n",
    "        print(f\"  级别: {first_episode['level_name']} ({first_episode['level']})\")\n",
    "        print(f\"  Content Length: {first_episode['content_length']} characters\")\n",
    "        print(f\"  内容长度: {first_episode['content_length']} 字符\")\n",
    "        print(f\"  Entities: {len(first_episode['metadata']['entities'])} items\")\n",
    "        print(f\"  实体: {len(first_episode['metadata']['entities'])} 个\")\n",
    "        print(f\"  Topics: {len(first_episode['metadata']['topics'])} items\")\n",
    "        print(f\"  主题: {len(first_episode['metadata']['topics'])} 个\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠ No episodes found to save\")\n",
    "    print(\"⚠ 未找到要保存的情节\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Episodes Data - Bilingual Dashboard\n",
    "# 情节数据可视化 - 双语仪表板\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set up Chinese font support for matplotlib\n",
    "# 设置matplotlib的中文字体支持\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "def create_episode_visualization(episodes: list):\n",
    "    \"\"\"Create comprehensive visualization of episodes data.\n",
    "    创建情节数据的综合可视化。\"\"\"\n",
    "\n",
    "    if not episodes:\n",
    "        print(\"No episodes to visualize | 没有情节可供可视化\")\n",
    "        return\n",
    "\n",
    "    # Create figure with subplots\n",
    "    # 创建包含子图的图形\n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "    fig.suptitle('Nemori Episodes Analysis Dashboard\\nNemori 情节分析仪表板',\n",
    "                 fontsize=16, fontweight='bold', y=0.95)\n",
    "\n",
    "    # 1. Episode Level Distribution (Pie Chart)\n",
    "    # 1. 情节级别分布（饼图）\n",
    "    ax1 = plt.subplot(2, 3, 1)\n",
    "    levels = [ep.level.name for ep in episodes]\n",
    "    level_counts = Counter(levels)\n",
    "\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
    "    wedges, texts, autotexts = ax1.pie(level_counts.values(),\n",
    "                                       labels=[f'{k}\\n{k}级别' for k in level_counts.keys()],\n",
    "                                       autopct='%1.1f%%',\n",
    "                                       colors=colors[:len(level_counts)],\n",
    "                                       startangle=90)\n",
    "    ax1.set_title('Episode Levels Distribution\\n情节级别分布', fontweight='bold', pad=20)\n",
    "\n",
    "    # 2. Content Length Comparison (Bar Chart)\n",
    "    # 2. 内容长度比较（条形图）\n",
    "    ax2 = plt.subplot(2, 3, 2)\n",
    "    episode_indices = range(1, len(episodes) + 1)\n",
    "    content_lengths = [len(ep.content) for ep in episodes]\n",
    "\n",
    "    bars = ax2.bar(episode_indices, content_lengths,\n",
    "                   color=['#FF6B6B', '#4ECDC4', '#45B7D1'][:len(episodes)])\n",
    "    ax2.set_title('Content Length by Episode\\n按情节的内容长度', fontweight='bold')\n",
    "    ax2.set_xlabel('Episode Index\\n情节索引')\n",
    "    ax2.set_ylabel('Characters\\n字符数')\n",
    "\n",
    "    # Add value labels on bars\n",
    "    # 在条形图上添加数值标签\n",
    "    for bar, length in zip(bars, content_lengths, strict=False):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + max(content_lengths)*0.01,\n",
    "                f'{length:,}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "    # 3. Timeline Visualization\n",
    "    # 3. 时间线可视化\n",
    "    ax3 = plt.subplot(2, 3, 3)\n",
    "    timestamps = [ep.temporal_info.timestamp for ep in episodes]\n",
    "    durations = [ep.temporal_info.duration or 3600 for ep in episodes]  # Default 1 hour if None\n",
    "\n",
    "    for i, (ts, dur) in enumerate(zip(timestamps, durations, strict=False)):\n",
    "        y_pos = i\n",
    "        start_time = 0  # Relative timeline\n",
    "        duration_hours = dur / 3600\n",
    "\n",
    "        # Draw timeline bar\n",
    "        # 绘制时间线条\n",
    "        rect = patches.Rectangle((start_time, y_pos - 0.3), duration_hours, 0.6,\n",
    "                               linewidth=1, edgecolor='black',\n",
    "                               facecolor=colors[i % len(colors)], alpha=0.7)\n",
    "        ax3.add_patch(rect)\n",
    "\n",
    "        # Add episode label\n",
    "        # 添加情节标签\n",
    "        ax3.text(duration_hours/2, y_pos, f'Ep {i+1}',\n",
    "                ha='center', va='center', fontweight='bold', fontsize=8)\n",
    "\n",
    "    ax3.set_xlim(0, max(durations)/3600 * 1.1)\n",
    "    ax3.set_ylim(-0.5, len(episodes) - 0.5)\n",
    "    ax3.set_title('Episode Timeline (Hours)\\n情节时间线（小时）', fontweight='bold')\n",
    "    ax3.set_xlabel('Duration (Hours)\\n持续时间（小时）')\n",
    "    ax3.set_ylabel('Episodes\\n情节')\n",
    "    ax3.set_yticks(range(len(episodes)))\n",
    "    ax3.set_yticklabels([f'Episode {i+1}\\n情节{i+1}' for i in range(len(episodes))])\n",
    "\n",
    "    # 4. Metadata Analysis (Topics & Entities)\n",
    "    # 4. 元数据分析（主题和实体）\n",
    "    ax4 = plt.subplot(2, 3, 4)\n",
    "\n",
    "    all_topics = []\n",
    "    all_entities = []\n",
    "\n",
    "    for ep in episodes:\n",
    "        all_topics.extend(ep.metadata.topics)\n",
    "        all_entities.extend(ep.metadata.entities)\n",
    "\n",
    "    # Count topics and entities\n",
    "    # 统计主题和实体\n",
    "    topic_counts = Counter(all_topics)\n",
    "    entity_counts = Counter(all_entities)\n",
    "\n",
    "    # Show top items\n",
    "    # 显示顶部项目\n",
    "    top_topics = dict(topic_counts.most_common(5))\n",
    "    top_entities = dict(entity_counts.most_common(5))\n",
    "\n",
    "    # Combine for visualization\n",
    "    # 合并用于可视化\n",
    "    combined_data = {}\n",
    "    for topic, count in top_topics.items():\n",
    "        combined_data[f'Topic: {topic}'] = count\n",
    "    for entity, count in top_entities.items():\n",
    "        combined_data[f'Entity: {entity}'] = count\n",
    "\n",
    "    if combined_data:\n",
    "        y_pos = np.arange(len(combined_data))\n",
    "        bars = ax4.barh(y_pos, list(combined_data.values()),\n",
    "                       color=['#FF6B6B' if 'Topic' in k else '#4ECDC4' for k in combined_data.keys()])\n",
    "        ax4.set_yticks(y_pos)\n",
    "        ax4.set_yticklabels([k.replace('Topic: ', 'T: ').replace('Entity: ', 'E: ')\n",
    "                            for k in combined_data.keys()], fontsize=8)\n",
    "        ax4.set_title('Top Topics & Entities\\n热门主题和实体', fontweight='bold')\n",
    "        ax4.set_xlabel('Frequency\\n频率')\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, 'No topics/entities\\n暂无主题/实体',\n",
    "                ha='center', va='center', transform=ax4.transAxes, fontsize=12)\n",
    "        ax4.set_title('Top Topics & Entities\\n热门主题和实体', fontweight='bold')\n",
    "\n",
    "    # 5. Quality Metrics\n",
    "    # 5. 质量指标\n",
    "    ax5 = plt.subplot(2, 3, 5)\n",
    "\n",
    "    confidence_scores = [ep.metadata.confidence_score for ep in episodes]\n",
    "    importance_scores = [ep.importance_score for ep in episodes]\n",
    "\n",
    "    x = np.arange(len(episodes))\n",
    "    width = 0.35\n",
    "\n",
    "    bars1 = ax5.bar(x - width/2, confidence_scores, width,\n",
    "                   label='Confidence\\n置信度', color='#45B7D1', alpha=0.7)\n",
    "    bars2 = ax5.bar(x + width/2, importance_scores, width,\n",
    "                   label='Importance\\n重要性', color='#96CEB4', alpha=0.7)\n",
    "\n",
    "    ax5.set_title('Quality Metrics by Episode\\n按情节的质量指标', fontweight='bold')\n",
    "    ax5.set_xlabel('Episode Index\\n情节索引')\n",
    "    ax5.set_ylabel('Score\\n分数')\n",
    "    ax5.set_xticks(x)\n",
    "    ax5.set_xticklabels([f'Ep {i+1}' for i in range(len(episodes))])\n",
    "    ax5.legend()\n",
    "    ax5.set_ylim(0, 1.1)\n",
    "\n",
    "            # 6. Episode Summary Cards Display\n",
    "    # 6. 情节摘要卡片显示\n",
    "    ax6 = plt.subplot(2, 3, 6)\n",
    "    ax6.axis('off')\n",
    "    ax6.set_xlim(0, 1)\n",
    "    ax6.set_ylim(0, 1)\n",
    "\n",
    "    # Create individual summary cards for better layout\n",
    "    # 创建单独的摘要卡片以改善布局\n",
    "    title_text = \"Episode Summaries\\\\n情节摘要\"\n",
    "    ax6.text(0.5, 0.95, title_text, transform=ax6.transAxes,\n",
    "            fontsize=11, fontweight='bold', ha='center', va='top',\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"#2E86AB\", alpha=0.8, edgecolor='none'),\n",
    "            color='white')\n",
    "\n",
    "    # Calculate positions for episode cards\n",
    "    # 计算情节卡片的位置\n",
    "    card_height = 0.25\n",
    "    start_y = 0.80\n",
    "\n",
    "    for i, ep in enumerate(episodes):\n",
    "        if i >= 3:  # Limit to 3 episodes for better display\n",
    "            break\n",
    "\n",
    "        y_pos = start_y - (i * card_height)\n",
    "\n",
    "        # Create episode title (shorter)\n",
    "        # 创建情节标题（较短）\n",
    "        title = ep.title[:28] + \"...\" if len(ep.title) > 28 else ep.title\n",
    "\n",
    "        # Create summary (2 lines max)\n",
    "        # 创建摘要（最多2行）\n",
    "        summary = ep.summary[:80]  # Shorter summary\n",
    "        words = summary.split(' ')\n",
    "        line1, line2 = \"\", \"\"\n",
    "        current_length = 0\n",
    "\n",
    "        for word in words:\n",
    "            if current_length + len(word) + 1 <= 30 and not line2:\n",
    "                if line1:\n",
    "                    line1 += \" \" + word\n",
    "                else:\n",
    "                    line1 = word\n",
    "                current_length = len(line1)\n",
    "            elif len(line2) + len(word) + 1 <= 30:\n",
    "                if line2:\n",
    "                    line2 += \" \" + word\n",
    "                else:\n",
    "                    line2 = word\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if len(words) > len((line1 + \" \" + line2).split()):\n",
    "            line2 += \"...\"\n",
    "\n",
    "        # Card background colors\n",
    "        # 卡片背景颜色\n",
    "        card_colors = ['#FF9999', '#99CCFF', '#99FF99']\n",
    "\n",
    "        # Draw card background\n",
    "        # 绘制卡片背景\n",
    "        card_rect = patches.Rectangle((0.05, y_pos - 0.18), 0.9, 0.15,\n",
    "                                    linewidth=1, edgecolor='gray',\n",
    "                                    facecolor=card_colors[i % len(card_colors)],\n",
    "                                    alpha=0.3, transform=ax6.transAxes)\n",
    "        ax6.add_patch(card_rect)\n",
    "\n",
    "        # Episode number and title\n",
    "        # 情节编号和标题\n",
    "        ax6.text(0.08, y_pos - 0.03, f\"Episode {i+1}\", transform=ax6.transAxes,\n",
    "                fontsize=9, fontweight='bold', color='#2E86AB')\n",
    "\n",
    "        ax6.text(0.08, y_pos - 0.07, title, transform=ax6.transAxes,\n",
    "                fontsize=8, fontweight='bold', color='black', wrap=True)\n",
    "\n",
    "        # Summary lines\n",
    "        # 摘要行\n",
    "        ax6.text(0.08, y_pos - 0.11, line1, transform=ax6.transAxes,\n",
    "                fontsize=7, color='#333333')\n",
    "\n",
    "        if line2:\n",
    "            ax6.text(0.08, y_pos - 0.14, line2, transform=ax6.transAxes,\n",
    "                    fontsize=7, color='#333333')\n",
    "\n",
    "        # Episode metadata on the right\n",
    "        # 右侧的情节元数据\n",
    "        level_text = f\"Level: {ep.level.name}\"\n",
    "        duration_hours = ep.temporal_info.duration / 3600 if ep.temporal_info.duration else 1\n",
    "        duration_text = f\"Duration: {duration_hours:.1f}h\"\n",
    "\n",
    "        ax6.text(0.75, y_pos - 0.08, level_text, transform=ax6.transAxes,\n",
    "                fontsize=6, color='#666666', ha='left')\n",
    "        ax6.text(0.75, y_pos - 0.12, duration_text, transform=ax6.transAxes,\n",
    "                fontsize=6, color='#666666', ha='left')\n",
    "\n",
    "    # Add a note if there are more episodes\n",
    "    # 如果有更多情节则添加提示\n",
    "    if len(episodes) > 3:\n",
    "        ax6.text(0.5, 0.05, f\"... and {len(episodes) - 3} more episodes\\\\n还有{len(episodes) - 3}个情节\",\n",
    "                transform=ax6.transAxes, fontsize=8, ha='center', va='bottom',\n",
    "                style='italic', color='#666666')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Create visualization if episodes exist\n",
    "# 如果存在情节则创建可视化\n",
    "if 'episodes_created' in globals() and episodes_created:\n",
    "    print(\"\\\\n=== Creating Episode Visualization ===\")\n",
    "    print(\"=== 创建情节可视化 ===\\\\n\")\n",
    "\n",
    "    try:\n",
    "        fig = create_episode_visualization(episodes_created)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"✓ Visualization created successfully!\")\n",
    "        print(\"✓ 可视化创建成功！\")\n",
    "\n",
    "        # Save the visualization to output directory\n",
    "        # 保存可视化到输出目录\n",
    "        output_dir = Path(\"output\")\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        viz_path = output_dir / \"episode_visualization.png\"\n",
    "        fig.savefig(viz_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"✓ Visualization saved to: {viz_path.absolute()}\")\n",
    "        print(f\"✓ 可视化已保存到: {viz_path.absolute()}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error creating visualization: {e}\")\n",
    "        print(f\"✗ 创建可视化时出错: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠ No episodes available for visualization\")\n",
    "    print(\"⚠ 没有可用于可视化的情节\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed Episode Analysis Report - Bilingual\n",
    "# 详细情节分析报告 - 双语\n",
    "\n",
    "def create_detailed_episode_report(episodes: list):\n",
    "    \"\"\"Create a detailed bilingual report of all episodes.\n",
    "    创建所有情节的详细双语报告。\"\"\"\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"🧠 NEMORI EPISODE ANALYSIS REPORT | NEMORI 情节分析报告 🧠\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Overall Statistics\n",
    "    # 整体统计\n",
    "    total_episodes = len(episodes)\n",
    "    total_content_length = sum(len(ep.content) for ep in episodes)\n",
    "    avg_content_length = total_content_length / total_episodes if total_episodes > 0 else 0\n",
    "\n",
    "    level_distribution = Counter([ep.level.name for ep in episodes])\n",
    "    type_distribution = Counter([ep.episode_type.name for ep in episodes])\n",
    "\n",
    "    print(\"\\n📊 OVERVIEW | 概览\")\n",
    "    print(f\"{'─' * 40}\")\n",
    "    print(f\"Total Episodes | 总情节数: {total_episodes}\")\n",
    "    print(f\"Total Content Length | 总内容长度: {total_content_length:,} characters | 字符\")\n",
    "    print(f\"Average Content Length | 平均内容长度: {avg_content_length:,.0f} characters | 字符\")\n",
    "    print(f\"Generation Time | 生成时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "    print(\"\\n🏷️ EPISODE LEVELS | 情节级别\")\n",
    "    print(f\"{'─' * 40}\")\n",
    "    for level, count in level_distribution.items():\n",
    "        percentage = (count / total_episodes) * 100\n",
    "        print(f\"{level:12} | {level}级别: {count:2d} episodes ({percentage:5.1f}%)\")\n",
    "\n",
    "    print(\"\\n📝 EPISODE TYPES | 情节类型\")\n",
    "    print(f\"{'─' * 40}\")\n",
    "    for ep_type, count in type_distribution.items():\n",
    "        percentage = (count / total_episodes) * 100\n",
    "        print(f\"{ep_type:15} | {ep_type}类型: {count:2d} episodes ({percentage:5.1f}%)\")\n",
    "\n",
    "    print(\"\\n🔍 DETAILED EPISODE BREAKDOWN | 详细情节分解\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "\n",
    "    # Individual Episode Analysis\n",
    "    # 单个情节分析\n",
    "    for i, episode in enumerate(episodes):\n",
    "        print(f\"\\n📖 EPISODE {i+1} | 情节 {i+1}\")\n",
    "        print(f\"{'─' * 60}\")\n",
    "\n",
    "        # Basic Information\n",
    "        # 基本信息\n",
    "        print(f\"🆔 ID: {episode.episode_id}\")\n",
    "        print(f\"👤 User ID | 用户ID: {episode.user_id}\")\n",
    "        print(f\"🏷️ Level | 级别: {episode.level.name} ({episode.level.value})\")\n",
    "        print(f\"📑 Type | 类型: {episode.episode_type.name}\")\n",
    "\n",
    "        # Title and Summary\n",
    "        # 标题和摘要\n",
    "        print(\"\\n📰 TITLE | 标题:\")\n",
    "        print(f\"   {episode.title}\")\n",
    "\n",
    "        print(\"\\n📋 SUMMARY | 摘要:\")\n",
    "        # Split summary into lines for better readability\n",
    "        # 将摘要分行以提高可读性\n",
    "        summary_lines = episode.summary.split('. ')\n",
    "        for line in summary_lines[:3]:  # Show first 3 sentences\n",
    "            if line.strip():\n",
    "                print(f\"   • {line.strip()}{'.' if not line.endswith('.') else ''}\")\n",
    "        if len(summary_lines) > 3:\n",
    "            print(f\"   • ... ({len(summary_lines) - 3} more sentences | 还有{len(summary_lines) - 3}句)\")\n",
    "\n",
    "        # Content Preview\n",
    "        # 内容预览\n",
    "        print(\"\\n📄 CONTENT PREVIEW | 内容预览:\")\n",
    "        content_preview = episode.content[:200].replace('\\n', ' ')\n",
    "        print(f\"   {content_preview}...\")\n",
    "        print(f\"   [Total length | 总长度: {len(episode.content):,} characters | 字符]\")\n",
    "\n",
    "        # Temporal Information\n",
    "        # 时间信息\n",
    "        print(\"\\n🕐 TEMPORAL INFO | 时间信息:\")\n",
    "        print(f\"   Timestamp | 时间戳: {episode.temporal_info.timestamp.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        if episode.temporal_info.duration:\n",
    "            hours = episode.temporal_info.duration / 3600\n",
    "            print(f\"   Duration | 持续时间: {hours:.1f} hours | 小时 ({episode.temporal_info.duration:.0f} seconds | 秒)\")\n",
    "        print(f\"   Timezone | 时区: {episode.temporal_info.timezone or 'Not specified | 未指定'}\")\n",
    "\n",
    "        # Metadata Analysis\n",
    "        # 元数据分析\n",
    "        print(\"\\n🏷️ METADATA | 元数据:\")\n",
    "        metadata = episode.metadata\n",
    "\n",
    "        print(\"   📊 Quality Scores | 质量分数:\")\n",
    "        print(f\"      Confidence | 置信度: {metadata.confidence_score:.2f}\")\n",
    "        print(f\"      Completeness | 完整性: {metadata.completeness_score:.2f}\")\n",
    "        print(f\"      Relevance | 相关性: {metadata.relevance_score:.2f}\")\n",
    "\n",
    "        if metadata.entities:\n",
    "            print(f\"   🏢 Entities | 实体 ({len(metadata.entities)}):\")\n",
    "            for entity in metadata.entities[:5]:  # Show first 5\n",
    "                print(f\"      • {entity}\")\n",
    "            if len(metadata.entities) > 5:\n",
    "                print(f\"      • ... and {len(metadata.entities) - 5} more | 还有{len(metadata.entities) - 5}个\")\n",
    "\n",
    "        if metadata.topics:\n",
    "            print(f\"   🏷️ Topics | 主题 ({len(metadata.topics)}):\")\n",
    "            for topic in metadata.topics[:5]:  # Show first 5\n",
    "                print(f\"      • {topic}\")\n",
    "            if len(metadata.topics) > 5:\n",
    "                print(f\"      • ... and {len(metadata.topics) - 5} more | 还有{len(metadata.topics) - 5}个\")\n",
    "\n",
    "        if metadata.emotions:\n",
    "            print(f\"   😊 Emotions | 情感 ({len(metadata.emotions)}):\")\n",
    "            for emotion in metadata.emotions[:3]:  # Show first 3\n",
    "                print(f\"      • {emotion}\")\n",
    "\n",
    "        if metadata.key_points:\n",
    "            print(f\"   🔑 Key Points | 关键点 ({len(metadata.key_points)}):\")\n",
    "            for point in metadata.key_points[:3]:  # Show first 3\n",
    "                print(f\"      • {point}\")\n",
    "\n",
    "        # Custom Fields\n",
    "        # 自定义字段\n",
    "        if metadata.custom_fields:\n",
    "            print(\"   🔧 Custom Fields | 自定义字段:\")\n",
    "            for key, value in metadata.custom_fields.items():\n",
    "                print(f\"      {key}: {value}\")\n",
    "\n",
    "        # Search Keywords\n",
    "        # 搜索关键词\n",
    "        if episode.search_keywords:\n",
    "            print(f\"\\n🔍 SEARCH KEYWORDS | 搜索关键词 ({len(episode.search_keywords)}):\")\n",
    "            keywords_str = \", \".join(episode.search_keywords[:10])  # Show first 10\n",
    "            print(f\"   {keywords_str}\")\n",
    "            if len(episode.search_keywords) > 10:\n",
    "                print(f\"   ... and {len(episode.search_keywords) - 10} more | 还有{len(episode.search_keywords) - 10}个\")\n",
    "\n",
    "        # Importance and Access Info\n",
    "        # 重要性和访问信息\n",
    "        print(\"\\n⭐ IMPORTANCE & ACCESS | 重要性和访问:\")\n",
    "        print(f\"   Importance Score | 重要性分数: {episode.importance_score:.2f}\")\n",
    "        print(f\"   Recall Count | 回忆次数: {episode.recall_count}\")\n",
    "        if episode.last_accessed:\n",
    "            print(f\"   Last Accessed | 最后访问: {episode.last_accessed.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        else:\n",
    "            print(\"   Last Accessed | 最后访问: Never | 从未\")\n",
    "\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"🎉 REPORT COMPLETE | 报告完成 🎉\")\n",
    "    print(f\"Generated {total_episodes} episodes from LoComo dataset using {'OpenAI' if openai_llm else 'Fallback mode'}\")\n",
    "    print(f\"使用{'OpenAI' if openai_llm else '回退模式'}从 LoComo 数据集生成了 {total_episodes} 个情节\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "\n",
    "# Generate detailed report if episodes exist\n",
    "# 如果存在情节则生成详细报告\n",
    "if 'episodes_created' in globals() and episodes_created:\n",
    "    print(\"\\n=== Generating Detailed Episode Report ===\")\n",
    "    print(\"=== 生成详细情节报告 ===\")\n",
    "\n",
    "    create_detailed_episode_report(episodes_created)\n",
    "\n",
    "    # Save report to text file\n",
    "    # 将报告保存到文本文件\n",
    "    import io\n",
    "    import sys\n",
    "\n",
    "    # Capture the report output\n",
    "    # 捕获报告输出\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = report_buffer = io.StringIO()\n",
    "\n",
    "    create_detailed_episode_report(episodes_created)\n",
    "\n",
    "    sys.stdout = old_stdout\n",
    "    report_content = report_buffer.getvalue()\n",
    "\n",
    "    # Save to file in output directory\n",
    "    # 保存到输出目录中的文件\n",
    "    output_dir = Path(\"output\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    report_path = output_dir / \"episode_analysis_report.txt\"\n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(report_content)\n",
    "\n",
    "    print(f\"\\n✓ Detailed report saved to: {report_path.absolute()}\")\n",
    "    print(f\"✓ 详细报告已保存到: {report_path.absolute()}\")\n",
    "    print(f\"  Report size: {report_path.stat().st_size / 1024:.1f} KB\")\n",
    "    print(f\"  报告大小: {report_path.stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠ No episodes available for detailed report\")\n",
    "    print(\"⚠ 没有可用于详细报告的情节\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary and File Overview\n",
    "# 最终总结和文件概览\n",
    "\n",
    "def display_final_summary():\n",
    "    \"\"\"Display a comprehensive summary of all generated files and next steps.\n",
    "    显示所有生成文件的综合总结和后续步骤。\"\"\"\n",
    "\n",
    "    print(\"🎯\" * 40)\n",
    "    print(\"🎉 NEMORI EPISODE GENERATION COMPLETE! | NEMORI 情节生成完成！ 🎉\")\n",
    "    print(\"🎯\" * 40)\n",
    "\n",
    "    print(\"\\n📁 GENERATED FILES | 生成的文件:\")\n",
    "    print(\"─\" * 50)\n",
    "\n",
    "        # Check which files exist in output directory\n",
    "    # 检查输出目录中存在哪些文件\n",
    "    output_dir = Path(\"output\")\n",
    "    files_to_check = [\n",
    "        (\"generated_episodes.json\", \"Structured episode data | 结构化情节数据\"),\n",
    "        (\"episode_visualization.png\", \"Visual dashboard | 可视化仪表板\"),\n",
    "        (\"episode_analysis_report.txt\", \"Detailed analysis report | 详细分析报告\")\n",
    "    ]\n",
    "\n",
    "    for filename, description in files_to_check:\n",
    "        file_path = output_dir / filename\n",
    "        if file_path.exists():\n",
    "            size_kb = file_path.stat().st_size / 1024\n",
    "            print(f\"✅ {filename}\")\n",
    "            print(f\"    📄 {description}\")\n",
    "            print(f\"    📊 Size | 大小: {size_kb:.1f} KB\")\n",
    "            print(f\"    📍 Path | 路径: {file_path.absolute()}\")\n",
    "            print()\n",
    "        else:\n",
    "            print(f\"❌ {filename} - Not found | 未找到\")\n",
    "            print(f\"    📄 {description}\")\n",
    "            print()\n",
    "\n",
    "    # Episode Statistics\n",
    "    # 情节统计\n",
    "    if 'episodes_created' in globals() and episodes_created:\n",
    "        print(\"\\n📊 EPISODE STATISTICS | 情节统计:\")\n",
    "        print(\"─\" * 50)\n",
    "\n",
    "        total_episodes = len(episodes_created)\n",
    "        total_messages = sum(int(ep.metadata.custom_fields.get('message_count', 0)) for ep in episodes_created)\n",
    "        total_participants = sum(int(ep.metadata.custom_fields.get('participant_count', 0)) for ep in episodes_created)\n",
    "\n",
    "        print(f\"📈 Total Episodes Generated | 生成的总情节数: {total_episodes}\")\n",
    "        print(f\"💬 Total Messages Processed | 处理的总消息数: {total_messages:,}\")\n",
    "        print(f\"👥 Total Participants | 总参与者数: {total_participants}\")\n",
    "        print(f\"🤖 LLM Provider Used | 使用的LLM提供商: {'OpenAI (gpt-4o-mini)' if openai_llm else 'Fallback Mode | 回退模式'}\")\n",
    "        print(\"⏱️ Processing Status | 处理状态: Complete | 完成\")\n",
    "\n",
    "        # Level breakdown\n",
    "        # 级别分解\n",
    "        levels = Counter([ep.level.name for ep in episodes_created])\n",
    "        print(\"\\n🏷️ Episode Level Breakdown | 情节级别分解:\")\n",
    "        for level, count in levels.items():\n",
    "            percentage = (count / total_episodes) * 100\n",
    "            print(f\"   {level}: {count} episodes ({percentage:.1f}%) | {count} 个情节 ({percentage:.1f}%)\")\n",
    "\n",
    "    print(\"\\n🚀 NEXT STEPS | 后续步骤:\")\n",
    "    print(\"─\" * 50)\n",
    "    print(\"1. 📖 Review the detailed analysis report | 查看详细分析报告\")\n",
    "    print(\"   - Open: episode_analysis_report.txt\")\n",
    "    print(\"   - Contains full episode breakdowns | 包含完整的情节分解\")\n",
    "\n",
    "    print(\"\\n2. 🎨 Examine the visualization dashboard | 检查可视化仪表板\")\n",
    "    print(\"   - Open: episode_visualization.png\")\n",
    "    print(\"   - Shows charts and graphs | 显示图表和图形\")\n",
    "\n",
    "    print(\"\\n3. 💾 Use the structured data | 使用结构化数据\")\n",
    "    print(\"   - File: generated_episodes.json\")\n",
    "    print(\"   - Import into other systems | 导入到其他系统\")\n",
    "    print(\"   - Further analysis and processing | 进一步分析和处理\")\n",
    "\n",
    "    print(\"\\n4. 🔧 Customize and Extend | 自定义和扩展\")\n",
    "    print(\"   - Modify visualization parameters | 修改可视化参数\")\n",
    "    print(\"   - Add new analysis functions | 添加新的分析功能\")\n",
    "    print(\"   - Integrate with other datasets | 与其他数据集集成\")\n",
    "\n",
    "    print(\"\\n💡 USAGE TIPS | 使用提示:\")\n",
    "    print(\"─\" * 50)\n",
    "    print(\"• Load episodes: json.load(open('generated_episodes.json'))\")\n",
    "    print(\"• 加载情节: json.load(open('generated_episodes.json'))\")\n",
    "    print(\"• Access episode content via ['episodes'][index]['content_preview']\")\n",
    "    print(\"• 通过['episodes'][index]['content_preview']访问情节内容\")\n",
    "    print(\"• Metadata includes entities, topics, emotions, and key points\")\n",
    "    print(\"• 元数据包括实体、主题、情感和关键点\")\n",
    "\n",
    "    print(\"\\n🔍 DATA STRUCTURE | 数据结构:\")\n",
    "    print(\"─\" * 50)\n",
    "    print(\"JSON Structure | JSON 结构:\")\n",
    "    print(\"├── metadata (generation info | 生成信息)\")\n",
    "    print(\"└── episodes[] (list of episodes | 情节列表)\")\n",
    "    print(\"    ├── index, episode_id, user_id\")\n",
    "    print(\"    ├── title, summary, content_preview\")\n",
    "    print(\"    ├── episode_type, level, timestamp\")\n",
    "    print(\"    ├── metadata (entities, topics, emotions)\")\n",
    "    print(\"    └── search_keywords, importance_score\")\n",
    "\n",
    "    print(\"\\n🎯 SUCCESS METRICS | 成功指标:\")\n",
    "    print(\"─\" * 50)\n",
    "    if 'episodes_created' in globals() and episodes_created:\n",
    "        print(\"✅ Episode generation: SUCCESS | 情节生成: 成功\")\n",
    "        print(\"✅ Data structuring: SUCCESS | 数据结构化: 成功\")\n",
    "        print(\"✅ Visualization: SUCCESS | 可视化: 成功\")\n",
    "        print(\"✅ Report generation: SUCCESS | 报告生成: 成功\")\n",
    "        print(\"✅ File export: SUCCESS | 文件导出: 成功\")\n",
    "    else:\n",
    "        print(\"❌ Episode generation: FAILED | 情节生成: 失败\")\n",
    "\n",
    "    print(f\"\\n{'🎯' * 40}\")\n",
    "    print(\"🎊 READY FOR EXPLORATION! | 准备好进行探索！ 🎊\")\n",
    "    print(\"Your LoComo conversations have been transformed into structured episodic memories!\")\n",
    "    print(\"您的 LoComo 对话已转换为结构化的情节记忆！\")\n",
    "    print(f\"{'🎯' * 40}\")\n",
    "\n",
    "# Display the final summary\n",
    "# 显示最终总结\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"FINAL SUMMARY | 最终总结\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "display_final_summary()\n",
    "\n",
    "# Show output directory contents\n",
    "# 显示输出目录内容\n",
    "print(\"\\\\n📂 OUTPUT DIRECTORY CONTENTS | 输出目录内容:\")\n",
    "print(\"─\" * 50)\n",
    "output_dir = Path(\"output\")\n",
    "if output_dir.exists():\n",
    "    print(f\"📍 Output Directory | 输出目录: {output_dir.absolute()}\")\n",
    "\n",
    "    # List output files\n",
    "    # 列出输出文件\n",
    "    relevant_extensions = ['.json', '.png', '.txt']\n",
    "    relevant_files = []\n",
    "\n",
    "    for file_path in output_dir.iterdir():\n",
    "        if file_path.is_file() and file_path.suffix in relevant_extensions:\n",
    "            relevant_files.append(file_path)\n",
    "else:\n",
    "    print(\"📂 Output directory not found | 输出目录未找到\")\n",
    "    relevant_files = []\n",
    "\n",
    "# Sort by modification time\n",
    "# 按修改时间排序\n",
    "relevant_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "\n",
    "for file_path in relevant_files:\n",
    "    size_kb = file_path.stat().st_size / 1024\n",
    "    mod_time = datetime.fromtimestamp(file_path.stat().st_mtime)\n",
    "    print(f\"📄 {file_path.name}\")\n",
    "    print(f\"    Size | 大小: {size_kb:.1f} KB\")\n",
    "    print(f\"    Modified | 修改时间: {mod_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print()\n",
    "\n",
    "print(\"🎉 ALL DONE! | 全部完成！ 🎉\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Test Results Summary\n",
    "# 测试结果总结\n",
    "\n",
    "## What This Notebook Tests\n",
    "## 本笔记本测试内容\n",
    "\n",
    "This notebook provides comprehensive testing for the Nemori conversation episode builder using the LoComo dataset with OpenAI integration:\n",
    "\n",
    "这个笔记本使用 LoComo 数据集和 OpenAI 集成为 Nemori 对话情节构建器提供了全面的测试：\n",
    "\n",
    "### Key Test Areas | 主要测试领域\n",
    "\n",
    "1. **Data Loading and Sampling** | **数据加载和采样**\n",
    "   - Loads LoComo RAG dataset\n",
    "   - Samples conversations for testing\n",
    "   - 加载 LoComo RAG 数据集\n",
    "   - 为测试采样对话\n",
    "\n",
    "2. **Data Format Conversion** | **数据格式转换**\n",
    "   - Converts LoComo format to Nemori RawUserData\n",
    "   - Handles timestamps and participant information\n",
    "   - 将 LoComo 格式转换为 Nemori RawUserData\n",
    "   - 处理时间戳和参与者信息\n",
    "\n",
    "3. **OpenAI Integration** | **OpenAI 集成**\n",
    "   - Sets up OpenAI provider with API key validation\n",
    "   - Tests connection to OpenAI API\n",
    "   - Uses gpt-4o-mini for cost-effective testing\n",
    "   - 设置带 API 密钥验证的 OpenAI 提供程序\n",
    "   - 测试与 OpenAI API 的连接\n",
    "   - 使用 gpt-4o-mini 进行经济高效的测试\n",
    "\n",
    "4. **Episode Building** | **情节构建**\n",
    "   - Tests ConversationEpisodeBuilder without LLM (fallback mode)\n",
    "   - Tests with real OpenAI LLM provider\n",
    "   - Compares AI-generated vs fallback content\n",
    "   - 测试不使用 LLM 的 ConversationEpisodeBuilder（回退模式）\n",
    "   - 使用真实 OpenAI LLM 提供程序进行测试\n",
    "   - 比较 AI 生成与回退内容\n",
    "\n",
    "5. **Validation** | **验证**\n",
    "   - Validates episode structure and required fields\n",
    "   - Tests episode level determination (ATOMIC, COMPOUND, THEMATIC)\n",
    "   - Compares OpenAI vs non-LLM results\n",
    "   - Error handling for API failures\n",
    "   - 验证情节结构和必需字段\n",
    "   - 测试情节级别确定（原子、复合、主题）\n",
    "   - 比较 OpenAI 与非 LLM 结果\n",
    "   - API 失败的错误处理\n",
    "\n",
    "### Requirements | 要求\n",
    "\n",
    "- OpenAI API key set in environment variable `OPENAI_API_KEY`\n",
    "- Internet connection for OpenAI API calls\n",
    "- 在环境变量 `OPENAI_API_KEY` 中设置 OpenAI API 密钥\n",
    "- 用于 OpenAI API 调用的网络连接\n",
    "\n",
    "### Expected Outcomes | 预期结果\n",
    "\n",
    "- Episodes should be created successfully from LoComo data\n",
    "- OpenAI integration should enhance episode quality with better titles and summaries\n",
    "- All required fields should be populated\n",
    "- Episode levels should be determined based on message count\n",
    "- Graceful fallback when OpenAI is unavailable\n",
    "- 应该从 LoComo 数据成功创建情节\n",
    "- OpenAI 集成应通过更好的标题和摘要提高情节质量\n",
    "- 所有必需字段都应填充\n",
    "- 应根据消息数确定情节级别\n",
    "- OpenAI 不可用时的优雅回退\n",
    "\n",
    "### Usage Instructions | 使用说明\n",
    "\n",
    "1. Set your OpenAI API key: `export OPENAI_API_KEY=your_key_here`\n",
    "2. Run cells in order from top to bottom\n",
    "3. Each cell builds on the previous ones\n",
    "4. Monitor API usage for cost control\n",
    "\n",
    "1. 设置您的 OpenAI API 密钥: `export OPENAI_API_KEY=your_key_here`\n",
    "2. 按顺序从上到下运行单元格\n",
    "3. 每个单元格都基于前面的单元格构建\n",
    "4. 监控 API 使用以控制成本\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
