"""
Semantic Memory Demo for Nemori Framework

This demo shows how to use the enhanced Nemori framework with semantic memory capabilities.
It demonstrates the complete workflow from conversation processing to semantic knowledge discovery.
"""

import asyncio
from datetime import datetime

from nemori.core.data_types import ConversationData, ConversationMessage, DataType, RawEventData, TemporalInfo
from nemori.storage.storage_types import StorageConfig
from nemori.storage.duckdb_semantic_storage import DuckDBSemanticMemoryRepository
from nemori.semantic.discovery import ContextAwareSemanticDiscoveryEngine
from nemori.semantic.evolution import SemanticEvolutionManager
from nemori.semantic.unified_retrieval import UnifiedRetrievalService
from nemori.builders.enhanced_conversation_builder import EnhancedConversationEpisodeBuilder


class MockLLMProvider:
    """Mock LLM provider for demo purposes."""

    def __init__(self):
        self.responses = {}
        self.call_count = 0

    def set_response_for_pattern(self, pattern: str, response: str):
        """Set response for a specific pattern in prompts."""
        self.responses[pattern] = response

    async def generate_async(self, prompt: str, **kwargs):
        """Generate response based on prompt patterns."""
        self.call_count += 1

        # Reconstruction prompts
        if "reconstruct" in prompt.lower():
            return """
            User: ä½ æœ€è¿‘åœ¨ç ”ç©¶ä»€ä¹ˆï¼Ÿ
            John: æˆ‘åœ¨ç ”ç©¶ä¸€äº›AIæŠ€æœ¯ã€‚
            User: èƒ½å…·ä½“è¯´è¯´å—ï¼Ÿ
            John: ä¸»è¦æ˜¯å…³äºæœºå™¨å­¦ä¹ çš„ã€‚
            """
        # Knowledge gap analysis prompts
        elif "knowledge gap" in prompt.lower():
            return """
            {
                "knowledge_gaps": [
                    {
                        "key": "Johnçš„ç ”ç©¶æ–¹å‘",
                        "value": "AI Agentè¡Œä¸ºè§„åˆ’",
                        "context": "ä¸“æ³¨äºå†³ç­–æœºåˆ¶",
                        "gap_type": "personal_fact",
                        "confidence": 0.9
                    }
                ]
            }
            """
        # Episode generation prompts
        elif "episodic memory generation" in prompt.lower():
            return """
            {
                "title": "Johnåˆ†äº«AI Agentç ”ç©¶è¿›å±•",
                "content": "åœ¨2024å¹´1æœˆ15æ—¥ä¸Šåˆ10ç‚¹ï¼ŒJohnä¸ç”¨æˆ·è®¨è®ºäº†ä»–çš„ç ”ç©¶æ–¹å‘ã€‚Johnè¡¨ç¤ºä»–æœ€è¿‘ä¸“æ³¨äºAI Agentçš„è¡Œä¸ºè§„åˆ’ç ”ç©¶ï¼Œç‰¹åˆ«æ˜¯å†³ç­–æœºåˆ¶æ–¹é¢çš„æŒ‘æˆ˜ã€‚è¿™æ ‡å¿—ç€Johnä»ä¹‹å‰çš„ç ”ç©¶é¢†åŸŸè½¬å‘äº†æ›´å…·æŒ‘æˆ˜æ€§çš„AgentæŠ€æœ¯ã€‚"
            }
            """

        return "Mock LLM response"

    async def generate(self, prompt: str, **kwargs):
        """Sync version of generate."""
        return await self.generate_async(prompt, **kwargs)


async def run_semantic_memory_demo():
    """Run the complete semantic memory demo."""
    print("ğŸš€ Nemori è¯­ä¹‰è®°å¿†æ¼”ç¤º | Semantic Memory Demo")
    print("=" * 60)

    # Initialize components
    print("\nğŸ”§ åˆå§‹åŒ–ç»„ä»¶ | Initializing Components...")

    # Storage
    config = StorageConfig(connection_string="duckdb:///:memory:")
    semantic_storage = DuckDBSemanticMemoryRepository(config)
    await semantic_storage.initialize()

    # LLM Provider
    mock_llm = MockLLMProvider()

    # Semantic components
    discovery_engine = ContextAwareSemanticDiscoveryEngine(mock_llm)
    evolution_manager = SemanticEvolutionManager(semantic_storage, discovery_engine)
    unified_retrieval = UnifiedRetrievalService(None, semantic_storage)

    # Enhanced builder
    builder = EnhancedConversationEpisodeBuilder(llm_provider=mock_llm, semantic_manager=evolution_manager)

    print("âœ… ç»„ä»¶åˆå§‹åŒ–å®Œæˆ | Components initialized")

    try:
        # Demo 1: First conversation about research
        print("\nğŸ“ æ¼”ç¤º1ï¼šé¦–æ¬¡ç ”ç©¶è®¨è®º | Demo 1: First Research Discussion")
        print("-" * 40)

        messages_1 = [
            ConversationMessage(
                speaker_id="User", content="Johnï¼Œä½ æœ€è¿‘åœ¨ç ”ç©¶ä»€ä¹ˆï¼Ÿ", timestamp=datetime(2024, 1, 15, 10, 0)
            ),
            ConversationMessage(
                speaker_id="John", content="æˆ‘æœ€è¿‘åœ¨ç ”ç©¶AI Agentçš„è¡Œä¸ºè§„åˆ’", timestamp=datetime(2024, 1, 15, 10, 1)
            ),
            ConversationMessage(speaker_id="User", content="å¬èµ·æ¥å¾ˆæœ‰è¶£ï¼", timestamp=datetime(2024, 1, 15, 10, 2)),
            ConversationMessage(
                speaker_id="John", content="æ˜¯çš„ï¼Œç‰¹åˆ«æ˜¯å†³ç­–æœºåˆ¶è¿™å—å¾ˆæœ‰æŒ‘æˆ˜æ€§", timestamp=datetime(2024, 1, 15, 10, 3)
            ),
        ]

        raw_data_1 = RawEventData(
            data_type=DataType.CONVERSATION,
            content=[msg.__dict__ for msg in messages_1],
            temporal_info=TemporalInfo(datetime(2024, 1, 15, 10, 0)),
        )

        conversation_data_1 = ConversationData(raw_data_1)
        episode_1 = await builder.build_episode(conversation_data_1, "john")

        print(f"ğŸ“– æƒ…æ™¯æ ‡é¢˜ | Episode Title: {episode_1.title}")
        print(
            f"ğŸ§  å‘ç°çš„è¯­ä¹‰çŸ¥è¯† | Discovered Semantic Knowledge: {episode_1.metadata.custom_fields.get('discovered_semantics', 0)} pieces"
        )

        # Check what was discovered
        discovered_nodes = await semantic_storage.get_all_semantic_nodes_for_owner("john")
        for node in discovered_nodes:
            print(f"   ğŸ’¡ {node.key}: {node.value}")

        # Demo 2: Search semantic knowledge
        print("\nğŸ” æ¼”ç¤º2ï¼šè¯­ä¹‰çŸ¥è¯†æœç´¢ | Demo 2: Semantic Knowledge Search")
        print("-" * 40)

        search_results = await unified_retrieval.search_semantic_memories("john", "ç ”ç©¶", 5)
        print(f"æœç´¢'ç ”ç©¶'æ‰¾åˆ° {len(search_results)} æ¡ç»“æœ:")
        for result in search_results:
            print(f"   ğŸ” {result.key}: {result.value} (ç½®ä¿¡åº¦: {result.confidence:.2f})")

        # Demo 3: Knowledge evolution (simulate research change)
        print("\nğŸ”„ æ¼”ç¤º3ï¼šçŸ¥è¯†æ¼”å˜ | Demo 3: Knowledge Evolution")
        print("-" * 40)

        # Mock different LLM response for evolution
        mock_llm.responses[
            "gap"
        ] = """
        {
            "knowledge_gaps": [
                {
                    "key": "Johnçš„ç ”ç©¶æ–¹å‘", 
                    "value": "å¤šæ¨¡æ€AI Agentç³»ç»Ÿ",
                    "context": "ä»è¡Œä¸ºè§„åˆ’æ‰©å±•åˆ°å¤šæ¨¡æ€",
                    "gap_type": "personal_fact",
                    "confidence": 0.95
                }
            ]
        }
        """

        messages_2 = [
            ConversationMessage(
                speaker_id="User", content="Johnï¼Œä½ çš„Agentç ”ç©¶æœ‰æ–°è¿›å±•å—ï¼Ÿ", timestamp=datetime(2024, 3, 20, 14, 0)
            ),
            ConversationMessage(
                speaker_id="John",
                content="æ˜¯çš„ï¼Œæˆ‘ç°åœ¨ä¸“æ³¨äºå¤šæ¨¡æ€AI Agentç³»ç»Ÿäº†",
                timestamp=datetime(2024, 3, 20, 14, 1),
            ),
            ConversationMessage(
                speaker_id="John", content="ä¸ä»…è¦å¤„ç†æ–‡æœ¬ï¼Œè¿˜è¦æ•´åˆè§†è§‰å’Œè¯­éŸ³", timestamp=datetime(2024, 3, 20, 14, 2)
            ),
        ]

        raw_data_2 = RawEventData(
            data_type=DataType.CONVERSATION,
            content=[msg.__dict__ for msg in messages_2],
            temporal_info=TemporalInfo(datetime(2024, 3, 20, 14, 0)),
        )

        conversation_data_2 = ConversationData(raw_data_2)
        episode_2 = await builder.build_episode(conversation_data_2, "john")

        print(f"ğŸ“– æ–°æƒ…æ™¯æ ‡é¢˜ | New Episode Title: {episode_2.title}")

        # Check evolution
        evolved_nodes = await semantic_storage.get_all_semantic_nodes_for_owner("john")
        for node in evolved_nodes:
            if node.version > 1:
                print(f"   ğŸ§¬ æ¼”å˜çš„çŸ¥è¯† | Evolved Knowledge: {node.key}")
                print(f"      å½“å‰å€¼ | Current: {node.value} (v{node.version})")
                print(f"      å†å²å€¼ | History: {', '.join(node.evolution_history)}")

        # Demo 4: Statistics and analysis
        print("\nğŸ“Š æ¼”ç¤º4ï¼šç»Ÿè®¡åˆ†æ | Demo 4: Statistics & Analysis")
        print("-" * 40)

        stats = await semantic_storage.get_semantic_statistics("john")
        print(f"ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯ | Statistics:")
        print(f"   æ€»èŠ‚ç‚¹æ•° | Total Nodes: {stats['total_nodes']}")
        print(f"   å¹³å‡ç½®ä¿¡åº¦ | Avg Confidence: {stats['average_confidence']:.2f}")
        print(f"   æ€»è®¿é—®æ¬¡æ•° | Total Access: {stats['total_access_count']}")
        evolution_stats = await evolution_manager.get_knowledge_evolution_stats("john")
        print(f"ğŸ”„ æ¼”å˜ç»Ÿè®¡ | Evolution Stats:")
        print(f"   æ¼”å˜èŠ‚ç‚¹æ•° | Evolved Nodes: {evolution_stats['evolved_nodes']}")
        print(f"   æ¼”å˜ç‡ | Evolution Rate: {evolution_stats['evolution_rate']:.1%}")
        print(f"   æ€»æ¼”å˜æ¬¡æ•° | Total Evolutions: {evolution_stats['total_evolutions']}")

        # Demo 5: Contextual retrieval
        print("\nğŸ¯ æ¼”ç¤º5ï¼šä¸Šä¸‹æ–‡æ£€ç´¢ | Demo 5: Contextual Retrieval")
        print("-" * 40)

        context = await unified_retrieval.get_contextual_knowledge(episode_2)
        print(f"ğŸ”— ä¸Šä¸‹æ–‡çŸ¥è¯†æ‘˜è¦ | Context Summary: {context['context_summary']}")
        print(f"   å…³è”è¯­ä¹‰èŠ‚ç‚¹ | Related Semantics: {len(context['related_semantics'])}")
        print(f"   ç›¸å…³å†å²æƒ…æ™¯ | Related Episodes: {len(context['related_episodes'])}")
        print("\nğŸ‰ è¯­ä¹‰è®°å¿†æ¼”ç¤ºå®Œæˆï¼| Semantic Memory Demo Complete!")
        print("\nğŸ’¡ å…³é”®ç‰¹æ€§å±•ç¤º | Key Features Demonstrated:")
        print("   âœ… è‡ªåŠ¨è¯­ä¹‰çŸ¥è¯†å‘ç° | Automatic semantic knowledge discovery")
        print("   âœ… çŸ¥è¯†æ¼”å˜è·Ÿè¸ª | Knowledge evolution tracking")
        print("   âœ… åŒå‘å…³è”é“¾æ¥ | Bidirectional association linking")
        print("   âœ… ç»Ÿä¸€æ£€ç´¢æœåŠ¡ | Unified retrieval service")
        print("   âœ… ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç”Ÿæˆ | Context-aware generation")
    finally:
        await semantic_storage.close()


async def simple_usage_example():
    """Simple usage example for quick testing."""
    print("\nğŸ¯ ç®€å•ä½¿ç”¨ç¤ºä¾‹ | Simple Usage Example")
    print("=" * 50)

    # Minimal setup for quick testing
    from nemori.core.data_types import SemanticNode

    config = StorageConfig(connection_string="duckdb:///:memory:")
    storage = DuckDBSemanticMemoryRepository(config)
    await storage.initialize()

    try:
        # Create and store knowledge
        knowledge = SemanticNode(
            owner_id="demo_user",
            key="æœ€å–œæ¬¢çš„ç¼–ç¨‹è¯­è¨€",
            value="Python",
            context="åœ¨è®¨è®ºæŠ€æœ¯æ ˆæ—¶æåˆ°æœ€å–œæ¬¢Pythonçš„ç®€æ´è¯­æ³•",
            confidence=0.9,
        )

        await storage.store_semantic_node(knowledge)
        print(f"âœ… å­˜å‚¨çŸ¥è¯†: {knowledge.key} = {knowledge.value}")

        # Search and retrieve
        results = await storage.similarity_search_semantic_nodes("demo_user", "ç¼–ç¨‹", 5)
        print(f"ğŸ” æœç´¢'ç¼–ç¨‹'æ‰¾åˆ° {len(results)} æ¡ç»“æœ")

        for result in results:
            print(f"   ğŸ’¡ {result.key}: {result.value}")

    finally:
        await storage.close()


if __name__ == "__main__":
    print("ğŸ§  Nemori Semantic Memory Framework")
    print("Choose demo to run:")
    print("1. Complete semantic memory demo")
    # print("2. Simple usage example")

    choice = "1"  # input("\nEnter choice (1 or 2): ").strip()

    if choice == "1":
        asyncio.run(run_semantic_memory_demo())
    elif choice == "2":
        asyncio.run(simple_usage_example())
    else:
        print("Running complete demo by default...")
        asyncio.run(run_semantic_memory_demo())
