"""
Unified Search Engine
"""

import logging
import asyncio
from typing import List, Dict, Any, Optional
import numpy as np
from concurrent.futures import ThreadPoolExecutor
from ..models import Episode, SemanticMemory
from ..utils import EmbeddingClient
from ..config import MemoryConfig
from .bm25_search import BM25Search
from .vector_search import VectorSearch

logger = logging.getLogger(__name__)


class UnifiedSearchEngine:
    """Unified search engine combining BM25 and vector search with incremental updates"""
    
    def __init__(self, embedding_client: EmbeddingClient, config: MemoryConfig, language: str = "en"):
        """
        Initialize unified search engine
        
        Args:
            embedding_client: Client for generating embeddings
            config: Memory system configuration
            language: Language for BM25 tokenization ("en" for English, "zh" for Chinese)
        """
        self.config = config
        self.language = language
        # Save embedding client for advanced ranking (e.g., NOR-LIFT)
        self.embedding_client = embedding_client
        
        # Initialize search engines
        self.bm25_search = BM25Search(language=language)
        self.vector_search = VectorSearch(embedding_client, config.storage_path, config.embedding_dimension)
        
        # Thread pool for parallel search
        self.executor = ThreadPoolExecutor(max_workers=2)
        
        logger.info(f"Unified search engine initialized with {language} tokenization")
    
    def index_episodes(self, user_id: str, episodes: List[Episode]):
        """
        Index episodes in both search engines
        
        Args:
            user_id: User ID
            episodes: List of episodes to index
        """
        try:
            # Index in both engines in parallel
            with ThreadPoolExecutor(max_workers=2) as executor:
                bm25_future = executor.submit(self.bm25_search.index_episodes, user_id, episodes)
                vector_future = executor.submit(self.vector_search.index_episodes, user_id, episodes)
                
                # Wait for both to complete
                bm25_future.result()
                vector_future.result()
            
            logger.info(f"Indexed {len(episodes)} episodes in unified search for user {user_id}")
            
        except Exception as e:
            logger.error(f"Error indexing episodes in unified search: {e}")
    
    def index_semantic_memories(self, user_id: str, memories: List[SemanticMemory]):
        """
        Index semantic memories in both search engines
        
        Args:
            user_id: User ID
            memories: List of semantic memories to index
        """
        try:
            # Index in both engines in parallel
            with ThreadPoolExecutor(max_workers=2) as executor:
                bm25_future = executor.submit(self.bm25_search.index_semantic_memories, user_id, memories)
                vector_future = executor.submit(self.vector_search.index_semantic_memories, user_id, memories)
                
                # Wait for both to complete
                bm25_future.result()
                vector_future.result()
            
            logger.info(f"Indexed {len(memories)} semantic memories in unified search for user {user_id}")
            
        except Exception as e:
            logger.error(f"Error indexing semantic memories in unified search: {e}")
    
    def search_episodes(
        self, 
        user_id: str, 
        query: str, 
        top_k: int = 10,
        search_method: str = "hybrid"
    ) -> List[Dict[str, Any]]:
        """
        Search episodes using unified approach
        
        Args:
            user_id: User ID
            query: Search query
            top_k: Number of results to return
            search_method: "bm25", "vector", or "hybrid"
            
        Returns:
            List of search results
        """
        try:
            if search_method == "bm25":
                return self.bm25_search.search_episodes(user_id, query, top_k)
            elif search_method == "vector":
                # å¯é€‰å¯ç”¨ NOR-LIFT æ’åº
                if getattr(self.config, 'enable_norlift_ranking', False):
                    return self.search_episodes_norlift(user_id, query, top_k)
                return self.vector_search.search_episodes(user_id, query, top_k)
            elif search_method == "vector_norlift":
                return self.search_episodes_norlift(user_id, query, top_k)
            elif search_method == "hybrid":
                return self._hybrid_search_episodes(user_id, query, top_k)
            else:
                logger.warning(f"Unknown search method: {search_method}, using hybrid")
                return self._hybrid_search_episodes(user_id, query, top_k)
                
        except Exception as e:
            logger.error(f"Error in unified episode search: {e}")
            return []
    
    def search_semantic_memories(
        self, 
        user_id: str, 
        query: str, 
        top_k: int = 10,
        search_method: str = "hybrid"
    ) -> List[Dict[str, Any]]:
        """
        Search semantic memories using unified approach
        
        Args:
            user_id: User ID
            query: Search query
            top_k: Number of results to return
            search_method: "bm25", "vector", or "hybrid"
            
        Returns:
            List of search results
        """
        try:
            if search_method == "bm25":
                return self.bm25_search.search_semantic_memories(user_id, query, top_k)
            elif search_method == "vector":
                return self.vector_search.search_semantic_memories(user_id, query, top_k)
            elif search_method == "hybrid":
                return self._hybrid_search_semantic_memories(user_id, query, top_k)
            else:
                logger.warning(f"Unknown search method: {search_method}, using hybrid")
                return self._hybrid_search_semantic_memories(user_id, query, top_k)
                
        except Exception as e:
            logger.error(f"Error in unified semantic memory search: {e}")
            return []
    
    def search(
        self,
        owner_id: str,
        query: str,
        top_k: int = 10,
        memory_types: Optional[List[str]] = None
    ) -> List[Dict[str, Any]]:
        """
        General search method for searching specified types of memories
        
        Args:
            owner_id: User identifier
            query: Search query
            top_k: Number of results to return
            memory_types: Types of memories to search ["episodic", "semantic"]
            
        Returns:
            List of search results
        """
        try:
            memory_types = memory_types or ["episodic", "semantic"]
            all_results = []
            
            # Search according to specified types
            if "episodic" in memory_types:
                episode_results = self.search_episodes(owner_id, query, top_k, "hybrid")
                all_results.extend(episode_results)
            
            if "semantic" in memory_types:
                semantic_results = self.search_semantic_memories(owner_id, query, top_k, "hybrid")
                all_results.extend(semantic_results)
            
            # Sort and return top_k results
            all_results.sort(
                key=lambda x: x.get("fused_score", x.get("score", 0)),
                reverse=True
            )
            
            return all_results[:top_k]
            
        except Exception as e:
            logger.error(f"Error in unified search: {e}")
            return []
    
    def search_all(
        self, 
        user_id: str, 
        query: str, 
        top_k_episodes: int = 5,
        top_k_semantic: int = 5,
        search_method: str = "hybrid"
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        Search both episodes and semantic memories
        
        Args:
            user_id: User ID
            query: Search query
            top_k_episodes: Number of episode results
            top_k_semantic: Number of semantic results
            search_method: "bm25", "vector", or "hybrid"
            
        Returns:
            Dictionary with separate episode and semantic results
        """
        try:
            # Perform searches in parallel
            with ThreadPoolExecutor(max_workers=2) as executor:
                episode_future = executor.submit(
                    self.search_episodes, 
                    user_id, query, top_k_episodes, search_method
                )
                semantic_future = executor.submit(
                    self.search_semantic_memories, 
                    user_id, query, top_k_semantic, search_method
                )
                
                episode_results = episode_future.result()
                semantic_results = semantic_future.result()
            
            return {
                "episodic": episode_results,
                "semantic": semantic_results,
                "combined": self._combine_and_rank_results(episode_results, semantic_results)
            }
            
        except Exception as e:
            logger.error(f"Error in unified search all: {e}")
            return {"episodic": [], "semantic": [], "combined": []}
    
    def _hybrid_search_episodes(self, user_id: str, query: str, top_k: int) -> List[Dict[str, Any]]:
        """
        Hybrid search for episodes combining BM25 and vector search
        
        Args:
            user_id: User ID
            query: Search query
            top_k: Number of results
            
        Returns:
            Hybrid search results
        """
        try:
            # Perform searches in parallel
            with ThreadPoolExecutor(max_workers=2) as executor:
                bm25_future = executor.submit(
                    self.bm25_search.search_episodes, 
                    user_id, query, top_k * 2  # Get more results for fusion
                )
                vector_future = executor.submit(
                    self.vector_search.search_episodes, 
                    user_id, query, top_k * 2
                )
                
                bm25_results = bm25_future.result()
                vector_results = vector_future.result()
            
            # Fuse results using Reciprocal Rank Fusion
            fused_results = self._reciprocal_rank_fusion(
                bm25_results, vector_results, top_k
            )
            
            return fused_results
            
        except Exception as e:
            logger.error(f"Error in hybrid episode search: {e}")
            # Fallback to BM25 search
            return self.bm25_search.search_episodes(user_id, query, top_k)
    
    def _hybrid_search_semantic_memories(self, user_id: str, query: str, top_k: int) -> List[Dict[str, Any]]:
        """
        Hybrid search for semantic memories combining BM25 and vector search
        
        Args:
            user_id: User ID
            query: Search query
            top_k: Number of results
            
        Returns:
            Hybrid search results
        """
        try:
            # Perform searches in parallel
            with ThreadPoolExecutor(max_workers=2) as executor:
                bm25_future = executor.submit(
                    self.bm25_search.search_semantic_memories, 
                    user_id, query, top_k * 2  # Get more results for fusion
                )
                vector_future = executor.submit(
                    self.vector_search.search_semantic_memories, 
                    user_id, query, top_k * 2
                )
                
                bm25_results = bm25_future.result()
                vector_results = vector_future.result()
            
            # Fuse results using Reciprocal Rank Fusion
            fused_results = self._reciprocal_rank_fusion(
                bm25_results, vector_results, top_k
            )
            
            return fused_results
            
        except Exception as e:
            logger.error(f"Error in hybrid semantic memory search: {e}")
            # Fallback to BM25 search
            return self.bm25_search.search_semantic_memories(user_id, query, top_k)
    
    def _reciprocal_rank_fusion(
        self, 
        results1: List[Dict[str, Any]], 
        results2: List[Dict[str, Any]], 
        top_k: int,
        k: int = 60
    ) -> List[Dict[str, Any]]:
        """
        Combine search results using Reciprocal Rank Fusion
        
        Args:
            results1: First set of results (e.g., BM25)
            results2: Second set of results (e.g., vector)
            top_k: Number of final results
            k: RRF parameter
            
        Returns:
            Fused and ranked results
        """
        # Create a mapping from item ID to result
        item_scores = {}
        item_data = {}
        
        # Process first result set
        for rank, result in enumerate(results1):
            item_id = result.get("episode_id") or result.get("memory_id")
            if item_id:
                rrf_score = 1.0 / (k + rank + 1)
                item_scores[item_id] = item_scores.get(item_id, 0) + rrf_score
                item_data[item_id] = result
        
        # Process second result set
        for rank, result in enumerate(results2):
            item_id = result.get("episode_id") or result.get("memory_id")
            if item_id:
                rrf_score = 1.0 / (k + rank + 1)
                item_scores[item_id] = item_scores.get(item_id, 0) + rrf_score
                
                # Update result data with hybrid information
                if item_id not in item_data:
                    item_data[item_id] = result
                else:
                    # Mark as hybrid result
                    item_data[item_id]["search_method"] = "hybrid"
        
        # Sort by fused score and return top k
        sorted_items = sorted(item_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]
        
        fused_results = []
        for item_id, fused_score in sorted_items:
            result = item_data[item_id].copy()
            result["fused_score"] = fused_score
            result["search_method"] = "hybrid"
            fused_results.append(result)
        
        return fused_results
    
    def _combine_and_rank_results(
        self, 
        episode_results: List[Dict[str, Any]], 
        semantic_results: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Combine and rank episode and semantic results
        
        Args:
            episode_results: Episode search results
            semantic_results: Semantic memory search results
            
        Returns:
            Combined and ranked results
        """
        all_results = episode_results + semantic_results
        
        # Sort by score (fused_score if available, otherwise original score)
        all_results.sort(
            key=lambda x: x.get("fused_score", x.get("score", 0)), 
            reverse=True
        )
        
        return all_results

    # ================= NOR-LIFT Aggregation Ranking =================
    def search_episodes_norlift(
        self,
        user_id: str,
        query: str,
        top_k: int = 10
    ) -> List[Dict[str, Any]]:
        """
        NOR-LIFT: ä½¿ç”¨æƒ…æ™¯å‘é‡ä¸å…¶è¯­ä¹‰è®°å¿†å‘é‡çš„ Noisy-OR + LIFT èšåˆå¯¹æƒ…æ™¯è¿›è¡Œæ’åºã€‚
        - å€™é€‰é›†ï¼šå–æƒ…æ™¯å‘é‡ Top-Pe ä¸è¯­ä¹‰å‘é‡ Top-Ps çš„å¹¶é›†æ‰€è¦†ç›–çš„æƒ…æ™¯
        - è¯„åˆ†ï¼šå¯¹å€™é€‰é›†æ‰€æœ‰åˆ†æ•°åš z-score æ ‡å‡†åŒ–â†’sigmoid æ˜ å°„â†’Noisy-OR èšåˆâ†’LIFT ä¿®æ­£
        """
        try:
            # å‰ç½®æ£€æŸ¥ï¼šç´¢å¼•å¯ç”¨
            if user_id not in self.vector_search.episode_data:
                logger.debug(f"No episode vector data for user {user_id}")
                return []

            # å‚æ•°
            pe = int(getattr(self.config, 'norlift_pool_size_episodes', 100))
            ps = int(getattr(self.config, 'norlift_pool_size_semantic', 200))
            tau_percentile = float(getattr(self.config, 'norlift_percentile_tau', 0.95))
            lam = float(getattr(self.config, 'norlift_sigmoid_lambda', 2.5))
            eps = float(getattr(self.config, 'norlift_epsilon', 1e-6))

            # è®¡ç®—æŸ¥è¯¢å‘é‡
            q_vec = np.array(self.embedding_client.embed_text(query), dtype=np.float32)
            if q_vec.size == 0:
                return []

            # è·å–æƒ…æ™¯å€™é€‰ï¼ˆTop-Peï¼‰
            epi_index = self.vector_search.episode_indices.get(user_id)
            episodes = self.vector_search.episode_data.get(user_id, [])
            if not episodes:
                return []
            pe = min(pe, len(episodes))

            epi_scores, epi_indices = self.vector_search._search_faiss_index(
                epi_index, q_vec, pe
            )

            episode_id_to_score: Dict[str, float] = {}
            episode_id_to_obj: Dict[str, Episode] = {}
            for score, idx in zip(epi_scores, epi_indices):
                if idx is None or idx < 0 or idx >= len(episodes):
                    continue
                ep = episodes[idx]
                episode_id_to_score[ep.episode_id] = float(score)
                episode_id_to_obj[ep.episode_id] = ep

            # è·å–è¯­ä¹‰è®°å¿†å€™é€‰ï¼ˆTop-Psï¼‰
            sem_index = self.vector_search.semantic_indices.get(user_id)
            sem_data = self.vector_search.semantic_data.get(user_id, [])
            sem_id_to_score: Dict[str, float] = {}
            sem_items = []  # (memory_obj, score)
            if sem_index is not None and sem_data:
                ps = min(ps, len(sem_data))
                sem_scores, sem_indices = self.vector_search._search_faiss_index(
                    sem_index, q_vec, ps
                )
                for score, idx in zip(sem_scores, sem_indices):
                    if idx is None or idx < 0 or idx >= len(sem_data):
                        continue
                    mem = sem_data[idx]
                    sem_id_to_score[mem.memory_id] = float(score)
                    sem_items.append((mem, float(score)))

            # æ„é€ å€™é€‰æƒ…æ™¯é›†åˆï¼šæƒ…æ™¯TopPe âˆª ç”±è¯­ä¹‰TopPsæŒ‡å‘çš„æƒ…æ™¯
            # å»º episode_id -> å…¶ç›¸å…³åˆ†æ•°ç»„åˆï¼ˆåŒ…å«æƒ…æ™¯è‡ªèº« + å‘½ä¸­çš„è¯­ä¹‰è®°å¿†ï¼‰
            # åŒæ—¶æ”¶é›†æ‰€æœ‰åŸå§‹åˆ†æ•°ç”¨äºå…¨å±€ z-score
            all_scores: List[float] = []
            episode_scores_map: Dict[str, List[float]] = {}

            # å…ˆæ”¾å…¥æƒ…æ™¯ TopPe
            for ep_id, s in episode_id_to_score.items():
                episode_scores_map.setdefault(ep_id, []).append(s)
                all_scores.append(s)

            # è¯­ä¹‰è®°å¿†å‘½ä¸­ï¼ŒæŒ‚åˆ°å„è‡ª source_episodes
            for mem, s in sem_items:
                if not getattr(mem, 'source_episodes', None):
                    continue
                for src_ep_id in mem.source_episodes:
                    episode_scores_map.setdefault(src_ep_id, []).append(s)
                    all_scores.append(s)

            # æœ‰äº›ä»…ç”±è¯­ä¹‰è®°å¿†å¸¦å…¥çš„æƒ…æ™¯ï¼Œå…¶â€œæƒ…æ™¯è‡ªèº«å‘é‡â€ä¸åœ¨ TopPeï¼Œéœ€è¦è¡¥å……å…¶æƒ…æ™¯åˆ†æ•°
            # é€šè¿‡å†…å­˜ä¸­çš„åµŒå…¥ç›´æ¥è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            if episode_scores_map:
                ep_emb_matrix = self.vector_search.episode_embeddings.get(user_id)
                if isinstance(ep_emb_matrix, np.ndarray) and ep_emb_matrix.size > 0:
                    # é¢„å»º id->index
                    ep_list = self.vector_search.episode_data[user_id]
                    id_to_idx = {ep.episode_id: i for i, ep in enumerate(ep_list)}

                    # å½’ä¸€åŒ–æŸ¥è¯¢å‘é‡
                    q_norm = q_vec / (np.linalg.norm(q_vec) + eps)

                    for ep_id in list(episode_scores_map.keys()):
                        if ep_id in episode_id_to_score:
                            # å·²æœ‰æƒ…æ™¯è‡ªèº«åˆ†æ•°
                            continue
                        idx = id_to_idx.get(ep_id, None)
                        if idx is None:
                            continue
                        vec = ep_emb_matrix[idx]
                        if vec is None or np.asarray(vec).size == 0:
                            continue
                        v = np.asarray(vec, dtype=np.float32)
                        v_norm = v / (np.linalg.norm(v) + eps)
                        sim = float(np.dot(v_norm, q_norm))
                        episode_scores_map[ep_id].append(sim)
                        all_scores.append(sim)

            if not episode_scores_map:
                return []

            # z-score æ ‡å‡†åŒ–ï¼ˆåŸºäºå€™é€‰é›†æ‰€æœ‰åˆ†æ•°ï¼‰
            scores_arr = np.array(all_scores, dtype=np.float32)
            mu = float(scores_arr.mean())
            sigma = float(scores_arr.std())
            denom = sigma + eps

            # å°†æ¯ä¸ª ep çš„åŸå§‹åˆ†æ•°æ˜ å°„ä¸º z/p
            def sigmoid(x: float) -> float:
                # æ•°å€¼ç¨³å®š Sigmoid
                if x >= 0:
                    z = np.exp(-x)
                    return float(1.0 / (1.0 + z))
                else:
                    z = np.exp(x)
                    return float(z / (1.0 + z))

            # è®¡ç®—å…¨éƒ¨ z ç”¨äºåˆ†ä½æ•°é˜ˆå€¼
            z_values = [ (s - mu) / denom for s in all_scores ]
            tau_z = float(np.percentile(np.array(z_values, dtype=np.float32), tau_percentile * 100.0))

            # é¢„è®¡ç®—æ¯ä¸ªåŸå§‹åˆ†æ•°çš„ pï¼ˆé‡ç”¨ï¼‰
            # ä¸ºé¿å…é‡å¤è®¡ç®—ï¼Œå»ºç«‹æ˜ å°„ï¼šraw_score -> p å€¼ã€‚è¿™é‡ŒåŸå§‹åˆ†æ•°æ˜¯æµ®ç‚¹ï¼Œå¯èƒ½é‡å¤ç‡ä½ï¼›ä¸ºå®‰å…¨ç”¨å››èˆäº”å…¥é”®ã€‚
            def p_from_score(s: float) -> float:
                z = (s - mu) / denom
                return sigmoid(lam * (z - tau_z))

            # è®¡ç®—æ¯ä¸ªæƒ…æ™¯çš„ Noisy-OR ä¸ LIFT
            all_p_values: List[float] = []
            per_episode_p_list: Dict[str, List[float]] = {}
            for ep_id, s_list in episode_scores_map.items():
                p_list = [p_from_score(s) for s in s_list]
                per_episode_p_list[ep_id] = p_list
                all_p_values.extend(p_list)

            if not all_p_values:
                return []

            bar_p = float(np.mean(np.array(all_p_values, dtype=np.float32)))

            def clamp01(x: float) -> float:
                if x <= eps:
                    return eps
                if x >= 1.0 - eps:
                    return 1.0 - eps
                return x

            def logit(x: float) -> float:
                x = clamp01(x)
                return float(np.log(x / (1.0 - x)))

            episode_scores: List[tuple[str, float]] = []
            for ep_id, p_list in per_episode_p_list.items():
                if not p_list:
                    continue
                # P_i via Noisy-OR
                one_minus = 1.0
                for p in p_list:
                    one_minus *= (1.0 - p)
                P_i = clamp01(1.0 - one_minus)

                m = len(p_list)  # d_i + 1
                P0 = clamp01(1.0 - (1.0 - bar_p) ** m)
                S_i = logit(P_i) - logit(P0)
                episode_scores.append((ep_id, float(S_i)))

            # æ’åºå¹¶ç»„è£…ç»“æœ
            episode_scores.sort(key=lambda x: x[1], reverse=True)
            top = episode_scores[:top_k]

            # è·å– Episode å¯¹è±¡ï¼ˆå¦‚ç¼ºå¤±åˆ™ä» episode_data ä¸­æŸ¥æ‰¾ï¼‰
            ep_list = self.vector_search.episode_data[user_id]
            id_to_ep = {ep.episode_id: ep for ep in ep_list}

            results: List[Dict[str, Any]] = []
            for ep_id, s in top:
                ep = id_to_ep.get(ep_id)
                if not ep:
                    continue
                results.append({
                    "type": "episodic",
                    "score": float(s),
                    "episode_id": ep.episode_id,
                    "title": ep.title,
                    "content": ep.content,
                    "original_messages": ep.original_messages,
                    "boundary_reason": ep.boundary_reason,
                    "timestamp": ep.timestamp.isoformat(),
                    "created_at": ep.created_at.isoformat(),
                    "message_count": ep.message_count,
                    "search_method": "vector_norlift"
                })

            return results

        except Exception as e:
            logger.error(f"Error in NOR-LIFT episode ranking: {e}")
            # å¤±è´¥å›é€€åˆ°æ™®é€šå‘é‡æ£€ç´¢
            return self.vector_search.search_episodes(user_id, query, top_k)
    
    def add_episode(self, user_id: str, episode: Episode):
        """
        Add episode to both search engines using INCREMENTAL UPDATE
        
        Args:
            user_id: User ID
            episode: Episode to add
        """
        try:
            # Add to both engines in parallel
            with ThreadPoolExecutor(max_workers=2) as executor:
                bm25_future = executor.submit(self.bm25_search.add_episode, user_id, episode)
                vector_future = executor.submit(self.vector_search.add_episode, user_id, episode)
                
                bm25_future.result()
                vector_future.result()
            
            logger.debug(f"ğŸš€ Incrementally added episode to unified search for user {user_id}")
            
        except Exception as e:
            logger.error(f"Error adding episode to unified search: {e}")
    
    def add_semantic_memory(self, user_id: str, memory: SemanticMemory):
        """
        Add semantic memory to both search engines using INCREMENTAL UPDATE
        
        Args:
            user_id: User ID
            memory: Semantic memory to add
        """
        try:
            # Check if executor is still available
            if hasattr(self, 'executor') and self.executor._shutdown:
                # Executor is shutting down, use synchronous calls
                logger.debug("Executor is shutting down, using synchronous updates")
                self.bm25_search.add_semantic_memory(user_id, memory)
                self.vector_search.add_semantic_memory(user_id, memory)
            else:
                # Add to both engines in parallel
                with ThreadPoolExecutor(max_workers=2) as executor:
                    bm25_future = executor.submit(self.bm25_search.add_semantic_memory, user_id, memory)
                    vector_future = executor.submit(self.vector_search.add_semantic_memory, user_id, memory)
                    
                    bm25_future.result()
                    vector_future.result()
            
            logger.debug(f"ğŸš€ Incrementally added semantic memory to unified search for user {user_id}")
            
        except Exception as e:
            logger.error(f"Error adding semantic memory to unified search: {e}")
    
    def clear_user_index(self, user_id: str) -> bool:
        """
        Clear all indices for a user from both search engines
        
        Args:
            user_id: User ID
            
        Returns:
            True if cleared successfully from both engines
        """
        try:
            bm25_success = self.bm25_search.clear_user_index(user_id)
            vector_success = self.vector_search.clear_user_index(user_id)
            
            return bm25_success and vector_success
            
        except Exception as e:
            logger.error(f"Error clearing unified search indices for user {user_id}: {e}")
            return False
    
    def get_index_stats(self) -> Dict[str, Any]:
        """
        Get comprehensive index statistics from both search engines
        
        Returns:
            Combined index statistics
        """
        try:
            bm25_stats = self.bm25_search.get_index_stats()
            vector_stats = self.vector_search.get_index_stats()
            
            # Combine stats
            combined_stats = {
                "search_engines": {
                    "bm25": bm25_stats,
                    "vector": vector_stats
                },
                # Overall stats
                "total_episodes": max(
                    bm25_stats.get("total_episodes", 0),
                    vector_stats.get("total_episodes", 0)
                ),
                "total_semantic_memories": max(
                    bm25_stats.get("total_semantic_memories", 0),
                    vector_stats.get("total_semantic_memories", 0)
                ),
                "episode_users": max(
                    bm25_stats.get("episode_users", 0),
                    vector_stats.get("episode_users", 0)
                ),
                "semantic_users": max(
                    bm25_stats.get("semantic_users", 0),
                    vector_stats.get("semantic_users", 0)
                ),
                # Features
                "incremental_updates": vector_stats.get("incremental_updates", False),
                "optimized_updates": bm25_stats.get("optimized_updates", False),
                "faiss_available": vector_stats.get("faiss_available", False),
                "spacy_available": bm25_stats.get("spacy_available", False),
                "language": self.language,
                "unified_engine": True
            }
            
            return combined_stats
            
        except Exception as e:
            logger.error(f"Error getting unified search index stats: {e}")
            return {
                "error": str(e),
                "unified_engine": True,
                "total_episodes": 0,
                "total_semantic_memories": 0
            }
    
    def __del__(self):
        """Cleanup resources"""
        try:
            self.executor.shutdown(wait=True)
        except:
            pass 